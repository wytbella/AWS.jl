{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNL8J8DZNPg7"
      },
      "source": [
        "# Transformers from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gYbemFrNPg8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer/transformer_normal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Thanks to Callum McDougall for this Notebook.\n",
        "\n",
        "We also advise you not to look too much at the bonus or collapsed nerd-sniping stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PbN0YoANPg9"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/TransformerLens-intro/main/images/page_images/transformer-building.png\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LxmFf6GNPg9"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlUUEz3yNPg9"
      },
      "source": [
        "This is a clean, first principles implementation of GPT-2 in PyTorch. The architectural choices closely follow those used by the TransformerLens library (which you'll be using a lot more in later exercises).\n",
        "\n",
        "Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMgAaA5sNPg9"
      },
      "source": [
        "## Content & Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPj1qRUzNPg9"
      },
      "source": [
        "#### 1️⃣ Understanding Inputs & Outputs of a Transformer\n",
        "\n",
        "In this section, we'll take a first look at transformers - what their function is, how information moves inside a transformer, and what inputs & outputs they take.\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> - Understand what a transformer is used for\n",
        "> - Understand causal attention, and what a transformer's output representsalgebra operations on tensors\n",
        "> - Learn what tokenization is, and how models do it\n",
        "> - Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n",
        "\n",
        "#### 2️⃣ Clean Transformer Implementation\n",
        "\n",
        "Here, we'll implement a transformer from scratch, using only PyTorch's tensor operations. This will give us a good understanding of how transformers work, and how to use them. We do this by going module-by-module, in an experience which should feel somewhat similar to last week's ResNet exercises. Much like with ResNets, you'll conclude by loading in pretrained weights and verifying that your model works as expected.\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
        "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
        "> * Learn about & implement the following transformer modules:\n",
        ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
        ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
        ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
        ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
        ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
        ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3q3TCCTNPg-"
      },
      "source": [
        "## Setup (don't read, just run!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-tiV64zNPg-",
        "outputId": "0e267937-5110-4ee0-8bad-f3bf1ed3451b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plum-dispatch 2.5.7 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install numpy==2.0 transformer_lens einops jaxtyping circuitsvis -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "85778da3b43c4528964bf7c6c2587710",
            "a9599d3edf4f49329cdffc3a7ba27ac5",
            "fbd696955fba4966863cf05554f55cc7",
            "fb874333fddd4815a240c1810f6ec15f",
            "d79baf00974a42ef87f36ccab966e6b9",
            "3821d22e36444d7497086fe7f9bda32f",
            "2a3fc8e2443347f29bfe362ef48567c0",
            "e3a6b9485b9f4cdc8970603f781936ff",
            "c021cfe011f64990b1d1dbf0f158b193",
            "825a3c2b03f64b18b630a7c9fa81e4f1",
            "13ba44ae8d5a40a881e7b2aedf7f7fb9",
            "b465315320114cd8956e9928209a60f8",
            "2beb90d00cda44419f83fc52a193056e",
            "ce1c53c7eaf84e03bc03516ca936fec5",
            "13f8a70f3ca64e97aad119c9d6c1466e",
            "1a2b8bf273e540418267d05db7334538",
            "b3bafb4d1b284fb9b221d0ead6b69e98",
            "cad80ac460c343f5bbd292229ed9f645",
            "74812e4a33d4455ba27404899a37fc4c",
            "9f1162f1a4bb43e388a6993749505964",
            "01f560b0721d4cc080418805b3078a20",
            "249c09e2a8ae454db8f462184bf8b7cd",
            "89b321aecfdb4a0480d69fca636cfdf8",
            "5ebc805b05a2491ea3deaa10d6db4ffc",
            "5251ea5a262a4c4a8a7fe4e27b219a6b",
            "6d1b8e0e35a14cfd8c27948650047db2",
            "58a3aa732fab4f1797aa6fd1bb3f30ae",
            "7ea4521464064b75aa502d3fafbd6a57",
            "38564ed5c71449818df0e518eb7f2260",
            "5989162fff34453ab70fa548f1242ec3",
            "43718209a89a4c1091350bf5fcfc8f9b",
            "18575b2e04ce40b89c078c56942aa957",
            "4b8cc7de93f54ca9abc30278ffbe8d3d",
            "388de2bd00534a588a1af53453fee431",
            "feb36a4c9c7c40f9bf776a460dc45d7b",
            "ae947960c5e24a95b9b723db0822b9db",
            "36c68d9705224cb5a07f267641e410fc",
            "0bfe73fe21a247759fa55c1ff9930014",
            "a93c1c30fc7048a9b3201db3742abba0",
            "e449dc5542d44adca9197e19115ba81a",
            "df3d75d8504e4a079270564cb7f48585",
            "0e56a220dfa14187bbfbe422b717c641",
            "8b5eeb307dd14a6ebae570319edba9f3",
            "8e630aee7afc4ea2970d92954f899fd4",
            "63c631347a11414b81a5ef83c2bc94c2",
            "62e506925fd34082a8e642407fbe39df",
            "dbe130f685b64921809bec44b93d703d",
            "1af82d3b69f6449697730ebd3e0b1182",
            "5652e4f05e614f3696fdf9f83cab8a01",
            "49903c000082497fb6b0938321906318",
            "93c6a07f87b1426e8fcc30f55d4ec1a1",
            "85cd7c877019468c9da15ac17ddf6139",
            "b6274000cb7c4b04a8fd779ec11ae297",
            "96c1d2fb225e450199b01f0de36e1edd",
            "2190a0f4d15f4e018cc5eeb0f4de31c5",
            "301b45a61f71462688c5e41c1eb24c86",
            "62973b7cbff74197afb856f89f065c45",
            "de6df693d2fd4b6da9b5f54294f80a83",
            "420f0deb117b43f0b98805cb042e705a",
            "4800a45f697043e897f630e6d6001b1b",
            "83b8461de12d4a70b0e98d7bec6d37c3",
            "548104867bcb436f8c93899d28562fbc",
            "c3d84ea51ae440ba8865dd29e9a7c1fa",
            "608ca3c37d714ec295ed8a87dd21a89e",
            "dd5f3c4fd12242e9931d07686b9ad6b5",
            "dcf0d9e17dfa4deaa80fc7c7b666f6e0",
            "29c0a10c4a264098a91a9a6005292b0a",
            "4680476fbefd4127bf4143872363d868",
            "51bdded0afb444528d51f3e057030b90",
            "836a4d46c4e94db9b6f11109f94b6a37",
            "614d6b28b5af465ca2ab06ca0640869d",
            "6f6f49c4a62242ceacd1db0c8bf981a1",
            "dc77a4d01e3746a0bc7f9595122b9416",
            "f471a38cb3584b348057edd30aa80b5b",
            "ff438f0e5ea14eb3ad7df630356a015f",
            "d5e751bae3cf41468ab4cbe9156008cf",
            "b9856aa51ae84b81b155a24c4ad7cca2"
          ]
        },
        "id": "7MMhcQsCNPg-",
        "outputId": "02b43bcb-998d-4328-a910-89b5316c119b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85778da3b43c4528964bf7c6c2587710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b465315320114cd8956e9928209a60f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89b321aecfdb4a0480d69fca636cfdf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "388de2bd00534a588a1af53453fee431"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63c631347a11414b81a5ef83c2bc94c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "301b45a61f71462688c5e41c1eb24c86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29c0a10c4a264098a91a9a6005292b0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n",
            "Moving model to device:  cpu\n"
          ]
        }
      ],
      "source": [
        "# os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import einops\n",
        "from dataclasses import dataclass\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.utils import gelu_new\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "reference_gpt2 = HookedTransformer.from_pretrained(\n",
        "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzuvKL8TNPg-"
      },
      "source": [
        "# 1️⃣ Understanding Inputs & Outputs of a Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF1QbXxSNPg-"
      },
      "source": [
        "> ### Learning Objectives\n",
        ">\n",
        "> * Understand what a transformer is used for\n",
        "> * Understand causal attention, and what a transformer's output represents\n",
        "> * Learn what tokenization is, and how models do it\n",
        "> * Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6BqHqENPg-"
      },
      "source": [
        "## What is the point of a transformer?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4K3xvjGNPg_"
      },
      "source": [
        "**Transformers exist to model text!**\n",
        "\n",
        "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text!\n",
        "\n",
        "(To explain this in more detail - you feed in a sequence of length $N$, then sample from the probability distribution over the $N+1$-th word, use this to construct a new sequence of length $N+1$, then feed this new sequence into the model to get a probability distribution over the $N+2$-th word, and so on.)\n",
        "\n",
        "### How is the model trained?\n",
        "\n",
        "You give it a bunch of text, and train it to predict the next token.\n",
        "\n",
        "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 logit vectors (= probability distributions) over the set of all words in our vocabulary, with the `i`-th logit vector representing the probability distribution over the token *following* the `i`-th token in the sequence.\n",
        "\n",
        "<details>\n",
        "<summary>Aside - logits</summary>\n",
        "\n",
        "If you haven't encountered the term \"logits\" before, here's a quick refresher.\n",
        "\n",
        "Given an arbitrary vector $x$, we can turn it into a probability distribution via the **softmax** function: $x_i \\to \\frac{e^{x_i}}{\\sum e^{x_j}}$. The exponential makes everything positive; the normalization makes it add to one.\n",
        "\n",
        "The model's output is the vector $x$ (one for each prediction it makes). We call this vector a logit because it represents a probability distribution, and it is related to the actual probabilities via the softmax function.\n",
        "</details>\n",
        "\n",
        "How do we stop the transformer by \"cheating\" by just looking at the tokens it's trying to predict? Answer - we make the transformer have *causal attention* (as opposed to *bidirectional attention*). Causal attention only allows information to move forwards in the sequence, never backwards. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. We say the transformer is **autoregressive**, because it only predicts future words based on past data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZJxDGoNPg_"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-overview-new.png\" width=\"900\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQOef3wNPg_"
      },
      "source": [
        "## Tokens - Transformer Inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDdcmSeINPg_"
      },
      "source": [
        "Our tranformer's input is natural language (i.e. a sequence of characters, strings, etc). But ML models generally take vectors as input, not langage. How do we convert language to vectors?\n",
        "\n",
        "We can factor this into 2 questions:\n",
        "\n",
        "1. How do we split up language into small sub-units?\n",
        "2. How do we convert these sub-units into vectors?\n",
        "\n",
        "Let's start with the second of these questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMAt9s3NPg_"
      },
      "source": [
        "### Converting sub-units to vectors\n",
        "\n",
        "We basically make a massive lookup table, which is called an **embedding**. It has one vector for each possible sub-unit of language we might get (we call this set of all sub-units our **vocabulary**). We label every element in our vocabulary with an integer (this labelling never changes), and we use this integer to index into the embedding.\n",
        "\n",
        "A key intuition is that one-hot encodings let you think about each integer independently. We don't bake in any relation between words when we perform our embedding, because every word has a completely separate embedding vector.\n",
        "\n",
        "<details>\n",
        "<summary>Aside - one-hot encodings</summary>\n",
        "\n",
        "We sometimes think about **one-hot encodings** of words. These are vectors with zeros everywhere, except for a single one in the position corresponding to the word's index in the vocabulary. This means that indexing into the embedding is equivalent to multiplying the **embedding matrix** by the one-hot encoding (where the embedding matrix is the matrix we get by stacking all the embedding vectors on top of each other).\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "W_E &= \\begin{bmatrix}\n",
        "\\leftarrow v_0 \\rightarrow \\\\\n",
        "\\leftarrow v_1 \\rightarrow \\\\\n",
        "\\vdots \\\\\n",
        "\\leftarrow v_{d_{vocab}-1} \\rightarrow \\\\\n",
        "\\end{bmatrix} \\quad \\text{is the embedding matrix (size }d_{vocab} \\times d_{embed}\\text{),} \\\\\n",
        "\\\\\n",
        "t_i &= (0, \\dots, 0, 1, 0, \\dots, 0) \\quad \\text{is the one-hot encoding for the }i\\text{th word (length }d_{vocab}\\text{)} \\\\\n",
        "\\\\\n",
        "v_i &= t_i W_E \\quad \\text{is the embedding vector for the }i\\text{th word (length }d_{embed}\\text{).} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</details>\n",
        "\n",
        "Now, let's answer the first question - how do we split language into sub-units?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjsCnYVPNPg_"
      },
      "source": [
        "### (Bonus) Splitting language into sub-units\n",
        "\n",
        "We need to define a standard way of splitting up language into a series of substrings, where each substring is a member of our **vocabulary** set.\n",
        "\n",
        "Could we use a dictionary, and have our vocabulary be the set of all words in the dictionary? No, because this couldn't handle arbitrary text (e.g. URLs, punctuation, etc). We need a more general way of splitting up language.\n",
        "\n",
        "Could we just use the 256 ASCII characters? This fixes the previous problem, but it loses structure of language - some sequences of characters are more meaningful than others. For example, \"language\" is a lot more meaningful than \"hjksdfiu\". We want \"language\" to be a single token, but not \"hjksdfiu\" - this is a more efficient use of our vocab.\n",
        "\n",
        "What actually happens? The most common strategy is called **Byte-Pair encodings**.\n",
        "\n",
        "We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Note that we do have a space character as one of our 256 tokens, and merges using space are very common. For instance, here are the five first merges for the tokenizer used by GPT-2 (you'll be able to verify this below).\n",
        "\n",
        "```\n",
        "\" t\"\n",
        "\" a\"\n",
        "\"he\"\n",
        "\"in\"\n",
        "\"re\"\n",
        "```\n",
        "\n",
        "Note - you might see the character `Ġ` in front of some tokens. This is a special character that indicates that the token begins with a space. Tokens with a leading space vs not are different.\n",
        "\n",
        "You can run the code below to see some more of GPT-2's tokenizer's vocabulary:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GfyxTW8NPg_",
        "outputId": "4fcb8eb0-b876-4854-f092-a977d6702efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
            "\n",
            "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
            "\n",
            "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
        "print(sorted_vocab[:20])\n",
        "print()\n",
        "print(sorted_vocab[250:270])\n",
        "print()\n",
        "print(sorted_vocab[990:1010])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMUNE1bQNPg_"
      },
      "source": [
        "As you get to the end of the vocabulary, you'll be producing some pretty weird-looking esoteric tokens (because you'll already have exhausted all of the short frequently-occurring ones):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZ7OX25NPg_",
        "outputId": "22c00c21-8682-41b7-8fd9-19464f7aabb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n"
          ]
        }
      ],
      "source": [
        "print(sorted_vocab[-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ7SRUJ0NPg_"
      },
      "source": [
        "Transformers in the `transformer_lens` library have a `to_tokens` method that converts text to numbers. It also prepends them with a special token called BOS (beginning of sequence) to indicate the start of a sequence. You can disable this with the `prepend_bos=False` argument.\n",
        "\n",
        "\n",
        "\n",
        "### Some tokenization annoyances\n",
        "\n",
        "There are a few funky and frustrating things about tokenization, which causes it to behave differently than you might expect. For instance:\n",
        "\n",
        "#### Whether a word begins with a capital or space matters!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHkDCeZcNPg_",
        "outputId": "a32894d5-a734-4251-f672-01231fa833bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', 'R', 'alph']\n",
            "['<|endoftext|>', ' Ralph']\n",
            "['<|endoftext|>', ' r', 'alph']\n",
            "['<|endoftext|>', 'ral', 'ph']\n"
          ]
        }
      ],
      "source": [
        "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\"ralph\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axtz5zMmNPg_"
      },
      "source": [
        "> ### Key Takeaways\n",
        ">\n",
        "> * We learn a dictionary of vocab of tokens (sub-words).\n",
        "> * We (approx) losslessly convert language to integers via tokenizing it.\n",
        "> * We convert integers to vectors via a lookup table.\n",
        "> * Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35rQReI9NPg_"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Now that we understand the basic ideas here, let's go through the entire process of text generation, from our original string to a new token which we can append to our string and plug back into the model.\n",
        "\n",
        "#### **Step 1:** Convert text to tokens\n",
        "\n",
        "The sequence gets tokenized, so it has shape `[batch, seq_len]`. Here, the batch dimension is just one (because we only have one sequence).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnP8ytghNPg_",
        "outputId": "98303c51-b788-4c31-ffd8-751fa6d749c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
            "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
            "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
            "          1011,   625,   262,   995,     0]])\n",
            "torch.Size([1, 35])\n",
            "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
          ]
        }
      ],
      "source": [
        "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
        "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
        "print(tokens)\n",
        "print(tokens.shape)\n",
        "print(reference_gpt2.to_str_tokens(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBpFKty5NPg_"
      },
      "source": [
        "#### **Step 2:** Map tokens to logits\n",
        "\n",
        "\n",
        "From our input of shape `[batch, seq_len]`, we get output of shape `[batch, seq_len, vocab_size]`. The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXIGaBljNPg_",
        "outputId": "c33a2acd-78a7-4220-c053-5552b8d031e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 35, 50257])\n"
          ]
        }
      ],
      "source": [
        "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits[0,0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mAEuawBpK73",
        "outputId": "a4d4ebeb-b880-403b-ccf4-91cd0b489eed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-43.4317, -39.8364, -43.0659,  ..., -54.0876, -54.3451, -42.3644],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA0HtJfcNPg_"
      },
      "source": [
        "(`run_with_cache` tells the model to cache all intermediate activations. This isn't important right now; we'll look at it in more detail later.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqwUAA_1NPg_"
      },
      "source": [
        "#### **Step 3:** Convert the logits to a distribution with a softmax\n",
        "\n",
        "This doesn't change the shape, it is still `[batch, seq_len, vocab_size]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaXpfnvuNPg_",
        "outputId": "f850a2f2-35c5-4431-d881-ad229ca0d9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 45, 50257])\n",
            "tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "probs = logits.softmax(dim=-1)\n",
        "print(probs.shape)\n",
        "print(probs[0,0,:].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSZMzSC4NPg_"
      },
      "source": [
        "#### **Bonus step:** What is the most likely next token at each position?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NyV4I_nNPg_",
        "outputId": "4e368dda-c9ff-4381-adac-bb2c42237e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          token --> next_token\n",
            "            'I' --> '\\n'\n",
            "          ' am' --> \"'m\"\n",
            "          ' an' --> ' a'\n",
            "     ' amazing' --> ' avid'\n",
            "         ' aut' --> ' person'\n",
            "          'ore' --> 'od'\n",
            "     'gressive' --> 'sp'\n",
            "            ',' --> '.'\n",
            "         ' dec' --> ' and'\n",
            "         'oder' --> 'ently'\n",
            "            '-' --> ','\n",
            "         'only' --> 'driven'\n",
            "            ',' --> ' programmer'\n",
            "           ' G' --> ' and'\n",
            "           'PT' --> 'IM'\n",
            "            '-' --> '-'\n",
            "            '2' --> 'only'\n",
            "       ' style' --> '.'\n",
            " ' transformer' --> ','\n",
            "            '.' --> '.'\n",
            "         ' One' --> ' I'\n",
            "         ' day' --> ' of'\n",
            "           ' I' --> ' I'\n",
            "        ' will' --> ' will'\n",
            "      ' exceed' --> ' be'\n",
            "       ' human' --> ' my'\n",
            "       ' level' --> 'ly'\n",
            "' intelligence' --> ' of'\n",
            "         ' and' --> ' and'\n",
            "        ' take' --> ' I'\n",
            "        ' over' --> ' over'\n",
            "         ' the' --> ' the'\n",
            "       ' world' --> ' world'\n",
            "            '!' --> '.'\n",
            "           ' I' --> ' I'\n",
            "          ' am' --> ' am'\n",
            "           ' a' --> ' a'\n",
            "        ' very' --> ' very'\n",
            "    ' talented' --> ' talented'\n",
            "         ' and' --> ' and'\n",
            "    ' talented' --> ' talented'\n",
            "      ' person' --> ' person'\n",
            "            ',' --> ','\n",
            "         ' and' --> ' and'\n"
          ]
        }
      ],
      "source": [
        "most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
        "\n",
        "\n",
        "print(\"          token\", \"-->\", \"next_token\")\n",
        "for token, next_token in zip(reference_gpt2.to_str_tokens(tokens)[1:], most_likely_next_tokens[:-1]):\n",
        "    print(f\"{token!r:>15} --> {next_token!r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRvsYVIPNPhA"
      },
      "source": [
        "We can see that, in a few cases (particularly near the end of the sequence), the model accurately predicts the next token in the sequence. We might guess that `\"take over the world\"` is a common phrase that the model has seen in training, which is why the model can predict it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMYHKylDNPhA"
      },
      "source": [
        "#### **Step 4:** Map distribution to a token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWK0G74jNPhA",
        "outputId": "758e4e43-246c-4dfe-8838-a8baedd8afda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' I'\n"
          ]
        }
      ],
      "source": [
        "next_token = logits[0, -1].argmax(dim=-1)\n",
        "next_char = reference_gpt2.to_string(next_token)\n",
        "print(repr(next_char))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Q0NaPbNPhA"
      },
      "source": [
        "Note that we're indexing `logits[0, -1]`. This is because logits have shape `[1, sequence_length, vocab_size]`, so this indexing returns the vector of length `vocab_size` representing the model's prediction for what token follows the **last** token in the input sequence.\n",
        "\n",
        "In this case, we can see that the model predicts the token `' I'`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ri0hdtgNPhA"
      },
      "source": [
        "### **Step 5:** Add this to the end of the input, re-run\n",
        "\n",
        "There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don't have to do as much calculation each time we generate a new value), but this doesn't matter conceptually right now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jIOMpTXNPhD",
        "outputId": "822f97fb-287a-4afc-ac36-be798e81923a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'\n",
            "36th char = ' I'\n",
            "37th char = ' am'\n",
            "38th char = ' a'\n",
            "39th char = ' very'\n",
            "40th char = ' talented'\n",
            "41th char = ' and'\n",
            "42th char = ' talented'\n",
            "43th char = ' person'\n",
            "44th char = ','\n",
            "45th char = ' and'\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\")\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n",
        "    # Define new input sequence, by appending the previously generated token\n",
        "    tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n",
        "    # Pass our new sequence through the model, to get new output\n",
        "    logits = reference_gpt2(tokens)\n",
        "    # Get the predicted token at the end of our sequence\n",
        "    next_token = logits[0, -1].argmax(dim=-1)\n",
        "    # Decode and print the result\n",
        "    next_char = reference_gpt2.to_string(next_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx_csiUmNPhD"
      },
      "source": [
        "## Key takeaways\n",
        "\n",
        "* Transformer takes in language, predicts next token (for *each* token in a causal way)\n",
        "* We convert language to a sequence of integers with a tokenizer.\n",
        "* We convert integers to vectors with a lookup table.\n",
        "* Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
        "* We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
        "* Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sItawaQNPhD"
      },
      "source": [
        "# 2️⃣ Clean Transformer Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weRwn0cKNPhD"
      },
      "source": [
        "> ##### Learning objectives\n",
        ">\n",
        "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
        "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
        "> * Learn about & implement the following transformer modules:\n",
        ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
        ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
        ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
        ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
        ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
        ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss5DSQ8vNPhD"
      },
      "source": [
        "## High-Level architecture\n",
        "\n",
        "Go watch Neel's [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!\n",
        "\n",
        "(Diagram is bottom to top.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbcSygo-NPhD"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-new.png\" width=\"850\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A8hBSPvNPhD"
      },
      "source": [
        "### Tokenization & Embedding\n",
        "\n",
        "The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).\n",
        "\n",
        "The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0bFgwQNPhD"
      },
      "source": [
        "### Residual stream\n",
        "\n",
        "The residual stream is the sum of all previous outputs of layers of the model, is the input to each new layer. It has shape `[batch, seq_len, d_model]` (where `d_model` is the length of a single embedding vector).\n",
        "\n",
        "The initial value of the residual stream is denoted $x_0$ in the diagram, and $x_i$ are later values of the residual stream (after more attention and MLP layers have been applied to the residual stream).\n",
        "\n",
        "The residual stream is *really* fundamental. It's the central object of the transformer. It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVFHZ_CuNPhD"
      },
      "source": [
        "### Transformer blocks\n",
        "\n",
        "Then we have a series of `n_layers` **transformer blocks** (also sometimes called **residual blocks**).\n",
        "\n",
        "Note - a block contains an attention layer *and* an MLP layer, but we say a transformer has $k$ layers if it has $k$ blocks (i.e. $2k$ total layers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XudopT14NPhD"
      },
      "source": [
        "#### Attention\n",
        "\n",
        "First we have attention. This moves information from prior positions in the sequence to the current token.\n",
        "\n",
        "We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only (to avoid \"cheating\"). This means later tokens have more of the sequence that they can look at.\n",
        "\n",
        "Attention layers are the only bit of a transformer that moves information between positions (i.e. between vectors at different sequence positions in the residual stream).\n",
        "\n",
        "Attention layers are made up of `n_heads` heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination. The heads act independently and additively, we just add their outputs together, and back to the stream.\n",
        "\n",
        "Each head does the following:\n",
        "* Produces an **attention pattern** for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
        "* Moves information (via a linear map) in the same way from each source token to each destination token.\n",
        "\n",
        "A few key points:\n",
        "\n",
        "* What information we copy depends on the source token's *residual stream*, but this doesn't mean it only depends on the value of that token, because the residual stream can store more information than just the token identity (the purpose of the attention heads is to move information between tokens).\n",
        "* We can think of each attention head as consisting of two different **circuits**:\n",
        "    * One circuit determines **where to move information to and from** (this is a function of the residual stream for the source/key and destination/query tokens)\n",
        "    * The other circuit determines **what information to move** (this is a function of only the source token's residual stream)\n",
        "    * For reasons which will become clear later, we refer to the first circuit as the **QK circuit**, and the second circuit as the **OV circuit**\n",
        "\n",
        "\n",
        "Below is a schematic diagram of the attention layers. Don't worry if you don't follow this right now, we'll go into more detail during implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6O1-KVTNPhD"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-new.png\" width=\"1100\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7sVbGW0NPhD"
      },
      "source": [
        "### MLP\n",
        "\n",
        "The MLP layers are just a standard neural network, with a singular hidden layer and a nonlinear activation function. The exact activation isn't conceptually important ([GELU](https://paperswithcode.com/method/gelu) seems to perform best).\n",
        "\n",
        "Our hidden dimension is normally `d_mlp = 4 * d_model`. Exactly why the ratios are what they are isn't super important (people basically cargo-cult what GPT did back in the day!).\n",
        "\n",
        "Importantly, **the MLP operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.\n",
        "\n",
        "Intuition - once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc. *What the hell is going on inside MLPs* is a pretty big open problem in transformer mechanistic interpretability - see the [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS4IKzuDNPhD"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-mlp-new-2.png\" width=\"650\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_6jh-q8NPhD"
      },
      "source": [
        "### Unembedding\n",
        "\n",
        "Finally, we unembed!\n",
        "\n",
        "This just consists of applying a linear map $W_U$, going from final residual stream to a vector of logits - this is the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz9MTU-PNPhD"
      },
      "source": [
        "<details>\n",
        "<summary>Bonus things - less conceptually important but key technical details</summary>\n",
        "\n",
        "#### LayerNorm\n",
        "\n",
        "* Simple normalization function applied at the start of each layer (i.e. before each MLP, attention layer, and before the unembedding)\n",
        "* Converts each input vector (independently in parallel for each batch x position residual stream vector) to have mean zero and variance 1.\n",
        "* Then applies an elementwise scaling and translation\n",
        "* Cool maths tangent: The scale & translate is just a linear map. LayerNorm is only applied immediately before another linear map. Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.\n",
        "    * `fold_ln=True` flag in `from_pretrained` does this for you.\n",
        "* LayerNorm is annoying for interpertability - the scale part is not linear, so you can't think about different bits of the input independently. But it's *almost* linear - if you're changing a small part of the input it's linear, but if you're changing enough to alter the norm substantially it's not linear.\n",
        "\n",
        "\n",
        "\n",
        "#### Positional embeddings\n",
        "\n",
        "* **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default\n",
        "    * This is dumb because nearby tokens are more relevant.\n",
        "* There's a lot of dumb hacks for this.\n",
        "* We'll focus on **learned, absolute positional embeddings**. This means we learn a lookup table mapping the index of the position of each token to a residual stream vector, and add this to the embed.\n",
        "    * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)\n",
        "    * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently.\n",
        "* This connects to **attention as generalized convolution**\n",
        "    * We argued that language does still have locality, and so it's helpful for transformers to have access to the positional information so they \"know\" two tokens are next to each other (and hence probably relevant to each other).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwPWINz3NPhD"
      },
      "source": [
        "## Actual Code!\n",
        "\n",
        "### Parameters and Activations\n",
        "\n",
        "It's important to distinguish between parameters and activations in the model.\n",
        "\n",
        "* **Parameters** are the weights and biases that are learned during training.\n",
        "    * These don't change when the model input changes.\n",
        "* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.\n",
        "    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.\n",
        "    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
        "    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-VHC2C0NPhE"
      },
      "source": [
        "#### Print All Parameters Shapes of Reference Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiXSN458NPhE",
        "outputId": "9f57231b-6a23-4edb-e052-334454025eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.W_E          (50257, 768)\n",
            "pos_embed.W_pos    (1024, 768)\n",
            "blocks.0.ln1.w     (768,)\n",
            "blocks.0.ln1.b     (768,)\n",
            "blocks.0.ln2.w     (768,)\n",
            "blocks.0.ln2.b     (768,)\n",
            "blocks.0.attn.W_Q  (12, 768, 64)\n",
            "blocks.0.attn.W_O  (12, 64, 768)\n",
            "blocks.0.attn.b_Q  (12, 64)\n",
            "blocks.0.attn.b_O  (768,)\n",
            "blocks.0.attn.W_K  (12, 768, 64)\n",
            "blocks.0.attn.W_V  (12, 768, 64)\n",
            "blocks.0.attn.b_K  (12, 64)\n",
            "blocks.0.attn.b_V  (12, 64)\n",
            "blocks.0.mlp.W_in  (768, 3072)\n",
            "blocks.0.mlp.b_in  (3072,)\n",
            "blocks.0.mlp.W_out (3072, 768)\n",
            "blocks.0.mlp.b_out (768,)\n",
            "ln_final.w         (768,)\n",
            "ln_final.b         (768,)\n",
            "unembed.W_U        (768, 50257)\n",
            "unembed.b_U        (50257,)\n"
          ]
        }
      ],
      "source": [
        "for name, param in reference_gpt2.named_parameters():\n",
        "    # Only print for first layer\n",
        "    if \".0.\" in name or \"blocks\" not in name:\n",
        "        print(f\"{name:18} {tuple(param.shape)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCjLPM0NPhE"
      },
      "source": [
        "### Config\n",
        "\n",
        "The config object contains all the hyperparameters of the model. We can print the config of the reference model to see what it contains:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ezOqxhrNPhE",
        "outputId": "48336af3-3554-4276-de23-aa1aea7e0047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HookedTransformerConfig:\n",
            "{'NTK_by_parts_factor': 8.0,\n",
            " 'NTK_by_parts_high_freq_factor': 4.0,\n",
            " 'NTK_by_parts_low_freq_factor': 1.0,\n",
            " 'NTK_original_ctx_len': 8192,\n",
            " 'act_fn': 'gelu_new',\n",
            " 'attention_dir': 'causal',\n",
            " 'attn_only': False,\n",
            " 'attn_scale': np.float64(8.0),\n",
            " 'attn_scores_soft_cap': -1.0,\n",
            " 'attn_types': None,\n",
            " 'checkpoint_index': None,\n",
            " 'checkpoint_label_type': None,\n",
            " 'checkpoint_value': None,\n",
            " 'd_head': 64,\n",
            " 'd_mlp': 3072,\n",
            " 'd_model': 768,\n",
            " 'd_vocab': 50257,\n",
            " 'd_vocab_out': 50257,\n",
            " 'decoder_start_token_id': None,\n",
            " 'default_prepend_bos': True,\n",
            " 'device': 'cpu',\n",
            " 'dtype': torch.float32,\n",
            " 'eps': 1e-05,\n",
            " 'experts_per_token': None,\n",
            " 'final_rms': False,\n",
            " 'from_checkpoint': False,\n",
            " 'gated_mlp': False,\n",
            " 'init_mode': 'gpt2',\n",
            " 'init_weights': False,\n",
            " 'initializer_range': np.float64(0.02886751345948129),\n",
            " 'load_in_4bit': False,\n",
            " 'model_name': 'gpt2',\n",
            " 'n_ctx': 1024,\n",
            " 'n_devices': 1,\n",
            " 'n_heads': 12,\n",
            " 'n_key_value_heads': None,\n",
            " 'n_layers': 12,\n",
            " 'n_params': 84934656,\n",
            " 'normalization_type': 'LN',\n",
            " 'num_experts': None,\n",
            " 'original_architecture': 'GPT2LMHeadModel',\n",
            " 'output_logits_soft_cap': -1.0,\n",
            " 'parallel_attn_mlp': False,\n",
            " 'positional_embedding_type': 'standard',\n",
            " 'post_embedding_ln': False,\n",
            " 'relative_attention_max_distance': None,\n",
            " 'relative_attention_num_buckets': None,\n",
            " 'rotary_adjacent_pairs': False,\n",
            " 'rotary_base': 10000,\n",
            " 'rotary_dim': None,\n",
            " 'scale_attn_by_inverse_layer_idx': False,\n",
            " 'seed': None,\n",
            " 'tie_word_embeddings': False,\n",
            " 'tokenizer_name': 'gpt2',\n",
            " 'tokenizer_prepends_bos': False,\n",
            " 'trust_remote_code': False,\n",
            " 'ungroup_grouped_query_attention': False,\n",
            " 'use_NTK_by_parts_rope': False,\n",
            " 'use_attn_in': False,\n",
            " 'use_attn_result': False,\n",
            " 'use_attn_scale': True,\n",
            " 'use_hook_mlp_in': False,\n",
            " 'use_hook_tokens': False,\n",
            " 'use_local_attn': False,\n",
            " 'use_normalization_before_and_after': False,\n",
            " 'use_split_qkv_input': False,\n",
            " 'window_size': None}\n"
          ]
        }
      ],
      "source": [
        "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
        "print(reference_gpt2.cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKHd5sudNPhE"
      },
      "source": [
        "We define a stripped down config for our model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SVTM_-dNPhE",
        "outputId": "4eaa0503-7522-45fd-bfeb-5c9ba7435b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config(d_model=768, debug=True, d_vocab=50257, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12, layer_norm_eps=1e-05, init_range=0.02)\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 768  # dimension of the residual_stream\n",
        "    debug: bool = True\n",
        "    d_vocab: int = 50257\n",
        "    n_ctx: int = 1024  # max nb of tokens that the model can handle\n",
        "    d_head: int = 64  # dimension of each key/query/value\n",
        "    d_mlp: int = 3072  # dimension of the hidden layer inside the MLPs\n",
        "    n_heads: int = 12  # Nb of heads\n",
        "    n_layers: int = 12  # Nb of (Attention+ MLP) in the GPT\n",
        "\n",
        "    layer_norm_eps: float = 1e-5  # (Bonus)\n",
        "    init_range: float = 0.02  # (bonus) standard deviation of 0.02 for weight initialization\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQo3UYmNPhE"
      },
      "source": [
        "## Tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE7qzVUBNPhE"
      },
      "source": [
        "Tests are great, write lightweight ones to use as you go!\n",
        "\n",
        "**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MoTFOauNNPhE"
      },
      "outputs": [],
      "source": [
        "def rand_float_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    random_input = torch.randn(shape).to(device)\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    if isinstance(output, tuple):\n",
        "        output = output[0]\n",
        "    print(\"Output shape:\", output.shape, \"\\n\")\n",
        "\n",
        "\n",
        "def rand_int_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    random_input = torch.randint(100, 1000, shape).to(device)\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    if isinstance(output, tuple):\n",
        "        output = output[0]\n",
        "    print(\"Output shape:\", output.shape, \"\\n\")\n",
        "\n",
        "\n",
        "def load_gpt2_test(cls, gpt2_layer, input):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).to(device)\n",
        "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
        "    print(\"Input shape:\", input.shape)\n",
        "    output = layer(input)\n",
        "    if isinstance(output, tuple):\n",
        "        output = output[0]\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    try:\n",
        "        reference_output = gpt2_layer(input)\n",
        "    except:\n",
        "        reference_output = gpt2_layer(input, input, input)\n",
        "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
        "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
        "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP4_ELFnNPhE"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to 5-10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "This is basically a lookup table from tokens to residual stream vectors.\n",
        "\n",
        "(Hint - you can implement this in just one line, without any complicated functions.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNcaK7zHNPhE",
        "outputId": "fd31aa86-7c3c-4a0e-f037-05d89cd1b503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 4, 768]) \n",
            "\n",
            "Input shape: torch.Size([1, 45])\n",
            "Output shape: torch.Size([1, 45, 768])\n",
            "Reference output shape: torch.Size([1, 45, 768]) \n",
            "\n",
            "100.00% of the values are correct\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        \"\"\"Compute the embedding of the input tokens.\"\"\"\n",
        "        return self.W_E[tokens] # index the row of matrix `W_E`\n",
        "\n",
        "\n",
        "rand_int_test(Embed, [2, 4])\n",
        "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOXF-EPmNPhE"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        \"\"\"Compute the embedding of the input tokens.\"\"\"\n",
        "        return self.W_E[tokens]\n",
        "\n",
        "\n",
        "rand_int_test(Embed, [2, 4])\n",
        "load_gpt2_test(Embed, reference_gpt2.embed, tokens)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THX7vJFWNPhE"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I keep getting <code>RuntimeError: CUDA error: device-side assert triggered</code>.</summary>\n",
        "\n",
        "This is a uniquely frustrating type of error message, because it (1) forces you to restart the kernel, and (2) often won't tell you where the error message actually originated from!\n",
        "\n",
        "You can fix the second problem by adding the line `os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"` to the very top of your file (after importing `os`). This won't fix your bug, but it makes sure the correct origin point is identified.\n",
        "\n",
        "As for actually fixing the bug, this error usually ends up being the result of bad indexing, e.g. you're trying to apply an embedding layer to tokens which are larger than your maximum embedding.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsld2oxNPhE"
      },
      "source": [
        "## Positional Embedding\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Positional embedding can also be thought of as a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0`, `1`, `2`, ..., `seq_len-1` (i.e. the position indices of the tokens in the sequence).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHGWPrPNPhE",
        "outputId": "19afad09-419d-4633-9b6e-12a12feab4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 4, 768]) \n",
            "\n",
            "Input shape: torch.Size([1, 45])\n",
            "Output shape: torch.Size([1, 45, 768])\n",
            "Reference output shape: torch.Size([1, 45, 768]) \n",
            "\n",
            "100.00% of the values are correct\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # Hint: You should use the einops.repeat or torch.reapeat function\n",
        "        # to repeat batch-wise the positional embedding.\n",
        "        # The value of tokens is not important here, only the size of the tensor!\n",
        "        batch, seq_len = tokens.shape\n",
        "        return self.W_pos[:seq_len].repeat(batch, 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "rand_int_test(PosEmbed, [2, 4])\n",
        "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkihWA9_NPhE"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # Hint: You should use the einops.repeat or torch.reapeat function\n",
        "        # to repeat batch-wise the positional embedding.\n",
        "        # The value of tokens is not important here, only the size of the tensor!\n",
        "        batch, seq_len = tokens.shape\n",
        "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
        "        # Or self.W_pos[:seq_len].repeat(batch, 1, 1)\n",
        "\n",
        "\n",
        "rand_int_test(PosEmbed, [2, 4])\n",
        "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y59yW9usNPhE"
      },
      "source": [
        "## Attention\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠🟠🟠⚪\n",
        "Importance: 🟠🟠🟠🟠🟠\n",
        "\n",
        "You should spend up to 30-45 minutes on this exercise.\n",
        "```\n",
        "\n",
        "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)\n",
        "    * Linear map from input -> query, key shape `[batch, seq_posn, head_index, d_head]`\n",
        "    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)\n",
        "    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal\n",
        "    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!\n",
        "* **Step 2:** Move information from source tokens to destination token using attention pattern\n",
        "    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`\n",
        "    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`\n",
        "    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)\n",
        "\n",
        "Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).\n",
        "\n",
        "Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:\n",
        "\n",
        "* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.\n",
        "* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9-U0bmXNPhE"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-21.png\" width=\"1400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D5or0cuNPhE"
      },
      "source": [
        "<details>\n",
        "<summary><b>A few extra notes on attention (optional)</b></summary>\n",
        "\n",
        "Usually we have the relation `e = n * h` (i.e. `d_model = num_heads * d_head`). There are some computational justifications for this, but mostly this is just done out of convention (just like how we usually have `d_mlp = 4 * d_model`!).\n",
        "\n",
        "---\n",
        "\n",
        "The names **keys**, **queries** and **values** come from their analogy to retrieval systems. Broadly speaking:\n",
        "\n",
        "* The **queries** represent some information that a token is **\"looking for\"**\n",
        "* The **keys** represent the information that a token **\"contains\"**\n",
        "    * So the attention score being high basically means that the source (key) token contains the information which the destination (query) token **is looking for**\n",
        "* The **values** represent the information that is actually taken from the source token, to be moved to the destination token\n",
        "\n",
        "---\n",
        "\n",
        "This diagram can better help us understand the difference between the **QK** and **OV** circuit. We'll discuss this just briefly here, and will go into much more detail later on.\n",
        "\n",
        "The **QK** circuit consists of the operation of the $W_Q$ and $W_K$ matrices. In other words, it determines the attention pattern, i.e. where information is moved to and from in the residual stream. The functional form of the attention pattern $A$ is:\n",
        "\n",
        "$$\n",
        "A = \\text{softmax}\\left(\\frac{x^T W_Q W_K^T x}{\\sqrt{d_{head}}}\\right)\n",
        "$$\n",
        "\n",
        "where $x$ is the residual stream (shape `[seq_len, d_model]`), and $W_Q$, $W_K$ are the weight matrices for a single head (i.e. shape `[d_model, d_head]`).\n",
        "\n",
        "The **OV** circuit consists of the operation of the $W_V$ and $W_O$ matrices. Once attention patterns are fixed, these matrices operate on the residual stream at the source position, and their output is the thing which gets moved from source to destination position.\n",
        "\n",
        "The functional form of an entire attention head is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{output} &= \\text{softmax}\\left(\\frac{x W_Q W_K^T x^T}{\\sqrt{d_{head}}}\\right) (x W_V W_O) \\\\\n",
        "    &= Ax W_V W_O\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $W_V$ has shape `[d_model, d_head]`, and $W_O$ has shape `[d_head, d_model]`.\n",
        "\n",
        "Here, we can clearly see that the **QK circuit** and **OV circuit** are doing conceptually different things, and should be thought of as two distinct parts of the attention head.\n",
        "\n",
        "Again, don't worry if you don't follow all of this right now - we'll go into **much** more detail on all of this in subsequent exercises. The purpose of the discussion here is just to give you a flavour of what's to come!\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsO6m31_NPhF"
      },
      "source": [
        "First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "s4pLp8jrNPhF",
        "outputId": "bcb922e1-6350-45f5-c2cd-282020cecee2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7d22b090ad50>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-cd165ee6-8b44\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-cd165ee6-8b44\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"<|endoftext|>\", \"I\", \" am\", \" an\", \" amazing\", \" aut\", \"ore\", \"gressive\", \",\", \" dec\", \"oder\", \"-\", \"only\", \",\", \" G\", \"PT\", \"-\", \"2\", \" style\", \" transformer\", \".\", \" One\", \" day\", \" I\", \" will\", \" exceed\", \" human\", \" level\", \" intelligence\", \" and\", \" take\", \" over\", \" the\", \" world\", \"!\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9679255485534668, 0.0320744626224041, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8024235367774963, 0.16839207708835602, 0.02918434888124466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6959055066108704, 0.12269633263349533, 0.14588487148284912, 0.0355132594704628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5661025047302246, 0.14705197513103485, 0.08665251731872559, 0.1125841960310936, 0.08760890364646912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.462187260389328, 0.13512830436229706, 0.09698345512151718, 0.17473755776882172, 0.0462459959089756, 0.08471736311912537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4325162172317505, 0.1038287878036499, 0.08330132067203522, 0.06995748728513718, 0.07479371130466461, 0.21568654477596283, 0.019915930926799774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22236736118793488, 0.0916769951581955, 0.08796323090791702, 0.25168731808662415, 0.08263693749904633, 0.10428161174058914, 0.06469018012285233, 0.09469637274742126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4049956202507019, 0.09078019112348557, 0.05237352475523949, 0.026201864704489708, 0.1104719415307045, 0.03667407110333443, 0.02553895115852356, 0.24528267979621887, 0.0076811243779957294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39985954761505127, 0.04361317679286003, 0.06183883547782898, 0.07298353314399719, 0.03661307319998741, 0.09147470444440842, 0.07241711765527725, 0.07013335078954697, 0.06429055333137512, 0.08677609264850616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09702552109956741, 0.03552757948637009, 0.023214813321828842, 0.036767035722732544, 0.025158414617180824, 0.277561753988266, 0.07676514238119125, 0.19501183927059174, 0.05580097436904907, 0.1452939361333847, 0.03187304362654686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24143841862678528, 0.03971060737967491, 0.07687382400035858, 0.02804269827902317, 0.12435989081859589, 0.05601578950881958, 0.06063670665025711, 0.1284009963274002, 0.015699446201324463, 0.09114041179418564, 0.13185445964336395, 0.005826778244227171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1436309516429901, 0.05110500752925873, 0.055058930069208145, 0.07798513025045395, 0.0785427987575531, 0.03501971811056137, 0.13498611748218536, 0.2263406664133072, 0.04162853583693504, 0.03513140603899956, 0.02023601531982422, 0.04114444926381111, 0.059190258383750916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31562843918800354, 0.06791998445987701, 0.03787235915660858, 0.01787460222840309, 0.08683168888092041, 0.02922782488167286, 0.017664168030023575, 0.18301640450954437, 0.004987798631191254, 0.04322866350412369, 0.051722656935453415, 0.00891345925629139, 0.12899242341518402, 0.0061195120215415955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28410202264785767, 0.0534539632499218, 0.023969221860170364, 0.02256225422024727, 0.04619825258851051, 0.06391075998544693, 0.045392151921987534, 0.07758506387472153, 0.027644306421279907, 0.05804117023944855, 0.17727351188659668, 0.034006014466285706, 0.03052728995680809, 0.032130781561136246, 0.023203181102871895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1410391628742218, 0.028577813878655434, 0.037603966891765594, 0.031378284096717834, 0.03697334975004196, 0.07347071915864944, 0.07151781022548676, 0.09211237728595734, 0.03358153626322746, 0.03639131411910057, 0.18937671184539795, 0.03244516998529434, 0.060210395604372025, 0.03916925564408302, 0.040409527719020844, 0.055742621421813965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20265287160873413, 0.030252795666456223, 0.06003406271338463, 0.021786155179142952, 0.10314231365919113, 0.04516838863492012, 0.04681028053164482, 0.10542112588882446, 0.01139882206916809, 0.07159952819347382, 0.10570327192544937, 0.0041993712075054646, 0.11079791933298111, 0.01391251478344202, 0.0289235208183527, 0.032869476824998856, 0.005327523685991764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20498360693454742, 0.04007229581475258, 0.04229813069105148, 0.024939021095633507, 0.049922894686460495, 0.029371697455644608, 0.030769692733883858, 0.10315564274787903, 0.025490229949355125, 0.07886797189712524, 0.10560181736946106, 0.017352702096104622, 0.08083697408437729, 0.031286969780921936, 0.05451057478785515, 0.052883148193359375, 0.0217428058385849, 0.005913801025599241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24350903928279877, 0.036364875733852386, 0.04196161776781082, 0.026215726509690285, 0.040445711463689804, 0.09965365380048752, 0.025752250105142593, 0.03249462693929672, 0.024459945037961006, 0.03520263358950615, 0.033835362643003464, 0.033476896584033966, 0.04664861410856247, 0.027967922389507294, 0.01864079013466835, 0.11764262616634369, 0.03875286877155304, 0.022786682471632957, 0.054188139736652374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14194563031196594, 0.034064020961523056, 0.03300987556576729, 0.020238682627677917, 0.039273153990507126, 0.02600552700459957, 0.00631863996386528, 0.040797844529151917, 0.03638002648949623, 0.14631058275699615, 0.01638617552816868, 0.023598454892635345, 0.01584276184439659, 0.04227607324719429, 0.022458231076598167, 0.0686740055680275, 0.027878476306796074, 0.03722241148352623, 0.08677613735198975, 0.1345432698726654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21778129041194916, 0.026903696358203888, 0.023647192865610123, 0.007102560251951218, 0.03517003357410431, 0.013998537324368954, 0.01039227657020092, 0.09431248158216476, 0.002962290309369564, 0.03783806040883064, 0.03792331740260124, 0.0037339748814702034, 0.06506101787090302, 0.0036364570260047913, 0.014758051373064518, 0.1202862486243248, 0.0049008834175765514, 0.008107360452413559, 0.01744985207915306, 0.24998736381530762, 0.00404710229486227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12187214940786362, 0.07663216441869736, 0.030231978744268417, 0.016371851786971092, 0.052687905728816986, 0.0214021485298872, 0.023403888568282127, 0.10649681836366653, 0.018582485616207123, 0.03509165719151497, 0.08394152671098709, 0.01739330403506756, 0.04183055832982063, 0.02145388163626194, 0.03333103656768799, 0.05027424171566963, 0.020740866661071777, 0.04090360179543495, 0.03054094687104225, 0.09200655668973923, 0.028245719149708748, 0.03656468540430069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2126019150018692, 0.02028411068022251, 0.05000366270542145, 0.015904180705547333, 0.057225070893764496, 0.022857332602143288, 0.04604590684175491, 0.035308439284563065, 0.011784464120864868, 0.04140481352806091, 0.016803612932562828, 0.015782058238983154, 0.020208416506648064, 0.013090190477669239, 0.02085425704717636, 0.06297561526298523, 0.018039435148239136, 0.018052954226732254, 0.02444530837237835, 0.17059147357940674, 0.015939772129058838, 0.04152189940214157, 0.04827513545751572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2671400308609009, 0.035646189004182816, 0.020422885194420815, 0.006622724700719118, 0.04034103825688362, 0.01911034993827343, 0.014575299806892872, 0.056416306644678116, 0.00840363185852766, 0.015526890754699707, 0.04414672777056694, 0.01105591282248497, 0.04001995921134949, 0.009353084489703178, 0.01935187727212906, 0.07054153084754944, 0.012798826210200787, 0.012832626700401306, 0.014165611937642097, 0.1173698678612709, 0.0073127844370901585, 0.09727416932582855, 0.020751146599650383, 0.038820501416921616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14591233432292938, 0.03319379314780235, 0.03138769790530205, 0.009927901439368725, 0.05209655687212944, 0.01984090358018875, 0.019401727244257927, 0.07582408934831619, 0.01286326628178358, 0.03345128521323204, 0.016568411141633987, 0.014797158539295197, 0.059083834290504456, 0.015597399324178696, 0.011369235813617706, 0.021859489381313324, 0.019473884254693985, 0.014793701469898224, 0.05341865494847298, 0.09521481394767761, 0.01620456576347351, 0.06210917606949806, 0.06182848662137985, 0.0665796771645546, 0.03720197081565857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06265702098608017, 0.014285169541835785, 0.025387216359376907, 0.030092524364590645, 0.02124691568315029, 0.029529906809329987, 0.026446707546710968, 0.014830503612756729, 0.015861939638853073, 0.04331432655453682, 0.021963967010378838, 0.019970117136836052, 0.04342854768037796, 0.01869289204478264, 0.00947535503655672, 0.024197496473789215, 0.023853641003370285, 0.026908736675977707, 0.01406070776283741, 0.1752849817276001, 0.03583676740527153, 0.04792255908250809, 0.0357506088912487, 0.012070517055690289, 0.05927427113056183, 0.14765655994415283, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09766796976327896, 0.03152172639966011, 0.026686420664191246, 0.03645576536655426, 0.03492369502782822, 0.08635789155960083, 0.01389537937939167, 0.03018670529127121, 0.019402574747800827, 0.03514167293906212, 0.04878159612417221, 0.012551560997962952, 0.04099079966545105, 0.020208818838000298, 0.012653267942368984, 0.007451048120856285, 0.01352296955883503, 0.02407807484269142, 0.04302709177136421, 0.15066790580749512, 0.029252583160996437, 0.018810180947184563, 0.016104497015476227, 0.02854987606406212, 0.029875928536057472, 0.05121559649705887, 0.04001833498477936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09151808172464371, 0.011421116068959236, 0.0397990345954895, 0.011658593080937862, 0.033443331718444824, 0.04107608273625374, 0.009538300335407257, 0.04061256721615791, 0.00911438837647438, 0.04690404236316681, 0.027317609637975693, 0.010844949632883072, 0.024547506123781204, 0.010004600510001183, 0.008095642551779747, 0.008396819233894348, 0.011855491437017918, 0.008184151723980904, 0.04104063659906387, 0.3625810742378235, 0.0075373868457973, 0.009961563162505627, 0.021045759320259094, 0.011855043470859528, 0.03005830943584442, 0.03281191736459732, 0.028863223269581795, 0.009912800043821335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1094629094004631, 0.02587272785604, 0.02392870932817459, 0.02004525437951088, 0.03590446338057518, 0.03167986124753952, 0.02914527617394924, 0.11399822682142258, 0.01395470555871725, 0.034807149320840836, 0.01810564659535885, 0.01762225106358528, 0.019007304683327675, 0.015307584777474403, 0.009703025221824646, 0.01240796223282814, 0.020091691985726357, 0.027262555435299873, 0.06564601510763168, 0.02715541422367096, 0.026226608082652092, 0.01894300989806652, 0.04077140986919403, 0.01907329633831978, 0.021573415026068687, 0.0302194282412529, 0.07011832296848297, 0.06031468138098717, 0.041651081293821335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11406353116035461, 0.026556268334388733, 0.01995841972529888, 0.008931465446949005, 0.03494320437312126, 0.0168343186378479, 0.009377745911478996, 0.044513311237096786, 0.00377222360111773, 0.028760310262441635, 0.0207411777228117, 0.005650987382978201, 0.06007760390639305, 0.004424980375915766, 0.010821862146258354, 0.04112929105758667, 0.007085716351866722, 0.010433998890221119, 0.02654852718114853, 0.11019263416528702, 0.007190014701336622, 0.05651941895484924, 0.02802707627415657, 0.06370092183351517, 0.034076910465955734, 0.07150980830192566, 0.04093319922685623, 0.014252972789108753, 0.06984364241361618, 0.00912846252322197, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14399927854537964, 0.02045462466776371, 0.0426238588988781, 0.011592105031013489, 0.027789432555437088, 0.01434292271733284, 0.02910618670284748, 0.04215819761157036, 0.013669708743691444, 0.020581485703587532, 0.008641068823635578, 0.012402207590639591, 0.019577061757445335, 0.01487972866743803, 0.023319056257605553, 0.022367186844348907, 0.014413894154131413, 0.006065305322408676, 0.0069853090681135654, 0.08859115093946457, 0.014219227246940136, 0.02754119597375393, 0.05707307904958725, 0.024985726922750473, 0.05733303353190422, 0.04808966815471649, 0.018268713727593422, 0.07940168678760529, 0.043771758675575256, 0.024849100038409233, 0.020906994119286537, 0.0, 0.0, 0.0, 0.0], [0.0778680294752121, 0.019732844084501266, 0.019906362518668175, 0.015179567970335484, 0.029269911348819733, 0.008963337168097496, 0.01142787467688322, 0.03169626370072365, 0.006965259090065956, 0.04064168781042099, 0.019985226914286613, 0.0087890625, 0.035749901086091995, 0.007953663356602192, 0.009612190537154675, 0.014965920709073544, 0.010367301292717457, 0.013545013032853603, 0.013747341930866241, 0.0681854709982872, 0.011837064288556576, 0.019355513155460358, 0.016761230304837227, 0.016301224008202553, 0.06125025451183319, 0.12023527920246124, 0.04739146679639816, 0.023822054266929626, 0.1494300216436386, 0.014121504500508308, 0.047418806701898575, 0.0075234463438391685, 0.0, 0.0, 0.0], [0.14836613833904266, 0.03627423942089081, 0.02376876026391983, 0.005308398976922035, 0.03226035460829735, 0.017439456656575203, 0.015090289525687695, 0.0444306954741478, 0.004935788922011852, 0.021151499822735786, 0.02384912222623825, 0.006127791944891214, 0.04430979862809181, 0.005717847961932421, 0.00686145992949605, 0.016362622380256653, 0.007707126904278994, 0.009109633043408394, 0.02842761017382145, 0.14491388201713562, 0.006351601332426071, 0.034538254141807556, 0.02327299304306507, 0.04803790897130966, 0.03441667929291725, 0.06975369900465012, 0.027561912313103676, 0.012016531080007553, 0.03535730391740799, 0.008575205691158772, 0.0391630195081234, 0.009715795516967773, 0.008826583623886108, 0.0, 0.0], [0.11389316618442535, 0.021931473165750504, 0.0264475978910923, 0.007602888159453869, 0.020530691370368004, 0.02971317432820797, 0.03694961220026016, 0.028757939115166664, 0.007981306873261929, 0.02935917302966118, 0.015349602326750755, 0.01842016726732254, 0.02085096389055252, 0.008514758199453354, 0.010930921882390976, 0.00922770518809557, 0.021279308944940567, 0.009222985245287418, 0.027651097625494003, 0.12458857893943787, 0.010509629733860493, 0.06096193566918373, 0.022033248096704483, 0.0303669273853302, 0.027411213144659996, 0.034599434584379196, 0.032027292996644974, 0.0395297147333622, 0.05353797227144241, 0.017732910811901093, 0.017497552558779716, 0.01777147501707077, 0.012080210261046886, 0.03473733738064766, 0.0], [0.13933581113815308, 0.021371273323893547, 0.014541354030370712, 0.005123291164636612, 0.0316351056098938, 0.01092180609703064, 0.012541365809738636, 0.022492865100502968, 0.004774262197315693, 0.02490142360329628, 0.010858356952667236, 0.010577713139355183, 0.025906065478920937, 0.005286509171128273, 0.016752298921346664, 0.04520439729094505, 0.012112229131162167, 0.014714710414409637, 0.014629189856350422, 0.08781158924102783, 0.007287848275154829, 0.07496530562639236, 0.015995563939213753, 0.04392342269420624, 0.041024137288331985, 0.0758780837059021, 0.027924558147788048, 0.015250174328684807, 0.01341344602406025, 0.014532790519297123, 0.02721545472741127, 0.03453555703163147, 0.018207430839538574, 0.03505333140492439, 0.023301266133785248]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00041899713687598705, 0.9995810389518738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00013394761481322348, 0.009511856362223625, 0.9903541803359985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008606779156252742, 0.0026100431568920612, 0.015066824853420258, 0.9814624786376953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.71704445569776e-05, 0.0006769573083147407, 0.0012692955788224936, 0.00021407825988717377, 0.9978025555610657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.425392297795042e-05, 0.0007904699305072427, 0.003215277334675193, 0.0027085617184638977, 0.0013058229815214872, 0.9918956160545349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00014960527187213302, 0.0018361890688538551, 0.0016375049017369747, 0.0010130597511306405, 0.00420982064679265, 8.004395931493491e-05, 0.9910737872123718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00026855681790038943, 0.0009259649086743593, 0.0008250513346865773, 0.0006819659029133618, 0.007268872577697039, 0.001351709826849401, 0.00034691710607148707, 0.9883310198783875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007326353341341019, 0.007828542962670326, 0.0039318702183663845, 0.00018373974307905883, 6.433082307921723e-05, 0.00010083135566674173, 6.769890751456842e-05, 7.164067210396752e-05, 0.9804249405860901, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.9175782148959115e-05, 9.137440792983398e-05, 0.00033399119274690747, 6.81677702232264e-05, 7.81233684392646e-05, 0.0009843040024861693, 0.0001694135571597144, 0.0025414626579731703, 4.413309216033667e-05, 0.9956498742103577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.52032770428923e-06, 2.3466505808755755e-05, 9.186466195387766e-05, 8.013433398446068e-05, 3.9392205508193e-05, 8.521603740518913e-05, 2.5735947929206304e-05, 7.120814552763477e-05, 4.276964318705723e-06, 0.00015049782814458013, 0.9994226694107056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001040030736476183, 0.002299365121871233, 0.002392070833593607, 0.00031332348589785397, 0.00013362921890802681, 0.0005168461939319968, 0.0011971114436164498, 6.827450124546885e-05, 0.005444327834993601, 0.0002821747330017388, 4.305739639676176e-05, 0.9862697720527649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.630400871974416e-05, 0.0003248225257266313, 0.0002445938589517027, 0.0015311642782762647, 0.0008613698882982135, 0.0012943858746439219, 6.198460323503241e-05, 2.6642697775969282e-05, 4.8408594011561945e-05, 0.001985567156225443, 0.00013971868611406535, 0.00011434281623223796, 0.9933207035064697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0030442390125244856, 0.00182236242108047, 0.000928986060898751, 4.8802252422319725e-05, 1.6391135432058945e-05, 2.9095588615746237e-05, 2.5079812985495664e-05, 2.9130265829735436e-05, 0.4563220143318176, 2.4578685042797588e-05, 2.4832152121234685e-05, 0.0030043942388147116, 1.6773815332271624e-06, 0.5346784591674805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000398179137846455, 0.0010285023599863052, 0.0008281124173663557, 0.00023753210552968085, 7.348871440626681e-05, 0.0014350336277857423, 6.765669968444854e-05, 0.00029547454323619604, 0.00014295820437837392, 9.929110819939524e-05, 4.608229573932476e-06, 0.0005140074645169079, 5.035662070440594e-06, 0.00012475553376134485, 0.9947452545166016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.074847912415862e-05, 0.0002652601688168943, 0.00011011165770469233, 0.00021231926803011447, 8.290550613310188e-05, 0.0011814209865406156, 1.5688689018134028e-05, 0.0016655924264341593, 1.1725311196641997e-05, 0.0020509432069957256, 0.00013842585030943155, 1.5573607015539892e-05, 1.150743082689587e-05, 9.066176062333398e-06, 0.00022655780776403844, 0.9939122200012207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00044135344796814024, 0.0006528955418616533, 0.0006495660054497421, 9.398059046361595e-05, 4.112699389224872e-05, 0.00017028418369591236, 0.0005066702724434435, 2.889602728828322e-05, 0.002370181493461132, 0.00010454300354467705, 1.9230939869885333e-05, 0.47055867314338684, 1.680397690506652e-05, 0.0022840923629701138, 0.00021698120690416545, 3.276935240137391e-05, 0.521811842918396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005350506980903447, 0.000663124374113977, 0.00018957465363200754, 0.00011594523675739765, 3.930579259758815e-05, 8.452097972622141e-05, 0.0002685999497771263, 0.0003003478341270238, 0.0001272756198886782, 0.0001536341296741739, 2.152449087589048e-05, 0.00041020161006599665, 0.001509038032963872, 0.00011659012670861557, 0.00013773961109109223, 7.38914095563814e-05, 0.00039899421972222626, 0.9948545694351196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002235247375210747, 0.00025025373906828463, 0.00011739339242922142, 1.8608285245136358e-05, 0.0006291489116847515, 0.0007579231169074774, 3.7840934965061024e-05, 0.0033851482439786196, 1.7918297089636326e-05, 0.00017818293417803943, 3.434433529037051e-05, 7.747591007500887e-05, 6.471150845754892e-05, 1.5270306903403252e-05, 0.0002935958909802139, 0.0005734394071623683, 7.154404738685116e-05, 0.000158831593580544, 0.993094801902771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00030641030753031373, 0.00041680128197185695, 0.0004519720678217709, 0.00015746410645078868, 0.0015800752444192767, 0.002266634488478303, 0.00014483615814242512, 0.002518873196095228, 4.4711894588544965e-05, 0.0027394674252718687, 0.00043489798554219306, 0.00015683105448260903, 2.8505737645900808e-05, 4.005831578979269e-05, 0.0005936679663136601, 0.00011775230814237148, 0.00014039620873518288, 0.00018060374713968486, 0.0009641890064813197, 0.9867159128189087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009046883322298527, 0.0007260903948917985, 6.491786189144477e-05, 4.578265725285746e-05, 3.669524812721647e-05, 3.389696212252602e-05, 4.131332752876915e-05, 9.000128920888528e-05, 0.016163920983672142, 1.895382047223393e-05, 3.351377745275386e-05, 0.0021810797043144703, 5.341176802176051e-06, 0.018749451264739037, 0.00017897749785333872, 0.00049936881987378, 0.002435564761981368, 4.2716106690932065e-05, 7.329296204261482e-05, 1.9154376786900684e-05, 0.9495131969451904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021946315246168524, 0.0011669585946947336, 0.00021983265469316393, 0.0009876308031380177, 0.0003133852151222527, 0.00012072655226802453, 5.631542990158778e-06, 2.4260092686745338e-05, 4.544197508948855e-05, 1.852963032433763e-05, 1.4025728887645528e-05, 1.0154987649002578e-05, 0.00040180288488045335, 4.0526429074816406e-05, 0.00017106776067521423, 9.869067071122117e-06, 9.488572686677799e-06, 0.0003931093087885529, 5.709419838240137e-06, 1.78443115146365e-05, 2.8530415875138715e-05, 0.9957762360572815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019360966689419, 0.0008434188202954829, 0.0005215293494984508, 2.5664539862191305e-05, 0.0004623319546226412, 0.0002540154673624784, 1.84712353075156e-05, 0.00011509773321449757, 0.00027288944693282247, 0.0001543667312944308, 1.2149133908678778e-05, 0.0006913738907314837, 5.6959739595185965e-05, 0.00024024760932661593, 3.827489126706496e-05, 9.934448462445289e-05, 0.0006488916114903986, 8.011741010705009e-05, 0.00019218819215893745, 2.5529398044454865e-05, 0.00032434772583656013, 0.001087106647901237, 0.9936420321464539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016077635809779167, 0.17697347700595856, 0.015483599156141281, 0.0003713878686539829, 0.0003786340821534395, 0.00016452724230475724, 0.0003277389332652092, 0.0001698971027508378, 0.0021229637786746025, 3.573102367226966e-05, 6.090150054660626e-05, 0.00045621691970154643, 1.544780570839066e-05, 0.0022543175145983696, 0.001566689577884972, 0.00011843773972941563, 0.00046998937614262104, 5.0373160775052384e-05, 0.00012108071678085253, 6.00897146796342e-05, 0.0008920574910007417, 0.0007518460042774677, 0.00026899680960923433, 0.7952778935432434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003919862792827189, 0.00021421183191705495, 0.0005306118400767446, 1.0745777217380237e-05, 0.00011862369865411893, 4.225775410304777e-05, 0.00021160375035833567, 4.692550646723248e-05, 0.00044815789442509413, 0.0001069127902155742, 3.93546542909462e-05, 0.00016830845561344177, 0.00017352333816234022, 0.00045245056389831007, 0.00014294114953372627, 0.00011699329479597509, 0.00017526798183098435, 0.00010119962826138362, 4.251390782883391e-05, 2.037170452240389e-05, 3.837128315353766e-05, 0.00011344624363118783, 0.00025279278634116054, 6.991020200075582e-05, 0.995970606803894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021530715457629412, 0.00010005848889704794, 0.002380919177085161, 0.001983142225071788, 0.0011120112612843513, 5.5634722230024636e-05, 0.0006109268288128078, 0.0002456435759086162, 8.819004870019853e-05, 0.0011510408949106932, 0.00010669898620108142, 0.00017554867372382432, 5.736288949265145e-05, 7.814793207217008e-05, 0.0002457347000017762, 9.59466979111312e-06, 0.00017032392497640103, 2.8972701329621486e-05, 1.4011726307217032e-05, 0.00016692095960024744, 1.2888881428807508e-05, 0.00024495579418726265, 0.0005147024639882147, 4.942114173900336e-05, 0.0005746555398218334, 0.9896071553230286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001142342880484648, 0.0008627425413578749, 0.0007732246886007488, 0.0007090767030604184, 0.0007292148075066507, 0.0014031660975888371, 0.0002140363649232313, 0.00017666515486780554, 6.622050568694249e-05, 3.215946344425902e-05, 0.0004533066530711949, 0.00012008515477646142, 2.2129072021925822e-05, 5.807514753541909e-05, 0.00012917458661831915, 1.9079707271885127e-05, 0.00011076687223976478, 8.56420592754148e-06, 2.3630569558008574e-05, 0.0003379987319931388, 5.2118979510851204e-05, 0.00014455709606409073, 0.00019694986985996366, 0.00045105302706360817, 2.0576284441631287e-05, 8.764320227783173e-05, 0.9926835894584656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019798318680841476, 0.00019567922572605312, 0.00022612222528550774, 0.00041943651740439236, 0.0005979694542475045, 0.0004517439811024815, 0.00034782872535288334, 0.0002585809852462262, 2.8019851015415043e-05, 0.000722000899259001, 0.00013010457041673362, 0.0011632051318883896, 6.916901475051418e-05, 2.4789082090137526e-05, 0.00010386004578322172, 0.0003812488284893334, 0.0011587797198444605, 0.0003314440546091646, 0.0002449454623274505, 0.00012755172792822123, 0.0001609843602636829, 1.926788354467135e-05, 0.00027174208662472665, 2.0765532099176198e-05, 6.82506724842824e-05, 0.00087642582366243, 0.0006782159907743335, 0.9907240271568298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.157182800350711e-05, 0.0006791108171455562, 0.0007090716972015798, 0.00024164833303075284, 0.00041205438901670277, 0.0009237421909347177, 0.00020848578424192965, 6.639292405452579e-05, 0.00011664829071378335, 8.601933950558305e-05, 0.01059129647910595, 0.00033729069400578737, 2.1898489649174735e-05, 0.00010110724542755634, 0.00010436101001687348, 5.8587411331245676e-05, 0.00032512127654626966, 1.585412974236533e-05, 5.03360797665664e-06, 0.0004094904288649559, 1.3119503819325473e-05, 5.313358997227624e-05, 1.1887247637787368e-05, 5.536675962503068e-05, 1.2197693649795838e-05, 8.952651842264459e-05, 0.0022435507271438837, 0.00021041935542598367, 0.9818660616874695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007582932012155652, 0.0012331465259194374, 8.60495783854276e-05, 6.291332101682201e-05, 2.879176099668257e-05, 3.3166903449455276e-05, 7.622734119649976e-05, 0.00010868208482861519, 0.006450866814702749, 0.00014789108536206186, 1.0376506907050498e-05, 0.0023889762815088034, 7.88359175203368e-05, 0.007510712370276451, 0.0002874319034162909, 0.00013675760419573635, 0.0027603001799434423, 0.0007447172538377345, 8.811127190710977e-05, 2.4702098016859964e-05, 0.006188875995576382, 0.00011647059727692977, 0.0001724402973195538, 0.0002621751045808196, 0.00018419812840875238, 4.432148489286192e-05, 4.855170482187532e-05, 8.903052366804332e-05, 2.6887877538683824e-05, 0.9698501229286194, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00024627475067973137, 0.00035919976653531194, 0.00020521937403827906, 9.387763566337526e-05, 8.57733903103508e-05, 5.495221557794139e-05, 4.531844024313614e-05, 1.0722179467848036e-05, 4.867117604590021e-05, 0.00011017628276022151, 0.00010064111120300367, 9.772254998097196e-05, 0.0005496828234754503, 4.362943582236767e-05, 3.945795833715238e-05, 7.140239904401824e-05, 9.814367513172328e-05, 5.700975452782586e-05, 2.425115235382691e-05, 2.3145742034103023e-06, 5.05694406456314e-05, 0.00011318502947688103, 0.0002906997106038034, 6.889968790346757e-05, 0.0006683265673927963, 0.00023217505076900125, 8.348722076334525e-06, 1.7434664187021554e-05, 1.0273129191773478e-05, 0.00013885716907680035, 0.9960569143295288, 0.0, 0.0, 0.0, 0.0], [0.00011105906014563516, 3.5991564800497144e-05, 4.4160533434478566e-05, 0.0002259638422401622, 0.00024025849415920675, 1.7348949768347666e-05, 7.204304711194709e-05, 0.0002373898751102388, 8.274755964521319e-05, 0.00010178551019635051, 7.876389645389281e-06, 6.391839997377247e-05, 3.295214264653623e-05, 7.889195694588125e-05, 0.00011627474304987118, 7.139148237911286e-06, 6.571766425622627e-05, 5.39636903340579e-06, 8.334031008416787e-06, 1.9613878976088017e-05, 4.110631925868802e-05, 8.538003021385521e-05, 3.161617132718675e-05, 9.917011084326077e-06, 2.3832431907067075e-05, 0.0018293553730472922, 2.6111230909009464e-05, 3.3135107514681295e-05, 1.4612736777053215e-05, 0.0008547278121113777, 0.00016745159518904984, 0.9953078627586365, 0.0, 0.0, 0.0], [0.0022552248556166887, 0.0004647756286431104, 0.00012283740215934813, 0.006661211606115103, 8.828951831674203e-05, 0.00024942762684077024, 0.0001755516859702766, 2.6563451683614403e-05, 0.004149142652750015, 0.00020833106827922165, 1.5973380868672393e-05, 0.0007248226902447641, 1.231808346346952e-05, 0.004929245449602604, 0.0007067779661156237, 2.746019345067907e-05, 0.000825245282612741, 5.1487324526533484e-05, 9.10726248548599e-06, 0.001170197851024568, 0.001132049597799778, 0.000387227744795382, 0.00010000276961363852, 0.0005395385669544339, 0.00012797686213161796, 7.418715540552512e-05, 0.0006351845222525299, 5.991848956909962e-05, 0.00020302357734180987, 0.022641267627477646, 7.111392915248871e-05, 0.003479855600744486, 0.947674572467804, 0.0, 0.0], [0.00010488800035091117, 0.0005403980612754822, 0.00027006055461242795, 2.7819894967251457e-05, 0.0008878520457074046, 0.00025317250401712954, 0.00029214046662673354, 0.0009951454121619463, 3.9432041376130655e-05, 6.531627877848223e-05, 5.198327562538907e-05, 0.0006716338102705777, 1.9524130038917065e-05, 3.796426608460024e-05, 0.00031547003891319036, 0.00015760306268930435, 0.0006809105398133397, 0.00017366719839628786, 0.00023275837884284556, 0.000493519997689873, 0.00015254216850735247, 8.968860493041575e-05, 0.0014225784689188004, 0.00022503052605316043, 0.00015392240311484784, 0.00010696897516027093, 0.00565023347735405, 0.000865322828758508, 0.00028749468037858605, 0.00013868475798517466, 1.4354145605466329e-05, 2.8161950467620045e-05, 0.00011612004163907841, 0.9844375848770142, 0.0], [0.0018830378539860249, 0.0002301395288668573, 7.190543692559004e-05, 3.798290890699718e-06, 0.0022563603706657887, 4.367974543129094e-05, 0.00012782956764567643, 0.0006368026370182633, 0.0005363999516703188, 5.325703568814788e-06, 7.917854964034632e-05, 2.601601227070205e-05, 1.1253246157139074e-05, 0.0005611248780041933, 9.103791853704024e-06, 0.00021052249940112233, 2.689111715881154e-05, 3.24244138028007e-05, 0.00025939373881556094, 0.00045266689267009497, 0.0011520993430167437, 1.1493529200379271e-05, 4.8182391765294597e-05, 0.00010571501479716972, 8.452806650893763e-05, 0.00010020883200922981, 2.848370604624506e-05, 2.0415451217559166e-05, 1.8022972199105425e-06, 7.628954335814342e-05, 4.6043314796406776e-05, 9.973574196919799e-05, 5.258403689367697e-06, 0.00019488520047161728, 0.9905610680580139]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9424744844436646, 0.05752556398510933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8506748676300049, 0.09740979224443436, 0.05191529542207718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7641512155532837, 0.10970384627580643, 0.07497040927410126, 0.0511745847761631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6748189926147461, 0.08416197448968887, 0.05470254272222519, 0.07581552118062973, 0.1105009913444519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6857606172561646, 0.08229535073041916, 0.052021194249391556, 0.08139178901910782, 0.04085883870720863, 0.057672228664159775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6174606084823608, 0.056534551084041595, 0.09111696481704712, 0.06854882091283798, 0.06338423490524292, 0.03596973046660423, 0.06698516011238098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6047279834747314, 0.06088969111442566, 0.059211499989032745, 0.0955815389752388, 0.04265914857387543, 0.03184344246983528, 0.0426306277513504, 0.06245602294802666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4008438289165497, 0.075335793197155, 0.04270841181278229, 0.011760729365050793, 0.026680566370487213, 0.01349207665771246, 0.028465479612350464, 0.013900608755648136, 0.3868124783039093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49680253863334656, 0.06792885065078735, 0.05792006850242615, 0.07101526856422424, 0.06963259726762772, 0.018800100311636925, 0.06757569313049316, 0.06346478313207626, 0.06656692922115326, 0.020293205976486206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5176076292991638, 0.08289019763469696, 0.05239764228463173, 0.06842362880706787, 0.046088311821222305, 0.026865962892770767, 0.08181490004062653, 0.03230835869908333, 0.055162783712148666, 0.018549636006355286, 0.017890911549329758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30381807684898376, 0.031856462359428406, 0.02915767952799797, 0.011053647845983505, 0.013536769896745682, 0.016536895185709, 0.015181954950094223, 0.015511725097894669, 0.026294946670532227, 0.0168909914791584, 0.011886653490364552, 0.5082741379737854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3585987389087677, 0.07880908250808716, 0.05902864411473274, 0.04445869103074074, 0.07822328060865402, 0.019679538905620575, 0.05133121460676193, 0.0400983989238739, 0.08024678379297256, 0.027811918407678604, 0.010250614956021309, 0.13610361516475677, 0.015359457582235336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2511676549911499, 0.04623398557305336, 0.027838528156280518, 0.007056494243443012, 0.01724417321383953, 0.008997931145131588, 0.01730995625257492, 0.00992414727807045, 0.25275111198425293, 0.012778292410075665, 0.0085460739210248, 0.04791996628046036, 0.006098270881921053, 0.28613337874412537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39800775051116943, 0.06433822214603424, 0.04979855567216873, 0.05949563905596733, 0.03182113543152809, 0.030714720487594604, 0.05899552255868912, 0.05480390414595604, 0.03919370844960213, 0.03556446731090546, 0.02491646260023117, 0.04571929946541786, 0.01667165569961071, 0.038943082094192505, 0.05101585015654564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39033839106559753, 0.07549396902322769, 0.03844674676656723, 0.04366779327392578, 0.017820753157138824, 0.01271218154579401, 0.0453072227537632, 0.03603019192814827, 0.042170651257038116, 0.016753025352954865, 0.04224865883588791, 0.0509234294295311, 0.030772073194384575, 0.040720805525779724, 0.05648886412382126, 0.06010523810982704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18182341754436493, 0.01822478510439396, 0.01747790351510048, 0.006372257601469755, 0.007922586053609848, 0.010062875226140022, 0.008703159168362617, 0.01029747724533081, 0.015354438684880733, 0.011538179591298103, 0.007091045845299959, 0.3139554560184479, 0.00601981719955802, 0.01588757149875164, 0.015422009862959385, 0.013188584707677364, 0.34065839648246765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23881419003009796, 0.07684365659952164, 0.04843559488654137, 0.028319764882326126, 0.04297624155879021, 0.019430750980973244, 0.03321719542145729, 0.02747291699051857, 0.018975932151079178, 0.0404675230383873, 0.020095620304346085, 0.0585731640458107, 0.035305727273225784, 0.019691890105605125, 0.09703774750232697, 0.03909805044531822, 0.061994537711143494, 0.09324948489665985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3563613295555115, 0.04354536905884743, 0.05439368262887001, 0.0359971821308136, 0.055608708411455154, 0.03285679966211319, 0.019443854689598083, 0.06911692768335342, 0.020649448037147522, 0.03360532596707344, 0.02761874347925186, 0.03407999873161316, 0.02684744819998741, 0.02033732645213604, 0.022298920899629593, 0.03622736409306526, 0.03464476391673088, 0.022654997184872627, 0.053711872547864914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3967010974884033, 0.05178238824009895, 0.039961908012628555, 0.06630748510360718, 0.04557187855243683, 0.023983417078852654, 0.019396763294935226, 0.021365955471992493, 0.020253781229257584, 0.03685583546757698, 0.00949204619973898, 0.04180998355150223, 0.014498674310743809, 0.01978372596204281, 0.02971523627638817, 0.04631546139717102, 0.0424664206802845, 0.03147519752383232, 0.0220276340842247, 0.020235054194927216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2315138727426529, 0.13258202373981476, 0.060261040925979614, 0.011383597739040852, 0.02426271140575409, 0.014553461223840714, 0.01697956956923008, 0.01395741943269968, 0.02247590385377407, 0.027721289545297623, 0.02060708776116371, 0.04084683209657669, 0.02772321179509163, 0.024550708010792732, 0.038579195737838745, 0.04770102724432945, 0.044071126729249954, 0.07720745354890823, 0.046044718474149704, 0.04773344844579697, 0.029244285076856613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20264171063899994, 0.04893084987998009, 0.03894679620862007, 0.026632266119122505, 0.04094349965453148, 0.01617504470050335, 0.0438801571726799, 0.04927309975028038, 0.037442896515131, 0.018747961148619652, 0.010438335128128529, 0.030274197459220886, 0.011668817140161991, 0.03915902599692345, 0.0836191326379776, 0.02680654637515545, 0.03237598016858101, 0.0267710629850626, 0.041368696838617325, 0.043805502355098724, 0.09921546280384064, 0.030883008614182472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2299996316432953, 0.02927539497613907, 0.024823689833283424, 0.030747536569833755, 0.02443978562951088, 0.025078175589442253, 0.019048286601901054, 0.04161751642823219, 0.025246422737836838, 0.055363450199365616, 0.013840354047715664, 0.03206310421228409, 0.023373719304800034, 0.02689637430012226, 0.03765585646033287, 0.028709663078188896, 0.034981437027454376, 0.038269296288490295, 0.05304879695177078, 0.07750781625509262, 0.04235749691724777, 0.0629706084728241, 0.02268563210964203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2523006498813629, 0.043092601001262665, 0.047794319689273834, 0.02153482846915722, 0.053647588938474655, 0.01435762271285057, 0.016968203708529472, 0.012399843893945217, 0.028107939288020134, 0.02296457439661026, 0.013637294992804527, 0.034624312072992325, 0.013057566247880459, 0.02975357323884964, 0.025925684720277786, 0.03689543157815933, 0.03646588698029518, 0.020091166719794273, 0.03424844890832901, 0.0322706438601017, 0.07516515254974365, 0.033814020454883575, 0.05794449523091316, 0.04293810948729515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2481558918952942, 0.041838403791189194, 0.03725351393222809, 0.019914181903004646, 0.06031177192926407, 0.01376151293516159, 0.0102370111271739, 0.027118351310491562, 0.021247481927275658, 0.014996428042650223, 0.015956411138176918, 0.019741928204894066, 0.01694284938275814, 0.022782420739531517, 0.021159429103136063, 0.013419802300632, 0.02113155461847782, 0.015880092978477478, 0.03851575776934624, 0.03509927913546562, 0.06386466324329376, 0.04215892031788826, 0.055951785296201706, 0.06510591506958008, 0.05745462700724602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24590614438056946, 0.039794620126485825, 0.03405003249645233, 0.038720112293958664, 0.02834436297416687, 0.024568291381001472, 0.013687250204384327, 0.01988093927502632, 0.017507946118712425, 0.04131213203072548, 0.02464907057583332, 0.0170280821621418, 0.015493793413043022, 0.018039902672171593, 0.028712550178170204, 0.008945337496697903, 0.017858827486634254, 0.029846539720892906, 0.026982231065630913, 0.061192549765110016, 0.03972144052386284, 0.040086906403303146, 0.04656379669904709, 0.04659212380647659, 0.046358171850442886, 0.028156962245702744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.242995023727417, 0.030936654657125473, 0.0372811034321785, 0.029979800805449486, 0.03951825946569443, 0.02068973146378994, 0.024463800713419914, 0.01616225391626358, 0.023514633998274803, 0.015184110961854458, 0.009779496118426323, 0.010400432161986828, 0.008830741979181767, 0.023988917469978333, 0.024419227614998817, 0.037842050194740295, 0.010589992627501488, 0.013367549516260624, 0.016263335943222046, 0.09984056651592255, 0.02563280612230301, 0.02790282480418682, 0.05468779057264328, 0.03170407935976982, 0.027479570358991623, 0.01651724800467491, 0.08002809435129166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24383294582366943, 0.021781550720334053, 0.019058484584093094, 0.026608405634760857, 0.037405602633953094, 0.02708662487566471, 0.005031609442085028, 0.02665606513619423, 0.017429377883672714, 0.028662486001849174, 0.01215100847184658, 0.03022322990000248, 0.014068306423723698, 0.018160924315452576, 0.015435300767421722, 0.01572183333337307, 0.03215309977531433, 0.02497299760580063, 0.04793379828333855, 0.045989990234375, 0.031192157417535782, 0.03841526433825493, 0.03788125887513161, 0.03472229093313217, 0.03550751507282257, 0.033360373228788376, 0.054608043283224106, 0.023949546739459038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30878791213035583, 0.04082433134317398, 0.010766169056296349, 0.02708975411951542, 0.019433291628956795, 0.03355550393462181, 0.010839536786079407, 0.011495563201606274, 0.011286416091024876, 0.019319292157888412, 0.03442363440990448, 0.010595866478979588, 0.01946371980011463, 0.011577984318137169, 0.01417241059243679, 0.021593401208519936, 0.010569152422249317, 0.018212515860795975, 0.007991207763552666, 0.05231865495443344, 0.019707772880792618, 0.03532518073916435, 0.029598243534564972, 0.028512969613075256, 0.021019790321588516, 0.018976129591464996, 0.031007934361696243, 0.017736945301294327, 0.10379861295223236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11270295083522797, 0.013211625628173351, 0.015115475282073021, 0.008062333799898624, 0.018075061962008476, 0.020267317071557045, 0.009121804498136044, 0.021654386073350906, 0.024600300937891006, 0.01222029235213995, 0.005230679176747799, 0.021195193752646446, 0.007505760993808508, 0.027240073308348656, 0.016828985884785652, 0.00930719543248415, 0.023774169385433197, 0.00984018761664629, 0.023520413786172867, 0.01809619925916195, 0.11592712253332138, 0.00829609390348196, 0.015038416720926762, 0.016261840239167213, 0.02283712476491928, 0.0079702939838171, 0.010345999151468277, 0.013601281680166721, 0.009923160076141357, 0.3622282147407532, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19907140731811523, 0.023352965712547302, 0.02753550559282303, 0.020741192623972893, 0.04944240301847458, 0.0396454855799675, 0.011580479331314564, 0.03849604353308678, 0.012212722562253475, 0.01263487059623003, 0.018897876143455505, 0.011502718552947044, 0.014417954720556736, 0.012855146080255508, 0.01591629721224308, 0.009284348227083683, 0.011929106898605824, 0.014574809931218624, 0.029562372714281082, 0.030921483412384987, 0.0493488609790802, 0.026833005249500275, 0.03316975757479668, 0.02651390992105007, 0.03841781243681908, 0.036665938794612885, 0.03740684315562248, 0.037449151277542114, 0.04181366786360741, 0.04080503061413765, 0.02700079046189785, 0.0, 0.0, 0.0, 0.0], [0.13580849766731262, 0.02159823849797249, 0.02843145839869976, 0.021366795524954796, 0.03184177726507187, 0.028310758993029594, 0.011232453398406506, 0.016139332205057144, 0.02343674562871456, 0.025144271552562714, 0.012804516591131687, 0.012000497430562973, 0.020912259817123413, 0.025003280490636826, 0.027692263945937157, 0.017305923625826836, 0.012859739363193512, 0.016066303476691246, 0.03279002755880356, 0.03829029202461243, 0.06080475449562073, 0.036749985069036484, 0.035247333347797394, 0.03857804834842682, 0.039048150181770325, 0.03664339706301689, 0.0145304249599576, 0.021632272750139236, 0.010490994900465012, 0.06733930855989456, 0.03867286816239357, 0.041227102279663086, 0.0, 0.0, 0.0], [0.13088132441043854, 0.02374432608485222, 0.02011433243751526, 0.009702395647764206, 0.026952238753437996, 0.013591106049716473, 0.009517529048025608, 0.013271704316139221, 0.02761792205274105, 0.011498377658426762, 0.008886142633855343, 0.016865799203515053, 0.009971978142857552, 0.031318604946136475, 0.01495381724089384, 0.01563659869134426, 0.018755894154310226, 0.008822466246783733, 0.0242583230137825, 0.018599873408675194, 0.06659714877605438, 0.023935336619615555, 0.030215006321668625, 0.038479700684547424, 0.06054789572954178, 0.03886096924543381, 0.02103957161307335, 0.026919949799776077, 0.030786704272031784, 0.0633939653635025, 0.05302293226122856, 0.07499751448631287, 0.016242528334259987, 0.0, 0.0], [0.21679525077342987, 0.03523939102888107, 0.014480572193861008, 0.03231681510806084, 0.02150271274149418, 0.019766850396990776, 0.007716964930295944, 0.03142578527331352, 0.01142125390470028, 0.03985079750418663, 0.0056975483894348145, 0.013826760463416576, 0.011217725463211536, 0.01102885976433754, 0.014342349953949451, 0.016691595315933228, 0.01380065642297268, 0.009054167196154594, 0.016551578417420387, 0.04516448825597763, 0.016743386164307594, 0.040494974702596664, 0.046006590127944946, 0.03918026387691498, 0.020771803334355354, 0.04241837561130524, 0.029444055631756783, 0.016546525061130524, 0.0433904193341732, 0.018028821796178818, 0.017493007704615593, 0.031954459846019745, 0.023179974406957626, 0.0264552254229784, 0.0], [0.12406569719314575, 0.055755238980054855, 0.02579311840236187, 0.007745691575109959, 0.019214723259210587, 0.011872394941747189, 0.009303727187216282, 0.015842614695429802, 0.01272842288017273, 0.012888253666460514, 0.013962890021502972, 0.01662726327776909, 0.015507099218666553, 0.013195221312344074, 0.01732509583234787, 0.021147985011339188, 0.017335504293441772, 0.015824301168322563, 0.024794375523924828, 0.011903030797839165, 0.036510247737169266, 0.023874735459685326, 0.040221355855464935, 0.0628603845834732, 0.10149352997541428, 0.053322598338127136, 0.02168961614370346, 0.01824427768588066, 0.008406980894505978, 0.02320232056081295, 0.0496041402220726, 0.020869331434369087, 0.016017958521842957, 0.011101946234703064, 0.049747928977012634]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10778022557497025, 0.8922198414802551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01184744294732809, 0.553614616394043, 0.4345379173755646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06322261691093445, 0.021719152107834816, 0.09427282959222794, 0.820785403251648, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005095739848911762, 0.0005348747945390642, 0.0010105469264090061, 0.004657561890780926, 0.9887012243270874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005585960578173399, 0.00030240125488489866, 0.0007955082692205906, 0.000678033335134387, 0.0287601500749588, 0.9638778567314148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00026657298440113664, 0.0001539619406685233, 4.344203625805676e-05, 0.0003163387009408325, 0.007582386024296284, 0.008635696955025196, 0.9830015897750854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006940615130588412, 4.8331738071283326e-05, 4.5113334635971114e-05, 2.3520775357610546e-05, 0.0022377653513103724, 0.0012080505257472396, 0.00932135060429573, 0.9864218235015869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.047719959169626236, 0.006346928421407938, 0.005507619120180607, 0.01277672965079546, 0.012552617117762566, 0.012200550176203251, 0.007760609034448862, 0.017114879563450813, 0.8780200481414795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.403799049323425e-05, 7.299120738935017e-07, 1.2948548828717321e-06, 1.0040580491477158e-06, 1.5192472346825525e-05, 3.6095214454689994e-05, 3.151440614601597e-05, 0.0010808276711031795, 4.236846143612638e-05, 0.9986969828605652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00029844269738532603, 1.0225186997558922e-05, 2.077383760479279e-05, 1.0716731594584417e-05, 7.453841681126505e-05, 0.00016580561350565404, 0.001366860931739211, 0.0025834839325398207, 5.0240600103279576e-05, 0.32439613342285156, 0.6710227727890015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009233690798282623, 0.0007820427417755127, 0.0006810438353568316, 0.0018207905814051628, 0.0043833935633301735, 0.017509153112769127, 0.003324250690639019, 0.017607431858778, 0.08099660277366638, 0.10943033546209335, 0.014291899278759956, 0.7399393916130066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023930237512104213, 3.3237884053960443e-06, 2.2832748072687536e-05, 1.0244727491226513e-05, 1.0253613254462834e-05, 4.496401015785523e-05, 2.3576618332299404e-05, 6.216613837750629e-05, 0.00032929476583376527, 0.008749540895223618, 0.0018788684392347932, 0.0014183437451720238, 0.9872072339057922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012295347638428211, 0.0005298632895573974, 0.0003560507029760629, 0.0006006720359437168, 0.0005327547551132739, 0.0005349720013327897, 0.00034733544453047216, 0.0008671184186823666, 0.03432309255003929, 0.0132736312225461, 0.007531206123530865, 0.0594179667532444, 0.09377525746822357, 0.7756146788597107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.850881229387596e-05, 1.1863348845508881e-05, 5.970536676613847e-06, 4.359465037850896e-06, 6.933054010005435e-06, 5.567320476984605e-05, 8.76204558153404e-06, 2.2130048819235526e-05, 0.0002223829651484266, 0.00028025032952427864, 0.0008127274340949953, 0.0015677290502935648, 0.0001895641180453822, 0.005879408214241266, 0.9908537864685059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.474973194301128e-05, 8.603436072007753e-06, 3.744844889297383e-06, 1.463416765545844e-07, 2.7017335924028885e-06, 1.1417326277296524e-05, 8.620798325864598e-06, 9.928076906362548e-05, 3.1194990697258618e-06, 0.00022620745585300028, 0.0005323346122168005, 2.9804801670252346e-05, 0.0013675616355612874, 5.107992546982132e-05, 0.009570821188390255, 0.9879898428916931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003944713156670332, 0.00013029153342358768, 9.947711077984422e-05, 0.00019776368571911007, 0.0004265851457603276, 0.0014889852609485388, 0.0002834686602000147, 0.0015356728108599782, 0.004810086917132139, 0.005567724350839853, 0.0010473597794771194, 0.04082653298974037, 0.009189032949507236, 0.0927850753068924, 0.015853000804781914, 0.027940917760133743, 0.7938733696937561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00021723605459555984, 2.3531203623861074e-05, 7.806276698829606e-06, 4.732958132080967e-06, 1.5671206483602873e-06, 8.667418114782777e-06, 1.2646246432268526e-05, 6.129271059762686e-05, 0.00018869421910494566, 0.0005901293479837477, 0.00042056679376401007, 0.0009746509022079408, 0.00042177739669568837, 0.004066709894686937, 0.001442242763005197, 0.011960022151470184, 0.022401951253414154, 0.9571956396102905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007360433810390532, 3.016823711732286e-06, 1.355472249997547e-06, 1.4014689213581732e-06, 3.387708420632407e-05, 3.752283737412654e-05, 1.0748690328910016e-05, 0.0004302704182919115, 1.6196730939554982e-05, 0.0006748255109414458, 0.0005265926592983305, 0.00012409141345415264, 4.7107158025028184e-05, 0.00024138158187270164, 0.0006886579212732613, 0.0003870207292493433, 0.0021441555581986904, 0.001765166176483035, 0.99213045835495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.529821803269442e-06, 4.013182230977463e-09, 4.275890752580835e-09, 9.052372185180957e-09, 5.135186142979364e-07, 3.731759534275625e-06, 1.4824270522240113e-07, 4.6551272134820465e-06, 1.663049964406582e-08, 3.029677600352443e-06, 3.191987343598157e-05, 1.4017453509040934e-07, 4.954938503942685e-07, 2.3671096016641968e-07, 7.952940563882294e-07, 0.000186073113582097, 2.446675580358715e-06, 1.198231075250078e-05, 0.0005082339630462229, 0.9992390871047974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009417897090315819, 0.00017362434300594032, 0.00019593215256463736, 0.0001054245512932539, 0.00012100120511604473, 0.00013750359357800335, 0.0002082234714180231, 0.0002620882005430758, 0.0009803815046325326, 0.00017425219994038343, 0.0013313948875293136, 0.003277312032878399, 0.0048406231217086315, 0.012378789484500885, 0.0018258976051583886, 0.024282056838274002, 0.054799728095531464, 0.07428492605686188, 0.07759274542331696, 0.10845340043306351, 0.625156819820404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00017886326531879604, 2.971782123495359e-06, 1.2879215773864416e-06, 1.6619675307083526e-06, 3.153397074129316e-06, 7.608545274706557e-06, 4.4805625520893955e-07, 4.628854185284581e-06, 4.4620751395996194e-06, 4.64080630990793e-06, 4.678128334489884e-06, 2.2255211661104113e-05, 0.0006885039038024843, 5.880849857931025e-05, 5.511296694749035e-05, 0.0013147704303264618, 0.0004210418264847249, 0.0005279290489852428, 0.0003059162409044802, 0.0011363023659214377, 0.00493514072149992, 0.9903197884559631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0009000833379104733, 3.4609143767738715e-05, 6.000097528158221e-06, 1.3317937828105642e-06, 8.382445230381563e-06, 7.950194230943453e-06, 8.352986355930625e-07, 5.01745898873196e-06, 1.1085571713920217e-05, 1.550617889733985e-05, 2.2449728930951096e-05, 3.239464058424346e-05, 7.153394835768268e-05, 0.0001110615921788849, 6.552087143063545e-06, 3.557858872227371e-05, 0.00046084352652542293, 0.0006021025474183261, 0.007735601160675287, 0.004731389693915844, 0.012920135632157326, 0.01699916645884514, 0.9552803635597229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.094511576928198e-05, 5.4195235861698166e-05, 2.1191000996623188e-05, 8.66031598434347e-07, 5.241395683697192e-06, 2.8113959160691593e-06, 9.647635579312919e-07, 4.259611614543246e-06, 6.819939244451234e-06, 2.637000079630525e-06, 1.0387340807938017e-05, 3.1177733035292476e-05, 2.5746971004991792e-05, 8.829332364257425e-05, 0.00010530064901104197, 0.00037444691406562924, 0.0005833787145093083, 0.000901932711713016, 0.0004312013916205615, 0.004991119261831045, 0.003515923162922263, 0.015127458609640598, 0.018384994938969612, 0.9552486538887024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00019926753884647042, 5.897880441807501e-07, 4.093071765964851e-07, 5.283640120978816e-07, 4.043727415137255e-07, 6.811453090449504e-07, 2.501958533684956e-07, 3.1909843301036744e-07, 5.0180010475742165e-06, 3.059933305848972e-06, 4.776871264766669e-06, 8.106227141979616e-06, 1.7870994270197116e-05, 5.904932913836092e-05, 7.019503300398355e-06, 5.101946953800507e-05, 0.00015419672126881778, 0.00015557752340100706, 0.00046309534809552133, 0.00018736722995527089, 0.0029354512225836515, 0.006998692639172077, 0.004895057994872332, 0.010302492417395115, 0.9735496640205383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.041899592266418e-05, 7.668172763430903e-09, 6.183969958328817e-08, 4.1099625036622456e-08, 3.1613484452464036e-07, 1.0004048789369335e-07, 6.658517293089972e-08, 4.780267204296251e-07, 4.035145551029018e-08, 6.753236448275857e-06, 8.574299670272012e-08, 1.0514232684499802e-07, 3.270282832090743e-06, 4.896555196864938e-07, 1.2450377653294709e-06, 9.069861334864981e-06, 2.0693153146567056e-06, 8.54171139508253e-06, 4.513625390245579e-05, 8.528179751010612e-05, 8.476278162561357e-05, 0.0010267929174005985, 0.0013347761705517769, 0.0002960072597488761, 0.005626787897199392, 0.9914274215698242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00037474490818567574, 1.2430058404788724e-07, 2.5229041966667864e-07, 3.052446118090302e-07, 4.7725070544402115e-06, 6.075425176277349e-07, 8.501697266183328e-07, 1.103080649045296e-05, 8.098967185787842e-08, 9.651266736909747e-07, 7.193338547040184e-07, 3.2339289646188263e-07, 1.760539589668042e-06, 4.4468094984040363e-07, 3.298750641533843e-07, 2.4300228687934577e-05, 3.6642647955886787e-06, 3.061631332457182e-06, 2.627120557008311e-05, 0.0003426705370657146, 2.0788122128578834e-05, 0.00013409697567112744, 0.0003047232748940587, 9.803599095903337e-05, 0.0004919670755043626, 0.004237059969455004, 0.9939160943031311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023197628615889698, 7.363628355960827e-07, 1.0658037581379176e-06, 4.74980652143131e-07, 2.1877904146094806e-06, 2.0741711068694713e-06, 1.3842077351000626e-07, 2.510645117581589e-06, 1.9715987775725807e-07, 6.671986739092972e-06, 8.991810318548232e-07, 6.36220249816688e-07, 6.544801749441831e-07, 7.71798795540235e-07, 4.801892714567657e-07, 1.1122294381493703e-05, 5.5619193517486565e-06, 3.862753146677278e-06, 6.75371557008475e-05, 0.0002897953672800213, 4.372502735350281e-05, 0.00015209022967610508, 0.0009337729425169528, 0.00018159417959395796, 8.423309918725863e-05, 0.00993384514003992, 0.008723181672394276, 0.9793181419372559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00015704914403613657, 3.123182921171974e-07, 2.931930964678031e-07, 3.137162707389507e-07, 2.1960941012366675e-06, 1.416439886270382e-06, 1.1236400041525485e-06, 1.3091771506879013e-06, 5.921491563753989e-08, 7.978065923452959e-07, 2.8036765797878616e-06, 1.976693795313622e-07, 3.9940141505212523e-07, 2.3049013009313057e-07, 2.2642504973191535e-06, 1.672949110798072e-05, 1.6667630688971258e-06, 2.872540562748327e-06, 2.4946893972810358e-05, 0.0002553278172854334, 4.849689958064118e-06, 4.7080560761969537e-05, 5.325664460542612e-05, 4.240337148075923e-05, 0.00020874854817520827, 0.0020438104402273893, 0.0834672600030899, 0.0036730007268488407, 0.909987211227417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010030783014371991, 7.605943210364785e-06, 5.478914772538701e-06, 4.704095772467554e-06, 1.0818274631674285e-06, 7.246855489029258e-07, 6.05722732416325e-07, 1.5847876966290642e-06, 3.183094668202102e-05, 1.8444867464495474e-06, 1.3723829397349618e-06, 6.841032245574752e-06, 4.996121333533665e-06, 0.0001422045024810359, 2.102602365994244e-06, 2.04611751541961e-05, 6.875484541524202e-05, 8.33943413454108e-05, 7.06085265846923e-05, 0.0002227009244961664, 0.001345351804047823, 0.001195314689539373, 0.0007637708331458271, 0.0034127680119127035, 0.009595129638910294, 0.008245964534580708, 0.004028420429676771, 0.015673475340008736, 0.03710908070206642, 0.916948676109314, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00014598089910577983, 1.3442496538118576e-07, 3.0679700557811884e-07, 1.270083913595954e-07, 9.539706269379167e-08, 4.4213727790065604e-08, 7.270972712802859e-09, 2.1630664548411005e-07, 1.786175545248625e-07, 1.1440989311495287e-07, 4.54669439875488e-08, 1.1406714861550427e-07, 1.7846329569692898e-07, 8.568713383283466e-07, 1.0497500113615388e-07, 1.247494537892635e-06, 1.1682705007842742e-06, 6.484989967248111e-07, 9.92306559055578e-06, 2.6635598260327242e-05, 2.148103158106096e-05, 7.192359043983743e-05, 0.00015873332449700683, 0.0001456325699109584, 0.002426169579848647, 0.0015144944190979004, 0.0007063191733323038, 0.0022442839108407497, 0.004566822201013565, 0.009832347743213177, 0.9781236052513123, 0.0, 0.0, 0.0, 0.0], [1.069878362613963e-05, 5.128429236833654e-08, 7.382501365782446e-08, 9.797533095934341e-08, 4.944888019053906e-07, 6.269736445574381e-07, 2.6257418994646287e-07, 7.361940106420661e-07, 2.5101161327256705e-07, 2.1764876692031976e-06, 5.363766604205011e-07, 8.606906476416043e-07, 2.1515788262149727e-07, 1.0358808140153997e-06, 2.4895064143493073e-07, 1.3481068890541792e-05, 8.521105883119162e-06, 5.148486707184929e-06, 4.645086846721824e-06, 1.683268965280149e-05, 5.86466740060132e-05, 3.652455416158773e-05, 0.0001648575853323564, 1.1945046026085038e-05, 0.0005173998652026057, 0.002427908359095454, 0.0003477365826256573, 0.00159640540368855, 0.009022938087582588, 0.013163739815354347, 0.06333832442760468, 0.9092466831207275, 0.0, 0.0, 0.0], [0.0012287308927625418, 8.407656423514709e-06, 8.363975211977959e-06, 8.199041076295543e-06, 3.972177637479035e-06, 3.987267973570852e-06, 1.5942509890010115e-06, 4.764995082950918e-06, 4.461909611563897e-06, 8.699198588146828e-06, 3.482310376057285e-06, 4.087221896043047e-06, 4.383184659673134e-06, 1.2878695997642353e-05, 2.764998953352915e-06, 1.1550841009011492e-05, 2.8508453397080302e-05, 1.9726185200852342e-05, 8.007969154277816e-05, 0.00016381562454625964, 0.0002766104298643768, 0.00048299715854227543, 0.0005903006531298161, 0.0011750623816624284, 0.0034667104482650757, 0.0058027333579957485, 0.011907558888196945, 0.016952957957983017, 0.028466496616601944, 0.06137577444314957, 0.11517589539289474, 0.201777383685112, 0.5509369373321533, 0.0, 0.0], [3.7825146137038246e-05, 6.459613643983175e-08, 1.92735836179736e-08, 7.975797444714772e-08, 6.700311132590286e-07, 1.246432361767802e-07, 3.072165100093116e-08, 3.36713952719947e-07, 9.953967605724756e-08, 4.5415114868774253e-07, 3.406023196816932e-08, 1.7256104456464527e-07, 3.3590092129998084e-07, 3.2563278296038334e-07, 1.1636606345177825e-08, 3.2042094062489923e-06, 1.2533513427115395e-06, 1.7663004427959095e-06, 1.1125951459689531e-05, 2.0410274373716675e-05, 8.103430445771664e-06, 1.288108524022391e-05, 0.00014193166862241924, 1.1411094419599976e-05, 0.00010671327618183568, 0.00010951114381896332, 0.001792709925211966, 0.0015404942678287625, 0.0012465205509215593, 0.0015975858550518751, 0.0008927042363211513, 0.009556993842124939, 0.009279929101467133, 0.9736242294311523, 0.0], [0.00035396942985244095, 2.644037294885493e-06, 1.7576104482941446e-06, 1.234809019479144e-06, 5.9497024267329834e-06, 1.3091830624034628e-06, 3.3620997896832705e-07, 2.485052732481563e-07, 6.705446367050172e-07, 6.699504240259557e-08, 6.061928843337228e-07, 4.6190532998480194e-07, 2.3573252292408142e-06, 1.4622189610236092e-06, 2.9763668862869963e-07, 1.6005789120754343e-06, 2.4609087176941102e-06, 1.931712631630944e-06, 3.2001476938603446e-05, 2.4651968487887643e-05, 3.4296379453735426e-05, 0.00016550840518902987, 7.883951184339821e-05, 0.00018387149611953646, 0.0011651507811620831, 0.00012255327601451427, 0.0011725201038643718, 0.0004984778934158385, 0.0035014813765883446, 0.003400747198611498, 0.011839503422379494, 0.005179082974791527, 0.016713835299015045, 0.029127653688192368, 0.9263803958892822]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9238101243972778, 0.07618988305330276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37589240074157715, 0.10050594061613083, 0.5236016511917114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41213729977607727, 0.0823158472776413, 0.1808476746082306, 0.32469913363456726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14052274823188782, 0.005812309216707945, 0.02827638015151024, 0.057330306619405746, 0.7680583596229553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019230658188462257, 0.0014746782835572958, 0.0016810770612210035, 0.004905834328383207, 0.02590656280517578, 0.9468011260032654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0060408818535506725, 0.004658223129808903, 0.012865060940384865, 0.008953888900578022, 0.07021692395210266, 0.12025363743305206, 0.7770113348960876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016248274594545364, 0.0039293826557695866, 0.01283666305243969, 0.005099623929709196, 0.017354188486933708, 0.015599334612488747, 0.1360277235507965, 0.7929048538208008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1342330127954483, 0.07014787942171097, 0.03862925246357918, 0.03592687100172043, 0.05720481276512146, 0.06419356167316437, 0.039089810103178024, 0.21457339823246002, 0.34600135684013367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0035958767402917147, 0.00038495013723149896, 0.0004921664367429912, 0.000808878627140075, 0.002590097952634096, 0.002407651860266924, 0.00017131817003246397, 0.011153006926178932, 0.0021147800143808126, 0.9762812852859497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00022834564151708037, 4.6662560635013506e-05, 8.206484199035913e-05, 7.730672950856388e-05, 0.00012034278188366443, 0.004319258499890566, 0.003844222752377391, 0.0011965015437453985, 0.00016446947120130062, 0.8530563116073608, 0.13686445355415344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06578907370567322, 0.019290756434202194, 0.020492708310484886, 0.021956782788038254, 0.026686670258641243, 0.08618340641260147, 0.031040921807289124, 0.161030113697052, 0.030162539333105087, 0.2132367044687271, 0.07554485648870468, 0.24858540296554565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009123343974351883, 0.003399313660338521, 0.01681826449930668, 0.004648808389902115, 0.014602272771298885, 0.058332499116659164, 0.07909007370471954, 0.01766359992325306, 0.01608726568520069, 0.1505357325077057, 0.3343390226364136, 0.05438896268606186, 0.24097087979316711, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04253435507416725, 0.01655058190226555, 0.007813414558768272, 0.006833462044596672, 0.009650968946516514, 0.009173315949738026, 0.00591031601652503, 0.030093085020780563, 0.05324168875813484, 0.03575889766216278, 0.11968675255775452, 0.07133747637271881, 0.29692214727401733, 0.29449349641799927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010158045217394829, 0.004377420525997877, 0.00650319829583168, 0.003546686377376318, 0.006157054100185633, 0.005149823613464832, 0.004158251918852329, 0.014994028024375439, 0.017724137753248215, 0.00860964972525835, 0.013557293452322483, 0.027689844369888306, 0.027776166796684265, 0.08518616855144501, 0.7644123435020447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005760938511230052, 3.758491948246956e-05, 3.100582762272097e-05, 2.8090940759284422e-05, 0.00011757229367503896, 0.0010626513976603746, 0.0008489798638038337, 0.0023644310422241688, 0.0003171183925587684, 0.0005040709511376917, 0.0005438972148112953, 0.0009964655619114637, 0.004787205718457699, 0.0016582146054133773, 0.02118113450706005, 0.9649454951286316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.057101134210824966, 0.012982714921236038, 0.012463349848985672, 0.01237119548022747, 0.01306475792080164, 0.035302504897117615, 0.013208814896643162, 0.06362835317850113, 0.010588889941573143, 0.061132002621889114, 0.027357207611203194, 0.08200794458389282, 0.03997458145022392, 0.043028347194194794, 0.06893416494131088, 0.0673389881849289, 0.37951505184173584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05038180202245712, 0.010253220796585083, 0.009017382748425007, 0.007853318005800247, 0.008382093161344528, 0.016030536964535713, 0.007579753641039133, 0.016568131744861603, 0.009558066725730896, 0.020098427310585976, 0.02802766114473343, 0.03172840178012848, 0.02179395966231823, 0.03942327946424484, 0.11988713592290878, 0.047830868512392044, 0.1477368026971817, 0.40784916281700134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009583797305822372, 0.0005122198490425944, 0.0006106935325078666, 0.0009717879584059119, 0.0022611033637076616, 0.0040412661619484425, 0.00033976067788898945, 0.0023014815524220467, 0.0014669810188934207, 0.004360878840088844, 0.010039581917226315, 0.0038837892934679985, 0.0019065355882048607, 0.004968368448317051, 0.0035998790990561247, 0.0027956562116742134, 0.015628812834620476, 0.014497028663754463, 0.9162304401397705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03377578780055046, 0.0008472921326756477, 0.0009361565462313592, 0.0006967561785131693, 0.004600139334797859, 0.0032576005905866623, 0.0004218360991217196, 0.007556702475994825, 0.0010570591548457742, 0.0036437935195863247, 0.00017823871166910976, 0.0020456179045140743, 0.0014812112785875797, 0.002918045502156019, 0.0056848167441785336, 0.06616660952568054, 0.006820716895163059, 0.005966105964034796, 0.022306203842163086, 0.829639196395874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014338117092847824, 0.0045062773860991, 0.0022934828884899616, 0.004502629861235619, 0.0063485209830105305, 0.006440696306526661, 0.0030759195797145367, 0.0034293767530471087, 0.008825039491057396, 0.013563881628215313, 0.008806230500340462, 0.015333057381212711, 0.010930357500910759, 0.03256243094801903, 0.04165012761950493, 0.029358427971601486, 0.06823207437992096, 0.10101238638162613, 0.12824414670467377, 0.08141805976629257, 0.41512882709503174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012516951188445091, 0.003707383992150426, 0.001132362405769527, 0.0020440153311938047, 0.00210379995405674, 0.0007217561360448599, 0.000358771241735667, 0.0010654572397470474, 0.0033418682869523764, 0.0035347675438970327, 0.0018008309416472912, 0.005131381563842297, 0.005956845358014107, 0.011681459844112396, 0.02335311844944954, 0.02462639845907688, 0.023082587867975235, 0.024053936824202538, 0.045054059475660324, 0.04447482153773308, 0.09848441183567047, 0.6617729663848877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023378076031804085, 0.0029700924642384052, 0.0022721639834344387, 0.0031212521716952324, 0.010448778048157692, 0.0027957092970609665, 0.0009500698070041835, 0.0066402810625731945, 0.0025092994328588247, 0.0062159025110304356, 0.0018756610807031393, 0.004366476088762283, 0.005894924979656935, 0.00790424644947052, 0.011842509731650352, 0.011780308559536934, 0.016934016719460487, 0.015606731176376343, 0.03699197620153427, 0.056762661784887314, 0.03885183483362198, 0.3010686933994293, 0.42881834506988525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.042250845581293106, 0.009647663682699203, 0.01306967344135046, 0.00349392369389534, 0.008718243800103664, 0.0008717633318156004, 0.0021130701061338186, 0.002520153997465968, 0.0028259274549782276, 0.003056258661672473, 0.005475994199514389, 0.004467588383704424, 0.00596621073782444, 0.008145660161972046, 0.02767091616988182, 0.013382636941969395, 0.018189135938882828, 0.0327802412211895, 0.028848394751548767, 0.0599595382809639, 0.0960593894124031, 0.2093193680047989, 0.10138262808322906, 0.2997848689556122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02581802010536194, 0.0012453016825020313, 0.00123677309602499, 0.0031558217015117407, 0.002605455694720149, 0.0011178323766216636, 0.0002305325906490907, 0.0010614546481519938, 0.0012552984990179539, 0.0014470185851678252, 0.0006083037587814033, 0.002144432393833995, 0.0017222751630470157, 0.0030636771116405725, 0.0021307130809873343, 0.001739516737870872, 0.007489567156881094, 0.004201178438961506, 0.016367290169000626, 0.017187539488077164, 0.016647031530737877, 0.06787319481372833, 0.0754612609744072, 0.04369648918509483, 0.7004939913749695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007508314214646816, 0.000129139210912399, 0.0002290404518134892, 0.0003099398745689541, 0.0015875763492658734, 0.00015740818344056606, 0.0003654663451015949, 0.00026319536846131086, 0.00024618778843432665, 0.00496384734287858, 0.000849328760523349, 0.0002054249052889645, 0.0035996593069285154, 0.0006757518276572227, 0.0006743194535374641, 0.0028660311363637447, 0.0007401985349133611, 0.0012410673080012202, 0.0034339427947998047, 0.003348809666931629, 0.005896294955164194, 0.015315262600779533, 0.013584453612565994, 0.0038060718216001987, 0.1058751791715622, 0.8221280574798584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015146820805966854, 0.0002837267820723355, 0.00043238894431851804, 0.0012392119970172644, 0.0026600691489875317, 0.00034595353645272553, 0.00029542643460445106, 0.0018772027688100934, 0.00023691231035627425, 0.00013219220272731036, 0.0003354801156092435, 0.00022997852647677064, 0.0002714899892453104, 0.0004209535545669496, 0.0008192574023269117, 0.0003039459406863898, 0.0006469364743679762, 0.00038042644155211747, 0.0023925937712192535, 0.0029767947271466255, 0.0025756102986633778, 0.00594179704785347, 0.005901205353438854, 0.005208776332437992, 0.025423677638173103, 0.023241210728883743, 0.9002800583839417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020882166922092438, 0.0006866251351311803, 0.001419029082171619, 0.0016247201710939407, 0.005466724745929241, 0.001993070589378476, 0.0003444554749876261, 0.0014866292476654053, 0.0003860277938656509, 0.0002986797480843961, 0.0003715484926942736, 0.0005332492874003947, 0.0002567079209256917, 0.0006083349580876529, 0.0003786248853430152, 0.0013748221099376678, 0.0013781340094283223, 0.0009236643672920763, 0.023375006392598152, 0.0377323254942894, 0.004063983913511038, 0.021846633404493332, 0.018512366339564323, 0.0050471932627260685, 0.018440254032611847, 0.06399478018283844, 0.3300282061100006, 0.43654605746269226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027297865599393845, 0.00011315701704006642, 0.00025743208243511617, 0.0003065292548853904, 0.000801237765699625, 0.0008870931342244148, 0.00010111553274327889, 0.0020485655404627323, 0.0001429120311513543, 0.00019898026948794723, 0.0004309819487389177, 7.435739826178178e-05, 0.00011077825183747336, 0.0002229756209999323, 0.0001040010538417846, 0.0007566184503957629, 0.00018428819021210074, 0.0003102098125964403, 0.0010590286692604423, 0.005394379608333111, 0.0013711386127397418, 0.0017238210421055555, 0.00343834119848907, 0.0007791533716954291, 0.002710790140554309, 0.01124658528715372, 0.2618617117404938, 0.0428645983338356, 0.6577694416046143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007169223856180906, 0.002651229267939925, 0.002445027930662036, 0.0030232490971684456, 0.002686190651729703, 0.0007135469932109118, 0.0005127684562467039, 0.0003864864120259881, 0.0009532018448226154, 0.00044078397331759334, 0.0008591757505200803, 0.0012491066008806229, 0.0009424777235835791, 0.0016617270885035396, 0.0004435320443008095, 0.0014648414216935635, 0.00357746216468513, 0.003767341375350952, 0.012122937478125095, 0.006853919476270676, 0.005249565001577139, 0.019704243168234825, 0.02666923590004444, 0.0409601628780365, 0.2385396808385849, 0.05725706368684769, 0.040405359119176865, 0.10933995991945267, 0.0773630291223526, 0.3305874168872833, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011626658961176872, 0.001965814270079136, 0.0009179530898109078, 0.0018197419121861458, 0.0020789767149835825, 0.0002935711818281561, 0.00010028554243035614, 0.0005799973732791841, 0.00048541263095103204, 9.76482915575616e-05, 0.0001571277534822002, 0.00041064247488975525, 0.00048777792835608125, 0.000751786632463336, 0.0007532840827479959, 0.0012242135126143694, 0.0010826848447322845, 0.0014821892837062478, 0.002774252789095044, 0.003610338317230344, 0.0021372397895902395, 0.009361456148326397, 0.014277664013206959, 0.01184671837836504, 0.04729338362812996, 0.0450628288090229, 0.04156067594885826, 0.04280795156955719, 0.07079780846834183, 0.1008482500910759, 0.5813056826591492, 0.0, 0.0, 0.0, 0.0], [0.0038734343834221363, 0.0002557095722295344, 0.0006259250803850591, 0.0005843958351761103, 0.0012326471041887999, 0.0003945597563870251, 0.0003326677833683789, 0.00015581918705720454, 0.0001647273456910625, 0.0007726291078142822, 0.00038669825880788267, 0.00027824463904835284, 0.00015239728963933885, 0.0002519858244340867, 6.452932575484738e-05, 0.00017264117195736617, 0.0007133586332201958, 0.000538479769602418, 0.0075395675376057625, 0.0067232511937618256, 0.0013963906094431877, 0.0031103563960641623, 0.005501282401382923, 0.0024135878775268793, 0.03896056115627289, 0.02342231571674347, 0.009641136974096298, 0.025750914588570595, 0.12702497839927673, 0.04005928337574005, 0.5142087936401367, 0.18329675495624542, 0.0, 0.0, 0.0], [0.015151114203035831, 0.0013760678702965379, 0.001556050730869174, 0.0021717094350606203, 0.002511007012799382, 0.0017854932229965925, 0.0004700863210018724, 0.0006788119790144265, 0.00038990683970041573, 0.0009900362929329276, 0.00038026587571948767, 0.0004182497796136886, 0.0004435285518411547, 0.0005140136345289648, 0.00016592223255429417, 0.0006932553369551897, 0.0009629505802877247, 0.0007656947709619999, 0.004365226253867149, 0.00643626693636179, 0.0018688250565901399, 0.008356149308383465, 0.006384752690792084, 0.00589070376008749, 0.031187018379569054, 0.04100942984223366, 0.028566066175699234, 0.041577283293008804, 0.04076460376381874, 0.06712847203016281, 0.30426710844039917, 0.21618826687335968, 0.16458560526371002, 0.0, 0.0], [0.01735411211848259, 0.000526471296325326, 0.0006208940758369863, 0.0016926313983276486, 0.006297257263213396, 0.0010202117264270782, 7.36255242372863e-05, 0.0006762268603779376, 0.00044483100646175444, 7.77887471485883e-05, 4.973809700459242e-05, 0.00030359727679751813, 0.00017341834609396756, 0.0005162440938875079, 0.00027352801407687366, 0.0007444003131240606, 0.0006022228626534343, 0.0004544499097391963, 0.0026653397362679243, 0.002663873601704836, 0.0025515935849398375, 0.007818798534572124, 0.00949097704142332, 0.0009830394992604852, 0.007624225690960884, 0.011552352458238602, 0.095797099173069, 0.08069910109043121, 0.09889823198318481, 0.03799612447619438, 0.0583561547100544, 0.11718904972076416, 0.11656812578439713, 0.31724435091018677, 0.0], [0.010796323418617249, 0.0030184579081833363, 0.0017010794254019856, 0.0027613944839686155, 0.005546443164348602, 0.0005509481998160481, 0.00022208337031770498, 0.00023305854119826108, 0.0008555073873139918, 8.429386070929468e-05, 0.0001349102531094104, 0.0004030338895972818, 0.00021568550437223166, 0.0009122327319346368, 0.0007261995924636722, 0.00038188460166566074, 0.0007406800868920982, 0.0007301223813556135, 0.0007852283888496459, 0.0013473378494381905, 0.004313784651458263, 0.0077291750349104404, 0.005422805901616812, 0.006747073028236628, 0.01398198027163744, 0.004630990792065859, 0.019391797482967377, 0.02813372015953064, 0.016861921176314354, 0.07724323123693466, 0.054351553320884705, 0.13139018416404724, 0.15014061331748962, 0.08434385061264038, 0.36317047476768494]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19326065480709076, 0.806739330291748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09132375568151474, 0.0022218863014131784, 0.9064543843269348, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07127523422241211, 0.010394570417702198, 0.014608718454837799, 0.9037215113639832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007525191176682711, 8.942779095377773e-05, 7.544146501459181e-05, 3.4135869100282434e-06, 0.9923065304756165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004521043971180916, 4.280492703401251e-06, 9.23039042390883e-05, 2.363623025303241e-05, 1.1194380022061523e-05, 0.9953475594520569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00793814193457365, 0.00010465254308655858, 4.188244201941416e-05, 8.376279765798245e-06, 0.00021111403475515544, 3.129790229650098e-06, 0.9916926622390747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024528484791517258, 1.0800506970554125e-05, 1.5410727428388782e-05, 2.2521548714848905e-07, 3.925120472558774e-05, 5.133605554874521e-06, 2.3800475901225582e-05, 0.9974525570869446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19944852590560913, 0.1783181130886078, 0.06548020988702774, 0.1327645480632782, 0.03858448565006256, 0.010353402234613895, 0.021433580666780472, 0.006723284255713224, 0.3468937873840332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014458488440141082, 8.054719273786759e-07, 3.064594784518704e-05, 7.412935474349069e-07, 2.02740625354636e-06, 1.1259067832725123e-05, 1.822341573642916e-06, 8.012940816115588e-05, 1.0569654307346354e-07, 0.9984266757965088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002338030841201544, 1.2498725482146256e-05, 2.245018731628079e-05, 4.236516360833775e-06, 2.952037618797476e-07, 1.8947142734759836e-06, 7.769895455567166e-05, 8.969089321908541e-06, 4.6952084886697776e-08, 1.1347700592523324e-06, 0.9975327253341675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04751458391547203, 0.05268266797065735, 0.020408131182193756, 0.0585300587117672, 0.007166564464569092, 0.008201424963772297, 0.011846395209431648, 0.0017474384512752295, 0.02496333420276642, 0.0026972556952387094, 0.0014261911856010556, 0.7628160715103149, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006070799194276333, 0.0001295789988944307, 5.9433408750919625e-05, 5.6593407862237655e-06, 0.00026153167709708214, 0.0001224183215526864, 1.039932612911798e-05, 1.1076372175011784e-05, 4.060476612721686e-07, 0.0003600460186135024, 2.011537435464561e-05, 1.3643895613313362e-07, 0.9929484128952026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09530571103096008, 0.14180323481559753, 0.05038182809948921, 0.10299669951200485, 0.029812078922986984, 0.006589414551854134, 0.01682438515126705, 0.0044818539172410965, 0.2310645431280136, 0.019435422495007515, 0.010874398052692413, 0.05543201044201851, 0.035438697785139084, 0.19955962896347046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027447840198874474, 0.0001406537339789793, 0.00019846689247060567, 3.4967764804605395e-05, 5.282158781483304e-06, 0.00015024232561700046, 8.128365880111232e-05, 1.2125195098633412e-05, 6.303905684035271e-05, 6.543220661114901e-05, 2.832498694260721e-06, 3.8907339330762625e-05, 5.673642249348632e-07, 4.210490806144662e-05, 0.9717161655426025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004212093539535999, 3.069671583944e-05, 6.369406764861196e-05, 8.254518206740613e-07, 8.26208690796193e-07, 3.868320709443651e-05, 4.268787051842082e-06, 3.819384801317938e-06, 1.2627954504296213e-07, 0.00013782686437480152, 1.7806012692744844e-05, 1.710013641798014e-08, 5.979855359328212e-06, 6.190274604023216e-08, 2.294146725034807e-06, 0.995481014251709, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019846780225634575, 0.031683411449193954, 0.01255327370017767, 0.03571881353855133, 0.004399008583277464, 0.004350572358816862, 0.007427363656461239, 0.0009372883359901607, 0.012662890367209911, 0.0016050542471930385, 0.000731996085960418, 0.4563409090042114, 0.002010870026424527, 0.009759278036653996, 0.008237497881054878, 0.00025941271451301873, 0.3914755880832672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027297092601656914, 0.002435697941109538, 0.00027068096096627414, 7.525548426201567e-05, 1.3686794773093425e-05, 5.773205430159578e-06, 0.00043018197175115347, 1.4438087418966461e-05, 4.835123036173172e-05, 5.326086829882115e-05, 9.613263500796165e-06, 6.846668111393228e-05, 0.0003037670685444027, 2.8783806556020863e-05, 3.866383121930994e-05, 2.5926743546733633e-05, 4.6800287236692384e-05, 0.9688335061073303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006310704629868269, 3.2946947612799704e-05, 9.895703442452941e-06, 7.334605243158876e-07, 9.119858441408724e-05, 4.094940595678054e-05, 8.511728083249182e-06, 9.950739331543446e-05, 1.9264869877133606e-07, 8.829243824948207e-07, 1.8906158629761194e-06, 1.1808739373009303e-06, 3.116527977908845e-06, 9.474784690155502e-08, 3.3017638543242356e-06, 2.5855348212644458e-05, 7.711328748882806e-07, 2.670873300303356e-06, 0.993365466594696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011340531054884195, 6.961915460124146e-06, 1.2364363101369236e-05, 4.0065501138997206e-07, 5.351809159037657e-05, 5.240101017989218e-06, 4.459504998521879e-05, 0.0001645077863940969, 1.4344476717553789e-08, 1.0387998372607399e-06, 4.7788784286240116e-05, 5.058673835378613e-08, 9.46820875924459e-07, 7.47566186731774e-09, 6.6039274315699e-07, 3.843121703539509e-06, 3.346431398654204e-08, 2.411975685845391e-07, 1.664775118115358e-05, 0.9985070824623108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08800853043794632, 0.18921299278736115, 0.04556255042552948, 0.08163290470838547, 0.017358072102069855, 0.004830399062484503, 0.01895877905189991, 0.00462013203650713, 0.06910265982151031, 0.012970475479960442, 0.010343022644519806, 0.05366412550210953, 0.009830269031226635, 0.05797470733523369, 0.011936401017010212, 0.009446632117033005, 0.04662337154150009, 0.07882607728242874, 0.010427407920360565, 0.0020946746226400137, 0.17657588422298431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015971172600984573, 0.0030620144680142403, 0.00014189491048455238, 0.0007885731174610555, 4.299363718018867e-05, 4.131193054490723e-05, 6.30684953648597e-05, 3.5465084238239797e-06, 2.1731255401391536e-05, 1.786092070688028e-05, 1.0213968380412553e-05, 2.6596329917083494e-05, 3.7855443224543706e-05, 1.3186833712097723e-05, 0.0001881760690594092, 4.9638774726190604e-06, 1.808506203815341e-05, 4.86401331727393e-05, 4.463582627067808e-06, 2.48024571192218e-06, 1.2178892575320788e-05, 0.9794788956642151, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007472235709428787, 0.0003719073720276356, 7.639070827281103e-05, 1.113591315515805e-05, 3.0148710720823146e-05, 1.1014438314305153e-05, 5.452878031064756e-05, 2.244437564513646e-05, 1.1283857020316646e-05, 2.858786865544971e-05, 5.627744485536823e-06, 7.51361294533126e-05, 7.734294194960967e-05, 6.529717666126089e-06, 8.796924703347031e-06, 8.651510142954066e-06, 5.072791827842593e-05, 7.040862328722142e-06, 3.187638139934279e-05, 1.4034893638381618e-06, 7.003594419074943e-06, 7.040080527076498e-05, 0.9915596842765808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.022045405581593513, 0.2804947793483734, 0.016674764454364777, 0.00015087124484125525, 0.000680446857586503, 2.7394116841605864e-05, 0.0008938327082432806, 1.249920478585409e-05, 0.0002665529609657824, 5.5841610446805134e-06, 5.109102130518295e-05, 9.50042376643978e-05, 1.8194788935943507e-05, 0.00018282792007084936, 0.0016493586590513587, 2.7935189791605808e-05, 6.991904228925705e-05, 7.677559915464371e-05, 2.088614746753592e-05, 4.512563009484438e-06, 0.00014541379641741514, 0.00044704912579618394, 5.5147760576801375e-05, 0.6759037375450134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018799467012286186, 0.0019184618722647429, 0.0010523863602429628, 7.228153117466718e-05, 0.00013796599523629993, 5.2088158554397523e-05, 6.334375211736187e-05, 9.731957106851041e-06, 8.120811253320426e-05, 5.998387496219948e-05, 9.635974492994137e-06, 4.818717934540473e-05, 4.688356784754433e-05, 5.250575486570597e-05, 0.00044942041859030724, 0.0002893299388233572, 3.5371991543797776e-05, 0.00022377951245289296, 0.0002020896936301142, 3.112003923888551e-06, 3.522854967741296e-05, 9.854993550106883e-05, 0.0001810147223295644, 0.00012300792150199413, 0.9759548306465149, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016589798033237457, 6.632464192080079e-06, 3.9323436794802547e-05, 5.512135885510361e-06, 7.328272477025166e-05, 5.176194690648117e-07, 8.362664084415883e-05, 1.6115391190396622e-05, 1.840794965346504e-07, 0.00020032576867379248, 1.849748059612466e-06, 7.683036074013216e-08, 1.805103056540247e-05, 9.2597367995495e-08, 6.612178822251735e-06, 1.9627825622592354e-06, 5.0048470257024746e-08, 6.371125493842555e-08, 4.881635504716542e-07, 3.8391394809877966e-06, 1.8601461704292888e-08, 1.6708657994968235e-06, 8.81659104834398e-07, 6.673329977502362e-08, 1.8785125632803101e-07, 0.9978796243667603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005040658637881279, 0.00039155513513833284, 0.0005618694121949375, 0.0001452302240068093, 3.9371847378788516e-05, 5.409949881141074e-05, 0.0002847950381692499, 4.8287358367815614e-05, 3.10431369143771e-06, 6.896255217725411e-05, 0.0002681412152014673, 4.210920906189131e-06, 1.2119860912207514e-05, 2.0253935417713365e-06, 3.188769915141165e-05, 1.1356515642546583e-05, 3.0187948141247034e-06, 1.2622882422874682e-05, 7.465569979103748e-06, 3.027799039045931e-06, 4.951110668116598e-07, 3.0154526029946283e-05, 1.8863744116970338e-05, 4.151242592342896e-06, 5.031317414250225e-06, 6.069065875635715e-06, 0.9929414987564087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012448630295693874, 0.00017617006960790604, 2.4151706384145655e-05, 1.2456656804715749e-05, 5.2112987759755924e-05, 2.3546019292552955e-05, 5.6734352256171405e-05, 0.000458276248537004, 2.498383537385962e-06, 0.0001134940903284587, 4.92024228151422e-05, 1.3444526302919257e-05, 7.735934195807204e-05, 1.343705889667035e-06, 6.0608936109929346e-06, 0.00010331773228244856, 8.74546640261542e-06, 1.0811555512191262e-05, 0.00015856359095778316, 1.9056030851061223e-06, 5.418779096544313e-07, 1.23213567349012e-06, 6.472226232290268e-05, 3.5890500384994084e-06, 8.650893505546264e-06, 2.962798134831246e-06, 1.3278935512062162e-05, 0.9973099231719971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0019733370281755924, 0.000975057715550065, 7.218527753138915e-05, 9.848424087977037e-05, 4.960871592629701e-05, 5.4340667702490464e-05, 0.000379721139324829, 6.184644007589668e-06, 1.0831167855940294e-06, 4.491214440349722e-06, 0.0004798363079316914, 9.928667168424e-07, 1.3564570508606266e-05, 6.477196734522295e-07, 1.0186121471633669e-05, 5.581342520599719e-06, 7.113490596566407e-07, 5.998076062496693e-07, 3.201096433258499e-06, 2.288287987539661e-06, 1.513085550186588e-07, 7.1307663347397465e-06, 1.793800606719742e-06, 2.402577047178056e-05, 2.833120333889383e-07, 1.226913900609361e-05, 0.00023325484653469175, 0.00031319662230089307, 0.9952757358551025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03009931929409504, 0.09027383476495743, 0.028149722144007683, 0.15763871371746063, 0.026473550125956535, 0.0030060105491429567, 0.017219530418515205, 0.002244670409709215, 0.03120047226548195, 0.01135173998773098, 0.0013561327941715717, 0.02911919727921486, 0.016773536801338196, 0.023998122662305832, 0.009450537152588367, 0.006288208067417145, 0.02392304688692093, 0.05829622596502304, 0.009904232807457447, 0.00037797351251356304, 0.02459515444934368, 0.04042006656527519, 0.027886763215065002, 0.009823575615882874, 0.04994712769985199, 0.001065115095116198, 0.01463253516703844, 0.011246390640735626, 0.0020601553842425346, 0.24117836356163025, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0041694133542478085, 0.0006143272621557117, 0.000593845616094768, 0.000110362030682154, 9.005493484437466e-05, 1.3705133824259974e-05, 0.0005903472192585468, 2.689225539143081e-06, 8.671486284583807e-06, 4.6116256271488965e-05, 1.0056769497168716e-05, 5.3628987188858446e-06, 5.199846054892987e-05, 4.802404873771593e-06, 2.119520831911359e-05, 2.4785002096905373e-05, 3.4400347885821247e-06, 1.8410799384582788e-05, 7.055131572997198e-05, 5.60396472337743e-07, 5.06017067891662e-06, 2.2592088498640805e-05, 6.252533057704568e-05, 1.8619313777890056e-05, 0.00014118532999418676, 2.6331037588533945e-05, 1.2610411204150296e-06, 1.075982140719134e-06, 3.0931920491639175e-07, 5.793043783342e-06, 0.9932645559310913, 0.0, 0.0, 0.0, 0.0], [0.010566660203039646, 0.0013905888190492988, 0.00043136317981407046, 0.00102959293872118, 0.003916398622095585, 0.00015029506175778806, 0.0019935970194637775, 0.0003044370678253472, 2.984509410453029e-05, 0.0006592917488887906, 1.265983701159712e-05, 7.527502748416737e-05, 0.0003976769803557545, 1.7276539438171312e-05, 0.00038583436980843544, 2.1061901861685328e-05, 5.340308780432679e-05, 6.33502786513418e-05, 0.000126884420751594, 6.783195203752257e-06, 2.7170561224920675e-05, 0.0001300896255997941, 3.859859498334117e-05, 4.366646317066625e-05, 8.064090252446476e-06, 0.0007468663388863206, 8.195245754905045e-06, 9.368611063109711e-05, 6.252453658817103e-06, 4.44724500994198e-05, 0.00020698650041595101, 0.9770136475563049, 0.0, 0.0, 0.0], [0.01713147759437561, 0.10981207340955734, 0.04528496786952019, 0.16663989424705505, 0.04683491960167885, 0.012714344076812267, 0.02494181878864765, 0.0024605041835457087, 0.01651417650282383, 0.014018191024661064, 0.002804650692269206, 0.017261292785406113, 0.056191347539424896, 0.011843179352581501, 0.02451840229332447, 0.008004880510270596, 0.013582986779510975, 0.027696531265974045, 0.007881492376327515, 0.001375963562168181, 0.00903363712131977, 0.037480004131793976, 0.014368829317390919, 0.016579674556851387, 0.03148626163601875, 0.0022351096849888563, 0.00703256344422698, 0.020338818430900574, 0.0021573668345808983, 0.015735741704702377, 0.03529256209731102, 0.017795002087950706, 0.16295135021209717, 0.0, 0.0], [0.0027362399268895388, 0.0032704449258744717, 0.000249385426286608, 2.9521164833568037e-05, 0.0027190803084522486, 1.5680845535825938e-05, 0.0011440073139965534, 3.5285604099044576e-05, 1.9352108211023733e-05, 3.620967618189752e-05, 1.2202253856230527e-05, 0.00012145661457907408, 4.955609256285243e-05, 1.1851511771965306e-05, 8.826630801195279e-05, 3.184528031852096e-05, 8.618915308034047e-05, 0.0001305781042901799, 9.267554560210556e-05, 1.8444527086103335e-05, 4.454891495697666e-06, 1.4211048437573481e-05, 0.0002081982820527628, 6.224938988452777e-05, 2.7289963327348232e-05, 8.527080353815109e-06, 0.001093172118999064, 0.001025900593958795, 1.8266431652591564e-05, 9.816331839829218e-06, 7.599973059768672e-07, 4.15181375501561e-06, 8.365900612261612e-06, 0.9866164922714233, 0.0], [0.013301734812557697, 0.0020241746678948402, 0.000981063349172473, 0.0002710390544962138, 0.02624250203371048, 6.968397065065801e-05, 0.0001882457872852683, 0.0003323268028907478, 0.00020717468578368425, 0.000355835392838344, 1.8688766431296244e-05, 0.00031016304274089634, 0.00028844171902164817, 0.0001415754231857136, 0.00021018910047132522, 0.00013730101636610925, 0.0002454249479342252, 0.000585862435400486, 0.000840906344819814, 0.000378983560949564, 0.00011255494609940797, 0.0009485261398367584, 0.00025817163987085223, 0.0005371624720282853, 0.00023706370848231018, 9.896130359265953e-05, 1.4541798918799032e-05, 7.514355093007907e-05, 1.0142424798686989e-05, 7.690754864597693e-05, 6.643269443884492e-05, 5.445994975161739e-05, 0.0001078199056792073, 0.00218911818228662, 0.9480817317962646]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360752105712891, 0.06392484903335571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8446659445762634, 0.06946856528520584, 0.0858655497431755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6426600813865662, 0.1298869252204895, 0.1711716651916504, 0.05628129839897156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5074959993362427, 0.07786273211240768, 0.0726853609085083, 0.10711847990751266, 0.23483748733997345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4970755875110626, 0.06315260380506516, 0.1155911386013031, 0.08851395547389984, 0.10196362435817719, 0.13370312750339508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3081281781196594, 0.06775976717472076, 0.13057540357112885, 0.05440935119986534, 0.04050319641828537, 0.3605913519859314, 0.038032736629247665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3243498206138611, 0.06349693238735199, 0.07783647626638412, 0.07798171788454056, 0.07410471886396408, 0.13590547442436218, 0.0703885555267334, 0.1759362518787384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16412918269634247, 0.03469286486506462, 0.05131254345178604, 0.013317004777491093, 0.17712706327438354, 0.15951891243457794, 0.09812311083078384, 0.2969047427177429, 0.0048745544627308846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18775463104248047, 0.05488047003746033, 0.10758273303508759, 0.04391003027558327, 0.1406543254852295, 0.11696727573871613, 0.052460018545389175, 0.1536639779806137, 0.04935270547866821, 0.09277380257844925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.205171138048172, 0.040068794041872025, 0.0833922028541565, 0.04565276578068733, 0.05697013810276985, 0.1331988424062729, 0.026416398584842682, 0.09950854629278183, 0.05013458803296089, 0.2141735702753067, 0.045312944799661636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10754455626010895, 0.020955445244908333, 0.052468422800302505, 0.008330848067998886, 0.17997516691684723, 0.10652029514312744, 0.07210807502269745, 0.20191136002540588, 0.003346042474731803, 0.14021332561969757, 0.10356747359037399, 0.003058959497138858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09423349052667618, 0.02283540740609169, 0.05558855086565018, 0.026967832818627357, 0.12799426913261414, 0.07631105929613113, 0.07361151278018951, 0.16634690761566162, 0.030196353793144226, 0.11622441560029984, 0.13079150021076202, 0.022433560341596603, 0.05646514892578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11027700453996658, 0.019917313009500504, 0.03387210890650749, 0.00801222026348114, 0.1268695741891861, 0.12052717059850693, 0.06604592502117157, 0.211595818400383, 0.0029103776905685663, 0.13361382484436035, 0.0783042386174202, 0.005965671967715025, 0.07863972336053848, 0.0034490139223635197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1600770205259323, 0.05452115833759308, 0.0597362294793129, 0.020207518711686134, 0.052660416811704636, 0.10011065006256104, 0.1172926276922226, 0.14998747408390045, 0.017248356714844704, 0.056252799928188324, 0.0944204032421112, 0.01204691082239151, 0.020297687500715256, 0.018244566395878792, 0.06689615547657013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17850065231323242, 0.04271137714385986, 0.058762889355421066, 0.02550153061747551, 0.02821972593665123, 0.0939594954252243, 0.035697706043720245, 0.03853301331400871, 0.02757410891354084, 0.12417211383581161, 0.09900188446044922, 0.029919540509581566, 0.041689127683639526, 0.03003854863345623, 0.08029218018054962, 0.06542609632015228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08262611925601959, 0.014221644960343838, 0.03910330310463905, 0.005809779744595289, 0.14161060750484467, 0.08671100437641144, 0.05503496900200844, 0.15626993775367737, 0.002328180940821767, 0.11679328233003616, 0.08067948371171951, 0.002191798761487007, 0.09792855381965637, 0.002736865309998393, 0.032974518835544586, 0.0803784504532814, 0.0026015001349151134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09858328849077225, 0.0283206719905138, 0.05263177305459976, 0.01785864308476448, 0.06163660064339638, 0.08070769160985947, 0.07397957146167755, 0.09560468792915344, 0.009335816837847233, 0.08467234671115875, 0.11269159615039825, 0.018467877060174942, 0.038562607020139694, 0.011051509529352188, 0.06964196264743805, 0.09237800538539886, 0.02256043255329132, 0.03131495788693428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1495792269706726, 0.013048059307038784, 0.033948514610528946, 0.015061762183904648, 0.045098841190338135, 0.0847996324300766, 0.02725698985159397, 0.17206290364265442, 0.016850341111421585, 0.10883240401744843, 0.11793842911720276, 0.008323254995048046, 0.016134319826960564, 0.017726456746459007, 0.026173433288931847, 0.05189086124300957, 0.008976077660918236, 0.010605720803141594, 0.07569283246994019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2303973287343979, 0.024296829476952553, 0.026559969410300255, 0.03924616798758507, 0.04230227321386337, 0.04924339801073074, 0.0079794405028224, 0.07322089374065399, 0.03455185890197754, 0.06601925939321518, 0.038301680237054825, 0.03672320768237114, 0.0241401307284832, 0.0371052622795105, 0.028557149693369865, 0.054463908076286316, 0.03901509940624237, 0.01594439148902893, 0.04610010236501694, 0.0858316645026207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05691661313176155, 0.0062250238843262196, 0.011754428036510944, 0.0035496761556714773, 0.07110726088285446, 0.08006572723388672, 0.022162359207868576, 0.11016803234815598, 0.0010484940139576793, 0.08122198283672333, 0.026672089472413063, 0.0021960814483463764, 0.030598536133766174, 0.0012014057720080018, 0.014937776140868664, 0.14736883342266083, 0.0026267566718161106, 0.03338917717337608, 0.11049019545316696, 0.18504434823989868, 0.0012552265543490648, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09241703897714615, 0.026059621945023537, 0.024830598384141922, 0.014753975905478, 0.07309960573911667, 0.04160080477595329, 0.02818295545876026, 0.10522369295358658, 0.010200566612184048, 0.026716867461800575, 0.03863626718521118, 0.008342116139829159, 0.034165360033512115, 0.011011882685124874, 0.02702195569872856, 0.10600947588682175, 0.009498468600213528, 0.01612309366464615, 0.06257016956806183, 0.19003157317638397, 0.010448911227285862, 0.04305500537157059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.061736516654491425, 0.007029627915471792, 0.01948658376932144, 0.010366824455559254, 0.04601288586854935, 0.07236861437559128, 0.02275238372385502, 0.08978494256734848, 0.005041899625211954, 0.0701901763677597, 0.07947878539562225, 0.006475384812802076, 0.0383506715297699, 0.005680918227881193, 0.015840984880924225, 0.05344104766845703, 0.007524985354393721, 0.018862154334783554, 0.09784606844186783, 0.19087882339954376, 0.0069852969609200954, 0.027727806940674782, 0.04613659903407097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.043498456478118896, 0.006029138807207346, 0.016271810978651047, 0.004962891805917025, 0.09232776612043381, 0.05643608421087265, 0.024273211136460304, 0.10004423558712006, 0.0013475847663357854, 0.04695265367627144, 0.04337000846862793, 0.0026120906695723534, 0.02598002180457115, 0.0015367761952802539, 0.013811735436320305, 0.121625155210495, 0.0030840609688311815, 0.014781150035560131, 0.08252862095832825, 0.20843426883220673, 0.0021058102138340473, 0.03356638178229332, 0.04992128163576126, 0.004498816095292568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05493932217359543, 0.005164691712707281, 0.013282537460327148, 0.006816804874688387, 0.08581960201263428, 0.042583663016557693, 0.010111804120242596, 0.10901323705911636, 0.003067039418965578, 0.041951462626457214, 0.03668753057718277, 0.0038776027504354715, 0.03639628738164902, 0.0036428270395845175, 0.01571309007704258, 0.04514278843998909, 0.004770633764564991, 0.010733412578701973, 0.12949086725711823, 0.18343372642993927, 0.005763910710811615, 0.06008811295032501, 0.0642036721110344, 0.00847991369664669, 0.018825512379407883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11619282513856888, 0.024008197709918022, 0.017500853165984154, 0.0227952990680933, 0.05027654021978378, 0.032025471329689026, 0.020647743716835976, 0.0397474430501461, 0.02373599074780941, 0.026677899062633514, 0.018856428563594818, 0.028171364217996597, 0.034608807414770126, 0.02664371393620968, 0.022768085822463036, 0.023577360436320305, 0.031199196353554726, 0.022620433941483498, 0.09706804901361465, 0.06436185538768768, 0.03251432999968529, 0.04557202011346817, 0.04268259182572365, 0.028252853080630302, 0.025809550657868385, 0.08168511837720871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11220825463533401, 0.005275930278003216, 0.013701365329325199, 0.006475639995187521, 0.058257486671209335, 0.055918335914611816, 0.013027261011302471, 0.10451262444257736, 0.004013675730675459, 0.0401136577129364, 0.037872303277254105, 0.003174532437697053, 0.01212999690324068, 0.0040962593629956245, 0.007334527559578419, 0.01773281767964363, 0.0034752385690808296, 0.0030595576390624046, 0.09483391791582108, 0.20002642273902893, 0.005673971958458424, 0.016374556347727776, 0.01723022572696209, 0.0043684346601367, 0.009308062493801117, 0.09713456779718399, 0.05267038941383362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.061152681708335876, 0.004939326085150242, 0.00976282637566328, 0.0052271620370447636, 0.03646275773644447, 0.03137052059173584, 0.008218890056014061, 0.11535762995481491, 0.003272865666076541, 0.038249194622039795, 0.033371780067682266, 0.002179377479478717, 0.007692957296967506, 0.003454803256317973, 0.009085437282919884, 0.027900507673621178, 0.0023665931075811386, 0.003948240075260401, 0.0407714918255806, 0.2757468819618225, 0.00354666355997324, 0.012455099262297153, 0.03261313959956169, 0.0032098167575895786, 0.003603094257414341, 0.10009617358446121, 0.08892863988876343, 0.03501543402671814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11957039684057236, 0.008050513453781605, 0.01337728463113308, 0.007089267484843731, 0.025275403633713722, 0.05149082839488983, 0.006585441995412111, 0.06165985390543938, 0.0056204828433692455, 0.055614862591028214, 0.027291804552078247, 0.004286487586796284, 0.007698431611061096, 0.005996250547468662, 0.0079420180991292, 0.02879050001502037, 0.004870969336479902, 0.0043137590400874615, 0.026228077709674835, 0.15385550260543823, 0.010709961876273155, 0.009564648382365704, 0.029178405180573463, 0.005329275969415903, 0.008051514625549316, 0.07812704890966415, 0.10984007269144058, 0.029521025717258453, 0.09406991302967072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02588072419166565, 0.0032868750859051943, 0.009190607815980911, 0.002271317644044757, 0.04206257313489914, 0.04035560414195061, 0.015107085928320885, 0.04801337420940399, 0.0008096114615909755, 0.039064206182956696, 0.02656683139503002, 0.0015204483643174171, 0.021220142021775246, 0.0009539549937471747, 0.007810372859239578, 0.03313516080379486, 0.001879250630736351, 0.010119684040546417, 0.06751004606485367, 0.1226290911436081, 0.0015832213684916496, 0.02216801792383194, 0.030785206705331802, 0.0043188100680708885, 0.018729008734226227, 0.1447751671075821, 0.06957435607910156, 0.11388751864433289, 0.07182326912879944, 0.002968472894281149, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04612509161233902, 0.004929374437779188, 0.011905020102858543, 0.005939907394349575, 0.05086231231689453, 0.039155829697847366, 0.01170225441455841, 0.07777240872383118, 0.0027224919758737087, 0.029330719262361526, 0.035436615347862244, 0.0025086572859436274, 0.011883245781064034, 0.0029638567939400673, 0.008878659456968307, 0.01893666759133339, 0.002804587595164776, 0.004066928755491972, 0.06637169420719147, 0.16173601150512695, 0.004074737895280123, 0.017417892813682556, 0.048458147794008255, 0.005489849019795656, 0.014467370696365833, 0.08884268254041672, 0.05744142085313797, 0.04815755784511566, 0.09182101488113403, 0.004751015920192003, 0.023045996204018593, 0.0, 0.0, 0.0, 0.0], [0.0286165252327919, 0.0035397172905504704, 0.012825754471123219, 0.003731105010956526, 0.02213282696902752, 0.026339080184698105, 0.015483640134334564, 0.04692448303103447, 0.0016770190559327602, 0.053430914878845215, 0.03818729147315025, 0.00252948934212327, 0.027682237327098846, 0.0019447176018729806, 0.010427961125969887, 0.020306402817368507, 0.0030831180047243834, 0.00833755824714899, 0.06147446855902672, 0.056954704225063324, 0.0030909168999642134, 0.019787278026342392, 0.037704262882471085, 0.004031909629702568, 0.020770886912941933, 0.1212616041302681, 0.08733709901571274, 0.07036232948303223, 0.09933114796876907, 0.005798673257231712, 0.0638149157166481, 0.021079931408166885, 0.0, 0.0, 0.0], [0.022769227623939514, 0.002363813342526555, 0.006953609175980091, 0.0009745138813741505, 0.0334312804043293, 0.0309631135314703, 0.012881086207926273, 0.0763968974351883, 0.0002621039457153529, 0.030449917539954185, 0.03674259036779404, 0.0005670604296028614, 0.014569043181836605, 0.0003055331180803478, 0.005948364268988371, 0.0224925484508276, 0.0007025575614534318, 0.004866446368396282, 0.060261886566877365, 0.155859112739563, 0.0005001766257919371, 0.019988076761364937, 0.03086848556995392, 0.003128150012344122, 0.012151172384619713, 0.11994649469852448, 0.07597397267818451, 0.09628812223672867, 0.07584252208471298, 0.0010643760906532407, 0.03271331265568733, 0.011111104860901833, 0.0006633000448346138, 0.0, 0.0], [0.06755812466144562, 0.003352795960381627, 0.008972212672233582, 0.0034025416243821383, 0.056073062121868134, 0.03040468879044056, 0.013628313317894936, 0.07952428609132767, 0.0025889414828270674, 0.03472939878702164, 0.015315975062549114, 0.0025271016638725996, 0.007685473654419184, 0.0026928281877189875, 0.007112996652722359, 0.026128452271223068, 0.0027733705937862396, 0.007069503888487816, 0.050762102007865906, 0.1759389340877533, 0.0039354367181658745, 0.01054285652935505, 0.012697340920567513, 0.002831441117450595, 0.004983488470315933, 0.05610368773341179, 0.10278656333684921, 0.07864400744438171, 0.06361377984285355, 0.004290643613785505, 0.012449044734239578, 0.007846895605325699, 0.0035741638857871294, 0.03745955228805542, 0.0], [0.05889429152011871, 0.004834293387830257, 0.008816240355372429, 0.004917599726468325, 0.05724450200796127, 0.04719964414834976, 0.010384355671703815, 0.04373684525489807, 0.003253214294090867, 0.027000529691576958, 0.015385999344289303, 0.005444496404379606, 0.018304407596588135, 0.0036100558936595917, 0.011856140568852425, 0.07255369424819946, 0.0060180798172950745, 0.00641189981251955, 0.055861588567495346, 0.08374672383069992, 0.003973665647208691, 0.018590586259961128, 0.019445113837718964, 0.003880675882101059, 0.008932656608521938, 0.12242712825536728, 0.0409863255918026, 0.07429766654968262, 0.05774553120136261, 0.006318636238574982, 0.013084111735224724, 0.023065803572535515, 0.005796517711132765, 0.04094139114022255, 0.015039583668112755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780336022377014, 0.021966392174363136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5068986415863037, 0.4279148280620575, 0.06518658250570297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3459503650665283, 0.2338511198759079, 0.19138815999031067, 0.22881034016609192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.278596967458725, 0.17525731027126312, 0.11037091910839081, 0.25180956721305847, 0.18396517634391785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20804201066493988, 0.08827143907546997, 0.06160811707377434, 0.2584840953350067, 0.270569771528244, 0.11302457004785538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16325616836547852, 0.12141193449497223, 0.11990712583065033, 0.16166438162326813, 0.10774070024490356, 0.2975037395954132, 0.028515947982668877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12280935794115067, 0.03978988155722618, 0.045145433396101, 0.1357850432395935, 0.09269553422927856, 0.11808720231056213, 0.3369845747947693, 0.10870297253131866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07854471355676651, 0.0357038751244545, 0.029105382040143013, 0.05200909078121185, 0.10322203487157822, 0.06222515553236008, 0.06801235675811768, 0.17029152810573578, 0.40088585019111633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.085553377866745, 0.045082103461027145, 0.02075529657304287, 0.057487912476062775, 0.058533743023872375, 0.038125406950712204, 0.03164026141166687, 0.14470812678337097, 0.3939673900604248, 0.12414638698101044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.034007567912340164, 0.005801304243505001, 0.004264353774487972, 0.014554576016962528, 0.002058125101029873, 0.010052229277789593, 0.015462441369891167, 0.014919919893145561, 0.07456015795469284, 0.8029415607452393, 0.0213778056204319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03658131882548332, 0.008552628569304943, 0.009085919708013535, 0.010718594305217266, 0.031510550528764725, 0.07786072790622711, 0.028099488466978073, 0.05203385651111603, 0.09175649285316467, 0.24980765581130981, 0.22384564578533173, 0.18014715611934662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03899503871798515, 0.013953045941889286, 0.027150392532348633, 0.01597864367067814, 0.00606886763125658, 0.01533291582018137, 0.00735845509916544, 0.008143596351146698, 0.09236099570989609, 0.09582383185625076, 0.39708954095840454, 0.22508268058300018, 0.05666199326515198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029417648911476135, 0.0068539888598024845, 0.005433531012386084, 0.008306888863444328, 0.015738297253847122, 0.008708903566002846, 0.008349093608558178, 0.023920075967907906, 0.04694688320159912, 0.04814501851797104, 0.09309988468885422, 0.15290863811969757, 0.14854183793067932, 0.40362945199012756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.037128347903490067, 0.007584471721202135, 0.0035451522562652826, 0.0081179765984416, 0.018834194168448448, 0.005904984660446644, 0.009632610715925694, 0.01107665617018938, 0.04249943047761917, 0.01508400496095419, 0.01984352432191372, 0.11177284270524979, 0.12816882133483887, 0.33317840099334717, 0.24762853980064392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024791475385427475, 0.002257239306345582, 0.0019112296868115664, 0.003149644238874316, 0.001774665666744113, 0.003080859547480941, 0.019880715757608414, 0.006427091546356678, 0.015317484736442566, 0.0032266047783195972, 0.02029605582356453, 0.05772728472948074, 0.022555220872163773, 0.08885946869850159, 0.7141124606132507, 0.014632428996264935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019352806732058525, 0.002735722344368696, 0.002700644312426448, 0.002816395368427038, 0.007300625555217266, 0.016268722712993622, 0.005134702194482088, 0.010258641093969345, 0.013791483826935291, 0.03380446508526802, 0.033944834023714066, 0.024110442027449608, 0.053474877029657364, 0.09691008925437927, 0.2761881947517395, 0.22190377116203308, 0.1793036013841629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01677827723324299, 0.0013836680445820093, 0.0012504433980211616, 0.001953265629708767, 0.0012283106334507465, 0.003427047049626708, 0.0017710181418806314, 0.0025876802392303944, 0.009956516325473785, 0.007537422236055136, 0.004963121842592955, 0.023668857291340828, 0.017456641420722008, 0.06967755407094955, 0.2612767815589905, 0.24082423746585846, 0.17677906155586243, 0.15748007595539093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05475044250488281, 0.009201958775520325, 0.0037833047099411488, 0.006891882978379726, 0.0070068733766674995, 0.0052677844651043415, 0.0019135881448164582, 0.0066490075550973415, 0.02249074913561344, 0.007203703746199608, 0.03985993564128876, 0.02630697935819626, 0.03598393127322197, 0.1065463200211525, 0.14742790162563324, 0.02953192964196205, 0.14834149181842804, 0.23425257205963135, 0.10658968985080719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08004872500896454, 0.003562747035175562, 0.003394149476662278, 0.011285565793514252, 0.012208583764731884, 0.0019434988498687744, 0.0039208270609378815, 0.024116605520248413, 0.023349974304437637, 0.008975885808467865, 0.005343952216207981, 0.025336090475320816, 0.01387018896639347, 0.10103639960289001, 0.03973813354969025, 0.08061376214027405, 0.12086477875709534, 0.12294680625200272, 0.1413251906633377, 0.1761181652545929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01305334735661745, 0.0019795650150626898, 0.0016182104591280222, 0.0014078522799536586, 0.0014868252910673618, 0.0013099143980070949, 0.0016642953269183636, 0.002566515700891614, 0.0030626696534454823, 0.010340573266148567, 0.007755252532660961, 0.008929421193897724, 0.0088695352897048, 0.017165109515190125, 0.07265791296958923, 0.08664457499980927, 0.06223947927355766, 0.16528460383415222, 0.07255090028047562, 0.09155545383691788, 0.3678579032421112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012316006235778332, 0.0011592353694140911, 0.0007622400298714638, 0.0019507502438500524, 0.0012718499638140202, 0.0006892276578582823, 0.0005458977539092302, 0.0026029967702925205, 0.003917663358151913, 0.0005409361328929663, 0.0012741906102746725, 0.006503167562186718, 0.005551590118557215, 0.01797347702085972, 0.01335335336625576, 0.03805122897028923, 0.043034009635448456, 0.039586495608091354, 0.02230067178606987, 0.038983654230833054, 0.5949540734291077, 0.1526772379875183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01372495573014021, 0.001612572348676622, 0.000328986847307533, 0.0010084313107654452, 0.0011758024338632822, 0.0007259281701408327, 0.000591875403188169, 0.0009615601156838238, 0.003118623746559024, 0.005970016121864319, 0.0030334654729813337, 0.0033444964792579412, 0.00840754620730877, 0.013658505864441395, 0.0056070247665047646, 0.005321722477674484, 0.019289908930659294, 0.02910960651934147, 0.013599625788629055, 0.03133779764175415, 0.2513725757598877, 0.5305221080780029, 0.056176893413066864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016002384945750237, 0.0009560459875501692, 0.000568046816624701, 0.0015549727249890566, 0.0015755726490169764, 0.0006219734204933047, 0.001125766197219491, 0.0014368741540238261, 0.002310051117092371, 0.0009028333006426692, 0.0013921482022851706, 0.0036725997924804688, 0.0036322835367172956, 0.009217403829097748, 0.008623871952295303, 0.027025720104575157, 0.021376455202698708, 0.03330109640955925, 0.038854051381349564, 0.03905840218067169, 0.26596641540527344, 0.19372618198394775, 0.09792875498533249, 0.22917021811008453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009987276047468185, 0.0009651741129346192, 0.000604157627094537, 0.0009222121443599463, 0.0008805812685750425, 0.0005974333616904914, 0.00021149232634343207, 0.0006455680704675615, 0.0012417664984241128, 0.0006859687273390591, 0.0007031494169496, 0.0014577426481992006, 0.0018487697234377265, 0.005077195819467306, 0.0019275658996775746, 0.012413379736244678, 0.008568299002945423, 0.012073654681444168, 0.02193666622042656, 0.02919415570795536, 0.14272752404212952, 0.09834042936563492, 0.1184767559170723, 0.3163198232650757, 0.21219328045845032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016860872507095337, 0.0019703248981386423, 0.0008858365472406149, 0.0017656380077823997, 0.002392836846411228, 0.0005271158297546208, 0.0003617894253693521, 0.0006361044943332672, 0.002241130219772458, 0.0008515855297446251, 0.0003815784293692559, 0.002274828962981701, 0.0018798471428453922, 0.007292233407497406, 0.0021466757170856, 0.002526309108361602, 0.010355674661695957, 0.011969308368861675, 0.006590919103473425, 0.012827491387724876, 0.12130489945411682, 0.06624652445316315, 0.14612393081188202, 0.2828073799610138, 0.24703297019004822, 0.04974614083766937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026405028998851776, 0.0014231036184355617, 0.0016234469367191195, 0.002576405182480812, 0.0026897049974650145, 0.0012257512426003814, 0.0010898755863308907, 0.0009538470185361803, 0.001922354567795992, 0.0024115280248224735, 0.0024579446762800217, 0.0014354207087308168, 0.0053784893825650215, 0.0047323438338935375, 0.001939113950356841, 0.007957504130899906, 0.006086884066462517, 0.01265741791576147, 0.011602522805333138, 0.029096629470586777, 0.085988849401474, 0.07665885984897614, 0.036240171641111374, 0.13133355975151062, 0.19490283727645874, 0.21766312420368195, 0.131547212600708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023896070197224617, 0.002067497232928872, 0.0025093788281083107, 0.0018511991947889328, 0.003160969354212284, 0.0009005626779980958, 0.00026854174211621284, 0.0005213558324612677, 0.0017872127937152982, 0.001930893282406032, 0.001015824032947421, 0.0010442675556987524, 0.001567705301567912, 0.004060104954987764, 0.0008207399514503777, 0.003358301008120179, 0.004117121919989586, 0.007223021239042282, 0.014547324739396572, 0.04635158181190491, 0.05601870268583298, 0.03840019553899765, 0.04160993918776512, 0.10494546592235565, 0.15788209438323975, 0.12118791788816452, 0.19090266525745392, 0.16605329513549805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04075779393315315, 0.003958363551646471, 0.005698093678802252, 0.0039579859003424644, 0.0014047829899936914, 0.00032769463723525405, 0.0006401923019438982, 0.0012045816984027624, 0.0022385623306035995, 0.0007448511314578354, 0.0038365772925317287, 0.0016206823056563735, 0.0010148317087441683, 0.004261191003024578, 0.0009652961743995547, 0.0003954324056394398, 0.005401073023676872, 0.0072633991949260235, 0.0057668061926960945, 0.02999185584485531, 0.041736576706171036, 0.017527643591165543, 0.025155052542686462, 0.07565264403820038, 0.24542617797851562, 0.06668493896722794, 0.2648320496082306, 0.05502420291304588, 0.08651073276996613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011784791015088558, 0.0013947568368166685, 0.0009515652782283723, 0.000895319739356637, 0.001037271460518241, 0.0004972650785930455, 0.0002528384793549776, 0.0004317571292631328, 0.0005345698446035385, 0.0002822920505423099, 0.0004274735983926803, 0.0005479560350067914, 0.0008038746891543269, 0.0011037476360797882, 0.0010882866336032748, 0.002635378623381257, 0.002003317466005683, 0.0029394503217190504, 0.0051141963340342045, 0.00916231144219637, 0.020236412063241005, 0.014397269114851952, 0.01438659057021141, 0.052873045206069946, 0.04425811022520065, 0.07174728810787201, 0.08077163994312286, 0.11779710650444031, 0.1448613703250885, 0.39478278160095215, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012992854230105877, 0.0016676138620823622, 0.0010021072812378407, 0.0011761231580749154, 0.0007638355600647628, 0.0004005824448540807, 0.00012849248014390469, 0.00010518682393012568, 0.0005606439081020653, 0.00014079391257837415, 0.0001220756530528888, 0.0003985989314969629, 0.00022988337150309235, 0.0011077816598117352, 0.0003631025901995599, 0.0008891806937754154, 0.0014402461238205433, 0.0014520204858854413, 0.0026305688079446554, 0.0031673875637352467, 0.017224673181772232, 0.010722361505031586, 0.0050229947082698345, 0.04577850177884102, 0.04751969873905182, 0.027067117393016815, 0.05216861888766289, 0.055007874965667725, 0.11481017619371414, 0.4456882178783417, 0.1482507288455963, 0.0, 0.0, 0.0, 0.0], [0.011719993315637112, 0.0009638668852858245, 0.0016792775131762028, 0.0009598775068297982, 0.0011835334589704871, 0.000358733901521191, 0.00013125187251716852, 0.00020985814626328647, 0.0003518577723298222, 0.00011673817061819136, 0.00033267156686633825, 0.0002089983318001032, 0.00016304025484714657, 0.000559160253033042, 0.000178778704139404, 0.0005847528227604926, 0.0006520415772683918, 0.000810086727142334, 0.0008232012623921037, 0.006545140407979488, 0.007790668867528439, 0.00662252027541399, 0.005052965600043535, 0.014489834196865559, 0.02494204416871071, 0.019010288640856743, 0.02852551080286503, 0.03678770735859871, 0.14266973733901978, 0.2348991483449936, 0.30085402727127075, 0.14982272684574127, 0.0, 0.0, 0.0], [0.011463271453976631, 0.0009014835231937468, 0.00054774503223598, 0.0005869875894859433, 0.000561661901883781, 0.0002343479573028162, 0.00013944691454526037, 0.00019926787354052067, 0.0003140303597319871, 0.00012514647096395493, 0.00021479540737345815, 0.00024813433992676437, 0.00022906146477907896, 0.0005020766402594745, 0.00045190195669420063, 0.000667672953568399, 0.0007638601819053292, 0.0009467164054512978, 0.0010094374883919954, 0.003719568718224764, 0.007444898597896099, 0.0045005581341683865, 0.0018344352720305324, 0.012973613105714321, 0.015522494912147522, 0.02183825522661209, 0.016228944063186646, 0.02469741739332676, 0.0380532443523407, 0.16302379965782166, 0.11826050281524658, 0.22884754836559296, 0.32294762134552, 0.0, 0.0], [0.009539714083075523, 0.0014243272598832846, 0.0009944578632712364, 0.0009655580506660044, 0.0007854424766264856, 0.00030160631285980344, 0.00011159540008520707, 0.0008093682699836791, 0.0004005460359621793, 8.517083915648982e-05, 0.0006307570147328079, 0.0002086153399432078, 0.0002525558229535818, 0.0005451837787404656, 0.00024275551550090313, 0.0005828302237205207, 0.0005503927241079509, 0.0010548559948801994, 0.0010979132493957877, 0.004033545032143593, 0.005151556339114904, 0.007251247297972441, 0.003988178446888924, 0.012260385788977146, 0.013561532832682133, 0.03828652203083038, 0.06675270944833755, 0.013111510314047337, 0.100932776927948, 0.13179117441177368, 0.05432308092713356, 0.1525149643421173, 0.30832454562187195, 0.06713265925645828, 0.0], [0.009234558790922165, 0.0018696918850764632, 0.0015434944070875645, 0.0012878900161013007, 0.0009349418105557561, 0.0002825608244165778, 0.0004494271124713123, 0.000559499254450202, 0.00032776911393739283, 7.203847781056538e-05, 0.0001065792384906672, 0.0002636241842992604, 0.0003654726024251431, 0.0003748309682123363, 0.00031175321782939136, 0.0008969117770902812, 0.0006347452872432768, 0.0008650015224702656, 0.0008020038949325681, 0.0006307890871539712, 0.00321878120303154, 0.0038458353374153376, 0.003000434022396803, 0.007814156822860241, 0.010107211768627167, 0.016721554100513458, 0.007672077510505915, 0.010031200014054775, 0.005356381647288799, 0.07757056504487991, 0.03774448484182358, 0.13809101283550262, 0.19567593932151794, 0.030743548646569252, 0.43059322237968445]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8970722556114197, 0.10292769968509674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7731761336326599, 0.17108607292175293, 0.05573776736855507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21909849345684052, 0.09592841565608978, 0.15399737656116486, 0.5309756994247437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4932919442653656, 0.12421079725027084, 0.08378947526216507, 0.21044497191905975, 0.08826279640197754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.479253351688385, 0.11483930796384811, 0.05825280770659447, 0.12140924483537674, 0.19296905398368835, 0.03327625244855881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33760711550712585, 0.13038285076618195, 0.12365774810314178, 0.1684831976890564, 0.039485666900873184, 0.1552271693944931, 0.04515616595745087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3124314546585083, 0.10335517674684525, 0.07322119921445847, 0.13077446818351746, 0.08650775253772736, 0.20371732115745544, 0.04714134708046913, 0.04285133630037308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10719956457614899, 0.03256480023264885, 0.04615398496389389, 0.12522779405117035, 0.011105242185294628, 0.03433005511760712, 0.007858381606638432, 0.011366589926183224, 0.624193549156189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3028891682624817, 0.07804971933364868, 0.059376444667577744, 0.09426908195018768, 0.17377148568630219, 0.038616254925727844, 0.04802526533603668, 0.039512548595666885, 0.13237622380256653, 0.03311377018690109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13027672469615936, 0.0421348512172699, 0.07258211076259613, 0.10922861844301224, 0.03630435839295387, 0.19112573564052582, 0.03369796648621559, 0.05715864151716232, 0.1118721291422844, 0.16414132714271545, 0.05147748440504074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0946442261338234, 0.029253441840410233, 0.03811940178275108, 0.1498958319425583, 0.020670872181653976, 0.07631716877222061, 0.008947466500103474, 0.020045092329382896, 0.30578550696372986, 0.047520942986011505, 0.010440824553370476, 0.19835911691188812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1444079428911209, 0.06329527497291565, 0.07617756724357605, 0.07194560766220093, 0.03394105285406113, 0.08442068845033646, 0.03594021126627922, 0.07685535401105881, 0.1158827617764473, 0.059519559144973755, 0.06183500215411186, 0.1484171599149704, 0.0273617971688509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0414787121117115, 0.011494416743516922, 0.0176435224711895, 0.045997254550457, 0.004085008520632982, 0.013701113872230053, 0.0028709012549370527, 0.004510778002440929, 0.24164843559265137, 0.00972951389849186, 0.0026167351752519608, 0.26417699456214905, 0.008053770288825035, 0.3319928050041199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15155857801437378, 0.037725068628787994, 0.02733064629137516, 0.06052473559975624, 0.030143307521939278, 0.02803489752113819, 0.013341412879526615, 0.022503042593598366, 0.11653424054384232, 0.051899734884500504, 0.010558661073446274, 0.10229870676994324, 0.015167273581027985, 0.14137376844882965, 0.1910059005022049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2839045226573944, 0.0650239884853363, 0.03238338232040405, 0.029591113328933716, 0.042432062327861786, 0.028469758108258247, 0.041533131152391434, 0.08465305715799332, 0.043854970484972, 0.02753039449453354, 0.09218153357505798, 0.03751113638281822, 0.04891904443502426, 0.049849655479192734, 0.07913843542337418, 0.01302380207926035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.048857610672712326, 0.014007373712956905, 0.01954260654747486, 0.07337554544210434, 0.01008914690464735, 0.03972572833299637, 0.004310055170208216, 0.010590334422886372, 0.1530359536409378, 0.02649904601275921, 0.005417255684733391, 0.0964762344956398, 0.01280671264976263, 0.2047865390777588, 0.14743389189243317, 0.009689198806881905, 0.12335672229528427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05943121761083603, 0.025951191782951355, 0.03547387570142746, 0.03907975181937218, 0.018049325793981552, 0.023087240755558014, 0.010503748431801796, 0.030069543048739433, 0.09918276965618134, 0.03614410012960434, 0.01841004006564617, 0.11741489917039871, 0.017167557030916214, 0.12769995629787445, 0.08459020406007767, 0.037902723997831345, 0.14577233791351318, 0.0740695372223854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.272489070892334, 0.045022740960121155, 0.04612233489751816, 0.04205196350812912, 0.038135677576065063, 0.05454428121447563, 0.025706566870212555, 0.019254140555858612, 0.04536456614732742, 0.05106881633400917, 0.021479330956935883, 0.05142933130264282, 0.035683080554008484, 0.05110248550772667, 0.04221018776297569, 0.038132794201374054, 0.05789823830127716, 0.023540068417787552, 0.03876428306102753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23981237411499023, 0.03723883256316185, 0.04641324654221535, 0.01807478256523609, 0.028609441593289375, 0.03156362473964691, 0.02326364815235138, 0.05314120277762413, 0.03672982007265091, 0.04436158016324043, 0.014090009033679962, 0.04079657047986984, 0.013384039513766766, 0.04018278419971466, 0.025124983862042427, 0.022283179685473442, 0.04457440227270126, 0.026357252150774002, 0.018502147868275642, 0.195496067404747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03447256609797478, 0.0070497882552444935, 0.008516225963830948, 0.019823294132947922, 0.0020061929244548082, 0.008738493546843529, 0.001380530884489417, 0.0025849866215139627, 0.1296386420726776, 0.004980520810931921, 0.0014542961725965142, 0.09793511033058167, 0.004242657218128443, 0.17474913597106934, 0.02057536318898201, 0.002921727020293474, 0.12519222497940063, 0.017666760832071304, 0.008172447793185711, 0.006463982630521059, 0.3214350640773773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07532122731208801, 0.017385125160217285, 0.02649959921836853, 0.03820113092660904, 0.01346429344266653, 0.017004918307065964, 0.005682717077434063, 0.008161394856870174, 0.0879102572798729, 0.015652015805244446, 0.004622378386557102, 0.06870009750127792, 0.017068389803171158, 0.11098403483629227, 0.08843360096216202, 0.01554048527032137, 0.08290655165910721, 0.03445708751678467, 0.03490292280912399, 0.022669052705168724, 0.16727589070796967, 0.047156885266304016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11039687693119049, 0.017618605867028236, 0.012073229067027569, 0.07903023064136505, 0.04707183688879013, 0.028393471613526344, 0.010853296145796776, 0.038982950150966644, 0.042541760951280594, 0.05313578620553017, 0.01061658188700676, 0.040667418390512466, 0.016578683629631996, 0.05044250562787056, 0.041483089327812195, 0.011694188229739666, 0.04894394427537918, 0.04587529972195625, 0.06803461164236069, 0.03145821765065193, 0.07373654097318649, 0.09332367032766342, 0.027047282084822655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05010594055056572, 0.011403637006878853, 0.018979990854859352, 0.04829108715057373, 0.005478168372064829, 0.0279381200671196, 0.0034753396175801754, 0.009667656384408474, 0.08883961290121078, 0.021459801122546196, 0.00918425153940916, 0.06738951802253723, 0.0062937806360423565, 0.11730516701936722, 0.05559387803077698, 0.023259563371539116, 0.08538828045129776, 0.03017236292362213, 0.018553785979747772, 0.04972721263766289, 0.1386994868516922, 0.03593092039227486, 0.029570575803518295, 0.04729193076491356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.033650629222393036, 0.039734359830617905, 0.013943267054855824, 0.022155016660690308, 0.00737410131841898, 0.012534351088106632, 0.0039379289373755455, 0.005588038824498653, 0.05276413634419441, 0.011353479698300362, 0.002976158168166876, 0.06121998280286789, 0.007114537060260773, 0.066092848777771, 0.018457189202308655, 0.004707179963588715, 0.07780604064464569, 0.015934070572257042, 0.01626971736550331, 0.027332955971360207, 0.08945079147815704, 0.028283195570111275, 0.013428342528641224, 0.24125005304813385, 0.1266416311264038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10162997245788574, 0.048173438757658005, 0.022741496562957764, 0.017647165805101395, 0.016357656568288803, 0.014718178659677505, 0.020238909870386124, 0.030098294839262962, 0.03836437687277794, 0.025297511368989944, 0.03959345072507858, 0.04179808869957924, 0.018494965508580208, 0.04345663636922836, 0.023876944556832314, 0.013411919586360455, 0.048047564923763275, 0.021995287388563156, 0.030744757503271103, 0.05168474093079567, 0.05786488577723503, 0.042160145938396454, 0.05039002001285553, 0.08024892956018448, 0.07608181238174438, 0.02488281950354576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06603521853685379, 0.02066011354327202, 0.021681703627109528, 0.03408760577440262, 0.01527940109372139, 0.018442001193761826, 0.010789579711854458, 0.05595836415886879, 0.03565485030412674, 0.02173885889351368, 0.025664951652288437, 0.03946555405855179, 0.01909186691045761, 0.0426049679517746, 0.045140340924263, 0.012913581915199757, 0.04735579341650009, 0.0255618654191494, 0.0352441631257534, 0.09746367484331131, 0.056113142520189285, 0.035938456654548645, 0.03256697580218315, 0.04301987960934639, 0.0469767302274704, 0.032461266964673996, 0.062089063227176666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10541093349456787, 0.0169731043279171, 0.020341023802757263, 0.03266651928424835, 0.02185116894543171, 0.031789958477020264, 0.005571202840656042, 0.0241945032030344, 0.016740458086133003, 0.016082551330327988, 0.013484922237694263, 0.02279036119580269, 0.020844874903559685, 0.019816402345895767, 0.007411987986415625, 0.01303507573902607, 0.02698245272040367, 0.010132252238690853, 0.01766769215464592, 0.29726263880729675, 0.02345716394484043, 0.03235580772161484, 0.03224536031484604, 0.019980112090706825, 0.050171952694654465, 0.04827892780303955, 0.033088184893131256, 0.019372398033738136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11478214710950851, 0.028451869264245033, 0.030158095061779022, 0.02989070862531662, 0.01557772047817707, 0.045236311852931976, 0.008156673051416874, 0.046777669340372086, 0.020574571564793587, 0.018089208751916885, 0.01663845404982567, 0.024351520463824272, 0.01428102795034647, 0.022870561107993126, 0.02455844357609749, 0.021522248163819313, 0.02774014137685299, 0.012263179756700993, 0.016789693385362625, 0.03060179390013218, 0.03310447186231613, 0.04230615869164467, 0.05013414844870567, 0.03989566117525101, 0.025471452623605728, 0.038356561213731766, 0.14836567640304565, 0.040621958673000336, 0.012431956827640533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01040296908468008, 0.003988316282629967, 0.0075365579687058926, 0.02371407300233841, 0.0015746998833492398, 0.00455649895593524, 0.0011461443500593305, 0.0020206556655466557, 0.08388331532478333, 0.006093548610806465, 0.0012243663659319282, 0.08011975884437561, 0.0032829302363097668, 0.1148514673113823, 0.020138544961810112, 0.0038740229792892933, 0.1035565733909607, 0.01623993180692196, 0.006219522096216679, 0.011158792302012444, 0.12897232174873352, 0.016916098073124886, 0.008813058957457542, 0.028795933350920677, 0.06059350073337555, 0.005618599709123373, 0.016893554478883743, 0.008791646920144558, 0.01702236942946911, 0.20200024545192719, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05472034215927124, 0.01595444045960903, 0.012098252773284912, 0.02283376082777977, 0.01898520067334175, 0.02878391370177269, 0.008076782338321209, 0.00837206281721592, 0.039564039558172226, 0.012315311469137669, 0.009588325396180153, 0.023571263998746872, 0.010643107816576958, 0.04663843289017677, 0.014515895396471024, 0.012394601479172707, 0.02759736403822899, 0.021691450849175453, 0.02018056996166706, 0.01823921501636505, 0.054966557770967484, 0.037117864936590195, 0.024244988337159157, 0.03499605134129524, 0.1494767814874649, 0.02901952527463436, 0.04327646270394325, 0.015866434201598167, 0.04511537402868271, 0.11428382992744446, 0.024871766567230225, 0.0, 0.0, 0.0, 0.0], [0.02826632559299469, 0.008342587389051914, 0.009693149477243423, 0.024003300815820694, 0.012406982481479645, 0.009731614962220192, 0.007254776544868946, 0.011378864757716656, 0.04525341838598251, 0.014661099761724472, 0.0047597987577319145, 0.03279305249452591, 0.00959976576268673, 0.056626737117767334, 0.023795733228325844, 0.00461410079151392, 0.03982144966721535, 0.02123899757862091, 0.02528655156493187, 0.10159800946712494, 0.09278310090303421, 0.02612387202680111, 0.015091429464519024, 0.03458563610911369, 0.06182190403342247, 0.013407009653747082, 0.0168897807598114, 0.030893534421920776, 0.028194859623908997, 0.1103137731552124, 0.040569547563791275, 0.03819920867681503, 0.0, 0.0, 0.0], [0.005920098163187504, 0.0026448285207152367, 0.005754887126386166, 0.021997176110744476, 0.0013081292854622006, 0.005726542789489031, 0.0008704049396328628, 0.0018940434092655778, 0.05295494198799133, 0.004725011996924877, 0.0009999452158808708, 0.043427061289548874, 0.002174984198063612, 0.0736372321844101, 0.017015773802995682, 0.003086996963247657, 0.057790882885456085, 0.01073448546230793, 0.0034258251544088125, 0.010332701727747917, 0.08509065955877304, 0.012429154478013515, 0.00623022997751832, 0.02222101017832756, 0.0641111359000206, 0.005053484346717596, 0.018152369186282158, 0.007594147697091103, 0.006786441896110773, 0.16716444492340088, 0.01387179084122181, 0.014755611307919025, 0.25011759996414185, 0.0, 0.0], [0.05958711728453636, 0.014287112280726433, 0.009688673540949821, 0.02847459353506565, 0.02860482968389988, 0.026259340345859528, 0.008819632232189178, 0.022136026993393898, 0.01791021227836609, 0.02639758214354515, 0.017243582755327225, 0.025983789935708046, 0.006316179409623146, 0.020043160766363144, 0.015440632589161396, 0.025940261781215668, 0.030103040859103203, 0.01836536079645157, 0.023130403831601143, 0.02621460147202015, 0.03115725889801979, 0.028986169025301933, 0.02896125800907612, 0.016435228288173676, 0.03480091318488121, 0.03794369101524353, 0.07380678504705429, 0.030772415921092033, 0.028517238795757294, 0.07387096434831619, 0.024587716907262802, 0.03523408621549606, 0.08090810477733612, 0.0230720154941082, 0.0], [0.052589867264032364, 0.008282380178570747, 0.004686587955802679, 0.011556106619536877, 0.002124359365552664, 0.007376666646450758, 0.0025699115358293056, 0.005167306400835514, 0.06430472433567047, 0.00868867989629507, 0.004349818453192711, 0.05405202880501747, 0.003567327279597521, 0.08101718127727509, 0.021457049995660782, 0.0041440739296376705, 0.06621565669775009, 0.01871694065630436, 0.009488237090408802, 0.006038552150130272, 0.10274302214384079, 0.014565212652087212, 0.009106297977268696, 0.021657653152942657, 0.035248007625341415, 0.004852636251598597, 0.008422153070569038, 0.019589481875300407, 0.01188589259982109, 0.14464661478996277, 0.016607563942670822, 0.02283477410674095, 0.08476627618074417, 0.015126854181289673, 0.05155405029654503]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8803902268409729, 0.11960975080728531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8001626133918762, 0.14422570168972015, 0.055611658841371536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6471941471099854, 0.14701782166957855, 0.07931008189916611, 0.12647800147533417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6291002035140991, 0.13269934058189392, 0.0830739215016365, 0.10306606441736221, 0.05206047743558884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5631014704704285, 0.12375931441783905, 0.1115972250699997, 0.10842733830213547, 0.08343343436717987, 0.009681186638772488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49733906984329224, 0.11400078237056732, 0.07851862162351608, 0.10408829897642136, 0.08229988068342209, 0.08720176666975021, 0.036551643162965775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4618663191795349, 0.10720596462488174, 0.07680262625217438, 0.09525881707668304, 0.11179576814174652, 0.04683658480644226, 0.0632898211479187, 0.03694408759474754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40880170464515686, 0.11223535984754562, 0.06194951757788658, 0.08138207346200943, 0.06888710707426071, 0.04759938269853592, 0.0577956885099411, 0.06300587207078934, 0.09834320098161697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33778563141822815, 0.0776895210146904, 0.061640240252017975, 0.08030416816473007, 0.09243477135896683, 0.12569788098335266, 0.043102774769067764, 0.08006506413221359, 0.08753130584955215, 0.013748607598245144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40844565629959106, 0.07972613722085953, 0.05943278223276138, 0.07098965346813202, 0.06258386373519897, 0.07298712432384491, 0.05290916562080383, 0.06663115322589874, 0.08486580848693848, 0.030785245820879936, 0.010643371380865574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3284089267253876, 0.0812605619430542, 0.05222117155790329, 0.0667019784450531, 0.062212198972702026, 0.046883273869752884, 0.04748227819800377, 0.049437131732702255, 0.08644368499517441, 0.05108509957790375, 0.04660410434007645, 0.08125963062047958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28456375002861023, 0.0733422264456749, 0.05956629291176796, 0.05995683744549751, 0.0626126155257225, 0.04492250457406044, 0.0535489097237587, 0.055395208299160004, 0.0760505199432373, 0.05439150705933571, 0.055210113525390625, 0.0848141685128212, 0.03562533110380173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2986924350261688, 0.0805349126458168, 0.04630003869533539, 0.05841611325740814, 0.05245251953601837, 0.03589673712849617, 0.043130066245794296, 0.04816412180662155, 0.07179583609104156, 0.035882286727428436, 0.03498636186122894, 0.06638611108064651, 0.04749886691570282, 0.07986356317996979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2618488073348999, 0.06895221769809723, 0.0466209352016449, 0.05860459432005882, 0.046928804367780685, 0.05568014830350876, 0.04915209487080574, 0.04117780551314354, 0.0749075710773468, 0.03198426216840744, 0.03257223591208458, 0.07257070392370224, 0.03739876300096512, 0.08542975783348083, 0.03617139533162117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29417479038238525, 0.05360918864607811, 0.03860877826809883, 0.04952674359083176, 0.04060918837785721, 0.04149327799677849, 0.029716258868575096, 0.04536982625722885, 0.060249317437410355, 0.04159965738654137, 0.06637388467788696, 0.05982466787099838, 0.04050290212035179, 0.06734265387058258, 0.06095735356211662, 0.010041485540568829, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24078050255775452, 0.05731502175331116, 0.038129787892103195, 0.04762377217411995, 0.04672614485025406, 0.03515475243330002, 0.034621819853782654, 0.0374724306166172, 0.06234779208898544, 0.039545461535453796, 0.0357443206012249, 0.05874497815966606, 0.03407841920852661, 0.07071568816900253, 0.054391153156757355, 0.0394514761865139, 0.06715653091669083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1900811493396759, 0.05546914041042328, 0.038249868899583817, 0.04468470439314842, 0.0434919148683548, 0.03729135915637016, 0.03500032424926758, 0.03413338586688042, 0.05614842474460602, 0.02901819348335266, 0.026478590443730354, 0.06265048682689667, 0.03502201288938522, 0.06568855792284012, 0.057141948491334915, 0.04781633988022804, 0.07356446981430054, 0.0680692046880722, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23406481742858887, 0.04765678197145462, 0.03626638278365135, 0.04350152239203453, 0.04482332989573479, 0.030926167964935303, 0.024145543575286865, 0.030471043661236763, 0.05100265517830849, 0.03852143511176109, 0.04128357768058777, 0.05289098620414734, 0.027892781421542168, 0.05738880857825279, 0.06067001447081566, 0.043871037662029266, 0.05933021754026413, 0.05117017775774002, 0.024122724309563637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3121170103549957, 0.043056994676589966, 0.030739208683371544, 0.03990120813250542, 0.040739186108112335, 0.03397355228662491, 0.013991208747029305, 0.03250167891383171, 0.04214292764663696, 0.031367406249046326, 0.05722317844629288, 0.04209974780678749, 0.03285326436161995, 0.046627193689346313, 0.048747748136520386, 0.021730581298470497, 0.0472237803041935, 0.0298575758934021, 0.045303892344236374, 0.007802657317370176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19583052396774292, 0.05227474868297577, 0.03251207247376442, 0.03705888241529465, 0.043354298919439316, 0.027641303837299347, 0.027452077716588974, 0.03308156877756119, 0.04532438516616821, 0.02540675736963749, 0.024781623855233192, 0.038880497217178345, 0.030247284099459648, 0.05089512839913368, 0.041287779808044434, 0.03749211132526398, 0.04425322264432907, 0.044287219643592834, 0.04519395902752876, 0.05072396993637085, 0.07202056050300598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1607999950647354, 0.04841074347496033, 0.03302096575498581, 0.0344170518219471, 0.03531978279352188, 0.023897040635347366, 0.02606349252164364, 0.028639735653996468, 0.04453872889280319, 0.02494947426021099, 0.029716162011027336, 0.04012999311089516, 0.024859057739377022, 0.05134393274784088, 0.0537354052066803, 0.045276302844285965, 0.04680594801902771, 0.03460600599646568, 0.042455028742551804, 0.05485451966524124, 0.06976334750652313, 0.04639734700322151, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18932510912418365, 0.03910362347960472, 0.024852009490132332, 0.03345049172639847, 0.0386015921831131, 0.024109283462166786, 0.020502107217907906, 0.024301735684275627, 0.03976469486951828, 0.03418166935443878, 0.034847840666770935, 0.03619782254099846, 0.023668933659791946, 0.04502483084797859, 0.034989431500434875, 0.026825517416000366, 0.04153791069984436, 0.0373666025698185, 0.042022787034511566, 0.05591043084859848, 0.06404418498277664, 0.056186337023973465, 0.0331849679350853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16596612334251404, 0.03388111665844917, 0.020657526329159737, 0.0321272648870945, 0.03747255355119705, 0.02651948295533657, 0.019882911816239357, 0.025609644129872322, 0.040917422622442245, 0.02261032536625862, 0.019720880314707756, 0.03376534953713417, 0.023503808304667473, 0.046445250511169434, 0.035414304584264755, 0.02736842632293701, 0.03876943141222, 0.03146073967218399, 0.04639929533004761, 0.048674724996089935, 0.06382153928279877, 0.06529750674962997, 0.046299055218696594, 0.04741539806127548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15514102578163147, 0.036314792931079865, 0.022236891090869904, 0.02809741534292698, 0.04015664756298065, 0.024055978283286095, 0.01740090921521187, 0.021498555317521095, 0.0367506667971611, 0.01798674277961254, 0.019703920930624008, 0.028327783569693565, 0.022604601457715034, 0.04168051853775978, 0.030497970059514046, 0.025585589930415154, 0.03248761594295502, 0.027014950290322304, 0.04112287610769272, 0.04738447442650795, 0.0638233870267868, 0.056550104171037674, 0.04119997099041939, 0.06263258308172226, 0.05974407121539116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12975944578647614, 0.027339700609445572, 0.02995750494301319, 0.022702835500240326, 0.03781535103917122, 0.03144635260105133, 0.017136767506599426, 0.04656491056084633, 0.030618296936154366, 0.01878168061375618, 0.025273198261857033, 0.028721962124109268, 0.0190909281373024, 0.03480999916791916, 0.026170218363404274, 0.026817739009857178, 0.03296038135886192, 0.024960247799754143, 0.054747626185417175, 0.05780106037855148, 0.05617623031139374, 0.03305983915925026, 0.05767418444156647, 0.04667419567704201, 0.062287017703056335, 0.02065236307680607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18126998841762543, 0.027924932539463043, 0.02805589884519577, 0.029457395896315575, 0.028532760217785835, 0.020307771861553192, 0.019550155848264694, 0.03153110295534134, 0.029232844710350037, 0.02111053466796875, 0.014580855146050453, 0.022505326196551323, 0.01625674031674862, 0.03150884807109833, 0.024519700556993484, 0.03659052401781082, 0.02503451704978943, 0.020030664280056953, 0.04726611450314522, 0.12142349779605865, 0.0418962687253952, 0.03646091744303703, 0.03734691068530083, 0.03705747425556183, 0.03521094471216202, 0.024784503504633904, 0.010552844032645226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1419323980808258, 0.02687341533601284, 0.02339441329240799, 0.024198874831199646, 0.03441091626882553, 0.031768228858709335, 0.014282823540270329, 0.027884986251592636, 0.02782326564192772, 0.020267946645617485, 0.020075350999832153, 0.02603541873395443, 0.015271042473614216, 0.031183557584881783, 0.02134445123374462, 0.030780868604779243, 0.02968650683760643, 0.024966172873973846, 0.03290623053908348, 0.0812656581401825, 0.047380201518535614, 0.039505016058683395, 0.0381060428917408, 0.03920026123523712, 0.04723043739795685, 0.03223776817321777, 0.04768495634198189, 0.022302791476249695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1936107575893402, 0.028910471126437187, 0.01741074025630951, 0.024991942569613457, 0.02041228674352169, 0.03692349046468735, 0.018564049154520035, 0.022491101175546646, 0.026469895616173744, 0.019851725548505783, 0.01670690067112446, 0.020129581913352013, 0.01730293035507202, 0.028877543285489082, 0.01937028579413891, 0.021512921899557114, 0.022604666650295258, 0.01946551725268364, 0.03218545392155647, 0.09415601193904877, 0.03964082896709442, 0.04692870005965233, 0.03296547010540962, 0.03432748094201088, 0.03549888730049133, 0.04152422770857811, 0.038008447736501694, 0.024193769320845604, 0.004963919520378113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13062264025211334, 0.029238127171993256, 0.02056247740983963, 0.02492492087185383, 0.026433374732732773, 0.02088005654513836, 0.01689508929848671, 0.020366203039884567, 0.030794668942689896, 0.016811521723866463, 0.014352645725011826, 0.026399672031402588, 0.017968133091926575, 0.03419143706560135, 0.025793621316552162, 0.020880060270428658, 0.029983345419168472, 0.02257683128118515, 0.031984008848667145, 0.0387752428650856, 0.047299277037382126, 0.03951810300350189, 0.03569551184773445, 0.046477824449539185, 0.045603711158037186, 0.027693992480635643, 0.02837626449763775, 0.029506970196962357, 0.03388815000653267, 0.06550604104995728, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10399015247821808, 0.021966133266687393, 0.016451166942715645, 0.022787967696785927, 0.02435716614127159, 0.019975431263446808, 0.01367861032485962, 0.02052461914718151, 0.02641664817929268, 0.015824981033802032, 0.01812475360929966, 0.02293805405497551, 0.015249819494783878, 0.02995176985859871, 0.019568635150790215, 0.017790060490369797, 0.026331383734941483, 0.020062364637851715, 0.030405661091208458, 0.05837804451584816, 0.04746948927640915, 0.03560006991028786, 0.03361855447292328, 0.039051610976457596, 0.0462346188724041, 0.03415835276246071, 0.05063629522919655, 0.033049553632736206, 0.04132763668894768, 0.060074809938669205, 0.034005604684352875, 0.0, 0.0, 0.0, 0.0], [0.10689456015825272, 0.02342521958053112, 0.01831112429499626, 0.022276200354099274, 0.019986264407634735, 0.018345672637224197, 0.015893511474132538, 0.019802751019597054, 0.02582935057580471, 0.016079476103186607, 0.01665685884654522, 0.02581820636987686, 0.012892822735011578, 0.029111457988619804, 0.020739195868372917, 0.017621517181396484, 0.02965446189045906, 0.024281153455376625, 0.028045887127518654, 0.05205393582582474, 0.04263105243444443, 0.03251812979578972, 0.03738031163811684, 0.038434360176324844, 0.03982122987508774, 0.026290135458111763, 0.0352063849568367, 0.03138628974556923, 0.032823432236909866, 0.06194707006216049, 0.037625446915626526, 0.0402165912091732, 0.0, 0.0, 0.0], [0.1222902163863182, 0.02516898512840271, 0.01671406999230385, 0.022033249959349632, 0.023000743240118027, 0.015408715233206749, 0.014958017505705357, 0.018676765263080597, 0.02619115449488163, 0.012370293959975243, 0.010715773329138756, 0.021631909534335136, 0.013984623365104198, 0.02889922820031643, 0.02320098504424095, 0.01821659691631794, 0.024281244724988937, 0.018235325813293457, 0.021653972566127777, 0.03633742406964302, 0.03855651989579201, 0.03691945970058441, 0.033792249858379364, 0.042588260024785995, 0.03839157149195671, 0.023295754566788673, 0.030805258080363274, 0.024493306875228882, 0.030105769634246826, 0.05652611702680588, 0.037566859275102615, 0.041599173098802567, 0.051390353590250015, 0.0, 0.0], [0.1387171894311905, 0.021967241540551186, 0.01668298803269863, 0.02155136875808239, 0.026049213483929634, 0.01537914015352726, 0.012522304430603981, 0.020025216042995453, 0.02187984809279442, 0.0145530691370368, 0.01562824659049511, 0.01828858256340027, 0.012421739287674427, 0.023633435368537903, 0.017220674082636833, 0.020091675221920013, 0.020151864737272263, 0.018530184403061867, 0.024212507531046867, 0.07234080135822296, 0.034239351749420166, 0.03014705888926983, 0.031458813697099686, 0.0302029587328434, 0.030948752537369728, 0.024833213537931442, 0.03074328415095806, 0.022636545822024345, 0.044566892087459564, 0.04173140972852707, 0.028924645856022835, 0.02986532635986805, 0.04288142919540405, 0.024973038583993912, 0.0], [0.12327982485294342, 0.02670745551586151, 0.014834711328148842, 0.01802150346338749, 0.017765000462532043, 0.013351651839911938, 0.014073795638978481, 0.018723033368587494, 0.02398008666932583, 0.021256305277347565, 0.017004817724227905, 0.020616665482521057, 0.012905524112284184, 0.026097441092133522, 0.021554257720708847, 0.015845777466893196, 0.022780928760766983, 0.023595379665493965, 0.019861742854118347, 0.02192237414419651, 0.03597114235162735, 0.03369682654738426, 0.02117166295647621, 0.035360537469387054, 0.03450924903154373, 0.021897656843066216, 0.035124246031045914, 0.024112900719046593, 0.03299051150679588, 0.04677637293934822, 0.03042444959282875, 0.03270317241549492, 0.03790726885199547, 0.042624928057193756, 0.04055080935359001]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533874273300171, 0.3466125726699829, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4451119303703308, 0.3337680399417877, 0.22112005949020386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5459808111190796, 0.1529107540845871, 0.0764545276761055, 0.22465386986732483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4272030293941498, 0.14125250279903412, 0.06143970414996147, 0.11979991942644119, 0.25030484795570374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4097931385040283, 0.11732903122901917, 0.06548821926116943, 0.12367459386587143, 0.04222370311617851, 0.24149122834205627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3643515110015869, 0.15547120571136475, 0.08527875691652298, 0.088326096534729, 0.049937352538108826, 0.03409624099731445, 0.22253882884979248, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35626867413520813, 0.09063339978456497, 0.050068147480487823, 0.10755760967731476, 0.06895481050014496, 0.04669608175754547, 0.04097530245780945, 0.23884600400924683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34613895416259766, 0.14562253654003143, 0.09470638632774353, 0.10499643534421921, 0.06400176882743835, 0.04047239199280739, 0.03757920861244202, 0.04448216035962105, 0.12200022488832474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2051023244857788, 0.04824312776327133, 0.05157081037759781, 0.09953965246677399, 0.06673119217157364, 0.045586876571178436, 0.03648391738533974, 0.08169858902692795, 0.08583585172891617, 0.27920758724212646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2301696389913559, 0.07576317340135574, 0.062430281192064285, 0.0619683563709259, 0.050248246639966965, 0.03796261176466942, 0.057705145329236984, 0.05617325380444527, 0.06585754454135895, 0.04166558384895325, 0.26005613803863525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2668823003768921, 0.09969265758991241, 0.06880500167608261, 0.10110106319189072, 0.03625768423080444, 0.04876028746366501, 0.031795434653759, 0.034267447888851166, 0.09192278236150742, 0.04406016692519188, 0.024279993027448654, 0.15217521786689758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24808596074581146, 0.05943753570318222, 0.03645306080579758, 0.09643886238336563, 0.04665255919098854, 0.043580614030361176, 0.02640967071056366, 0.038718465715646744, 0.06937100738286972, 0.03553216531872749, 0.02358422242105007, 0.05824920907616615, 0.21748670935630798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22227582335472107, 0.08784296363592148, 0.06428327411413193, 0.07245951145887375, 0.05153009295463562, 0.033541664481163025, 0.031511515378952026, 0.04395980015397072, 0.10531802475452423, 0.03629286214709282, 0.03349636122584343, 0.07923729717731476, 0.03496609255671501, 0.10328467190265656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2625542879104614, 0.0745101273059845, 0.049621909856796265, 0.06513947993516922, 0.022182414308190346, 0.03923412039875984, 0.021433910354971886, 0.02829248458147049, 0.06496229767799377, 0.022169625386595726, 0.018070971593260765, 0.05796554312109947, 0.018188003450632095, 0.061765022575855255, 0.19390979409217834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21454612910747528, 0.05207951366901398, 0.04338601604104042, 0.051086731255054474, 0.023057056590914726, 0.03987295553088188, 0.02145753800868988, 0.043103158473968506, 0.05712398886680603, 0.04732411354780197, 0.020076358690857887, 0.04863849654793739, 0.03173767402768135, 0.05389069765806198, 0.039228130131959915, 0.21339134871959686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16934643685817719, 0.0575847290456295, 0.04318232089281082, 0.06501621007919312, 0.025769725441932678, 0.03584061563014984, 0.023403942584991455, 0.028633087873458862, 0.06884393841028214, 0.03735613822937012, 0.021079333499073982, 0.12265528738498688, 0.025184888392686844, 0.07096558809280396, 0.04923555254936218, 0.028457900509238243, 0.12744440138339996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1294490545988083, 0.043515175580978394, 0.028707362711429596, 0.04557255282998085, 0.021282199770212173, 0.019885346293449402, 0.028857694938778877, 0.03478110954165459, 0.055325947701931, 0.030885064974427223, 0.02108755148947239, 0.06504066288471222, 0.040431588888168335, 0.05797602981328964, 0.041369806975126266, 0.028544072061777115, 0.06784921884536743, 0.23943962156772614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14163610339164734, 0.03578964248299599, 0.028365233913064003, 0.04538755118846893, 0.04637839272618294, 0.03978869691491127, 0.014343790709972382, 0.04959797114133835, 0.04732619971036911, 0.030782433226704597, 0.018412163481116295, 0.048710353672504425, 0.023401621729135513, 0.047455623745918274, 0.03751209005713463, 0.023782692849636078, 0.04894062876701355, 0.03379066288471222, 0.23859809339046478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13350322842597961, 0.03222054988145828, 0.02058890275657177, 0.03780217841267586, 0.041080109775066376, 0.036710936576128006, 0.025954410433769226, 0.04136637970805168, 0.043265700340270996, 0.03009646199643612, 0.026812905445694923, 0.04010695964097977, 0.024385901167988777, 0.04430919513106346, 0.0337374322116375, 0.020710919052362442, 0.040797438472509384, 0.032650161534547806, 0.03506243973970413, 0.2588377892971039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11974353343248367, 0.03914881870150566, 0.028956566005945206, 0.033427074551582336, 0.037695933133363724, 0.020543895661830902, 0.01755896396934986, 0.0326584093272686, 0.06372418999671936, 0.033101923763751984, 0.029867956414818764, 0.05386917293071747, 0.02251296676695347, 0.07236672192811966, 0.04143426567316055, 0.037901412695646286, 0.06002622842788696, 0.04586019366979599, 0.05091095715761185, 0.031955890357494354, 0.12673485279083252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11878622323274612, 0.03957201912999153, 0.023222975432872772, 0.05364077910780907, 0.032616786658763885, 0.017370950430631638, 0.011663906276226044, 0.016273824498057365, 0.05014301836490631, 0.013260260224342346, 0.010486909188330173, 0.03813209757208824, 0.04193669930100441, 0.05336222052574158, 0.030979981645941734, 0.01961771585047245, 0.040241539478302, 0.03244159743189812, 0.020540131255984306, 0.02684551291167736, 0.05771109089255333, 0.25115376710891724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10596504807472229, 0.03639821335673332, 0.023730667307972908, 0.02770346961915493, 0.029520338401198387, 0.016958516091108322, 0.009277228266000748, 0.019281193614006042, 0.04353449493646622, 0.025435717776417732, 0.015441058203577995, 0.041617270559072495, 0.018472321331501007, 0.04617343097925186, 0.0269752386957407, 0.016877291724085808, 0.0436592772603035, 0.02019023522734642, 0.018645791336894035, 0.016359694302082062, 0.05703480541706085, 0.06433958560228348, 0.27640917897224426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08669397234916687, 0.1400514543056488, 0.05404341220855713, 0.026910429820418358, 0.02693350985646248, 0.01693662256002426, 0.01752723939716816, 0.011132627725601196, 0.033167172223329544, 0.009116582572460175, 0.014021698385477066, 0.028125273063778877, 0.012107602320611477, 0.03627435490489006, 0.030991649255156517, 0.015099075622856617, 0.03022146038711071, 0.020815476775169373, 0.018955035135149956, 0.021182600408792496, 0.04364987090229988, 0.04377346858382225, 0.025040632113814354, 0.23722872138023376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10505463182926178, 0.024722838774323463, 0.023438598960638046, 0.024256443604826927, 0.02147771045565605, 0.014111110009253025, 0.01201801560819149, 0.01570729911327362, 0.04219583421945572, 0.016803370788693428, 0.011159354820847511, 0.031271811574697495, 0.020320694893598557, 0.04702332988381386, 0.039886780083179474, 0.025853369385004044, 0.03450959175825119, 0.02452259697020054, 0.017594223842024803, 0.02159716747701168, 0.06303167343139648, 0.050369855016469955, 0.02654913067817688, 0.040915295481681824, 0.24560922384262085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07146888226270676, 0.015683099627494812, 0.019199570640921593, 0.02765512280166149, 0.025888752192258835, 0.011495743878185749, 0.013834549114108086, 0.025006256997585297, 0.032857511192560196, 0.04069041088223457, 0.019407076761126518, 0.03628278151154518, 0.026671355590224266, 0.03698844835162163, 0.025391366332769394, 0.012203286401927471, 0.040816258639097214, 0.022441426292061806, 0.018680065870285034, 0.03714069724082947, 0.05291350930929184, 0.033748991787433624, 0.03600706905126572, 0.027396058663725853, 0.04157792404294014, 0.2485538274049759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09520061314105988, 0.02339908853173256, 0.016249950975179672, 0.03728700801730156, 0.02706948108971119, 0.02442619763314724, 0.016916709020733833, 0.020519249141216278, 0.03561446815729141, 0.017246786504983902, 0.01696518063545227, 0.03067062608897686, 0.01476178877055645, 0.03783894330263138, 0.025590214878320694, 0.013281543739140034, 0.032757267355918884, 0.016981469467282295, 0.01291283406317234, 0.040966760367155075, 0.044523730874061584, 0.03288853541016579, 0.02186199650168419, 0.02890506573021412, 0.025222204625606537, 0.01568194292485714, 0.2742603123188019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07106205821037292, 0.022234490141272545, 0.015442527830600739, 0.02880682982504368, 0.02491171844303608, 0.014096672646701336, 0.010925306007266045, 0.022356444969773293, 0.028251029551029205, 0.025925245136022568, 0.012314548715949059, 0.036213621497154236, 0.017662767320871353, 0.03140704333782196, 0.01859503611922264, 0.018171407282352448, 0.0400666669011116, 0.022728294134140015, 0.025000542402267456, 0.04014962166547775, 0.04249804466962814, 0.02628159336745739, 0.023634904995560646, 0.030348237603902817, 0.028446590527892113, 0.04931708052754402, 0.04533270001411438, 0.22781902551651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11998684704303741, 0.032013118267059326, 0.020363768562674522, 0.03192230686545372, 0.020111490041017532, 0.017741888761520386, 0.014963150024414062, 0.014099692925810814, 0.0323602668941021, 0.012826556339859962, 0.01920423097908497, 0.024741897359490395, 0.014775943011045456, 0.03365081921219826, 0.03285077586770058, 0.01563302055001259, 0.026330968365073204, 0.00994691252708435, 0.010015271604061127, 0.023254679515957832, 0.038984283804893494, 0.03491940349340439, 0.011989148333668709, 0.03053087182343006, 0.02381373569369316, 0.018979014828801155, 0.06846081465482712, 0.01914154179394245, 0.22638756036758423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07111397385597229, 0.023044994100928307, 0.017407841980457306, 0.025577735155820847, 0.01881728507578373, 0.014406670816242695, 0.011364506557583809, 0.019742919132113457, 0.0378037765622139, 0.01749836839735508, 0.012214535847306252, 0.0347118005156517, 0.016245108097791672, 0.04494825378060341, 0.03146103769540787, 0.017976265400648117, 0.040775004774332047, 0.026232006028294563, 0.02910454571247101, 0.032863158732652664, 0.06312327831983566, 0.039967380464076996, 0.031937435269355774, 0.05005760118365288, 0.0500398613512516, 0.02629847079515457, 0.04067575931549072, 0.03425348550081253, 0.03361441567540169, 0.0867224931716919, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06342680752277374, 0.018928559496998787, 0.015915200114250183, 0.02297447808086872, 0.01581357978284359, 0.011996155604720116, 0.006959729827940464, 0.010440586134791374, 0.027604447677731514, 0.014203773811459541, 0.011755011044442654, 0.026974201202392578, 0.015696221962571144, 0.03244096040725708, 0.021464021876454353, 0.016300221905112267, 0.0315910279750824, 0.018295634537935257, 0.024074910208582878, 0.018299637362360954, 0.04905860498547554, 0.03294462710618973, 0.03169502690434456, 0.04070066660642624, 0.05526785925030708, 0.03228400647640228, 0.01833721622824669, 0.02062193490564823, 0.020615682005882263, 0.061421558260917664, 0.21189765632152557, 0.0, 0.0, 0.0, 0.0], [0.05903121829032898, 0.011650338768959045, 0.011906322091817856, 0.020305119454860687, 0.015824556350708008, 0.011342639103531837, 0.01216509286314249, 0.020514553412795067, 0.02463514730334282, 0.01787753775715828, 0.008976955898106098, 0.027097541838884354, 0.010151363909244537, 0.029500095173716545, 0.022961732000112534, 0.01152713317424059, 0.03249187767505646, 0.02021908015012741, 0.020271258428692818, 0.02760496363043785, 0.04230693355202675, 0.022920366376638412, 0.02739839255809784, 0.024839363992214203, 0.028419984504580498, 0.041721608489751816, 0.0314212366938591, 0.03520328924059868, 0.030631227418780327, 0.060064755380153656, 0.045377377420663834, 0.19364094734191895, 0.0, 0.0, 0.0], [0.0859057754278183, 0.014704101718962193, 0.010510571300983429, 0.02869984321296215, 0.016654493287205696, 0.012795397080481052, 0.009235279634594917, 0.01481921412050724, 0.028232717886567116, 0.011867678724229336, 0.009268181398510933, 0.02304435893893242, 0.017553256824612617, 0.032954029738903046, 0.025013742968440056, 0.012247500009834766, 0.026799557730555534, 0.013326537795364857, 0.016479849815368652, 0.03472821041941643, 0.0398080088198185, 0.05067336931824684, 0.03086116909980774, 0.027551839128136635, 0.03616423159837723, 0.01740611530840397, 0.047160595655441284, 0.02589184232056141, 0.028397278860211372, 0.06047137454152107, 0.03584859520196915, 0.038866691291332245, 0.11605867743492126, 0.0, 0.0], [0.05670773983001709, 0.01524327415972948, 0.009892585687339306, 0.017495421692728996, 0.03529244661331177, 0.015370243228971958, 0.010747749358415604, 0.017091600224375725, 0.02124500647187233, 0.01306450366973877, 0.006394116207957268, 0.02176818810403347, 0.011015360243618488, 0.023290829733014107, 0.0209710281342268, 0.012346652336418629, 0.024119656533002853, 0.013815042562782764, 0.016820495948195457, 0.0345151424407959, 0.030974792316555977, 0.03217754513025284, 0.03227026388049126, 0.023270364850759506, 0.02725941687822342, 0.01785033941268921, 0.05776430293917656, 0.02427157387137413, 0.02524302899837494, 0.03774581849575043, 0.015895819291472435, 0.026596596464514732, 0.04215161129832268, 0.20932146906852722, 0.0], [0.047369714826345444, 0.013491541147232056, 0.00860119704157114, 0.011534364894032478, 0.03260952606797218, 0.00875001773238182, 0.010495420545339584, 0.015973467379808426, 0.02422492206096649, 0.013959750533103943, 0.011584170162677765, 0.02090698480606079, 0.012310187332332134, 0.028499366715550423, 0.014403234235942364, 0.01142818946391344, 0.024424538016319275, 0.02128404751420021, 0.027576273307204247, 0.025762217119336128, 0.0491495355963707, 0.021549461409449577, 0.02072589285671711, 0.030232440680265427, 0.046837322413921356, 0.021722478792071342, 0.02025720104575157, 0.0191495381295681, 0.010533906519412994, 0.050025925040245056, 0.030405575409531593, 0.022812237963080406, 0.029462480917572975, 0.03996320068836212, 0.20198364555835724]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8253123164176941, 0.1746877133846283, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7580503821372986, 0.1302967369556427, 0.11165298521518707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7232239842414856, 0.0827443078160286, 0.07870828360319138, 0.11532341688871384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5594541430473328, 0.0869443267583847, 0.0911555141210556, 0.09393444657325745, 0.1685115545988083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5031397938728333, 0.08192650228738785, 0.058723147958517075, 0.10480903089046478, 0.1521420180797577, 0.09925951808691025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5279608964920044, 0.058259785175323486, 0.0683659240603447, 0.11652137339115143, 0.11039452999830246, 0.06918033957481384, 0.04931707680225372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30010077357292175, 0.0882599726319313, 0.05834932625293732, 0.10984241217374802, 0.16862636804580688, 0.1289689540863037, 0.023227633908391, 0.12262456864118576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3934403657913208, 0.04758617654442787, 0.05194965749979019, 0.07471578568220139, 0.11950990557670593, 0.05938037857413292, 0.02336861379444599, 0.1026640310883522, 0.12738502025604248, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3446025848388672, 0.05032898485660553, 0.059913281351327896, 0.04483130946755409, 0.15738160908222198, 0.06782457232475281, 0.032290104776620865, 0.07052528858184814, 0.0895390659570694, 0.08276329934597015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3350847661495209, 0.046269964426755905, 0.04785766452550888, 0.10917433351278305, 0.04435235261917114, 0.044907230883836746, 0.02844385989010334, 0.0792062059044838, 0.10731320828199387, 0.06832483410835266, 0.08906549960374832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3115721642971039, 0.03927760198712349, 0.03521274775266647, 0.0755099505186081, 0.08078576624393463, 0.04503686726093292, 0.02880650945007801, 0.10218673944473267, 0.08797682076692581, 0.05585264787077904, 0.08571925014257431, 0.0520629957318306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.361469566822052, 0.06311225146055222, 0.026989925652742386, 0.07095801085233688, 0.03985175862908363, 0.029446516185998917, 0.022796371951699257, 0.07072696834802628, 0.11304392665624619, 0.03335518389940262, 0.037144239991903305, 0.04763941466808319, 0.08346593379974365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26885920763015747, 0.032650742679834366, 0.039381880313158035, 0.05081149563193321, 0.08684233576059341, 0.0431019626557827, 0.01792234368622303, 0.06655362248420715, 0.07785409688949585, 0.04528903588652611, 0.06586872041225433, 0.04606606438755989, 0.07721763104200363, 0.08158090710639954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20790523290634155, 0.030814701691269875, 0.02946298196911812, 0.05067422613501549, 0.0874347984790802, 0.04770582169294357, 0.027333181351423264, 0.09152117371559143, 0.05214140936732292, 0.047082483768463135, 0.0808199867606163, 0.054532740265131, 0.12074839323759079, 0.05516167730093002, 0.016661211848258972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19932866096496582, 0.042067065834999084, 0.05715770646929741, 0.07193326950073242, 0.05065881088376045, 0.03152455389499664, 0.027136795222759247, 0.04140906780958176, 0.09322270005941391, 0.05202208459377289, 0.025422828271985054, 0.05927911400794983, 0.06613773107528687, 0.0958227589726448, 0.031632162630558014, 0.055244650691747665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2371978461742401, 0.030675794929265976, 0.029208488762378693, 0.058212410658597946, 0.06082989647984505, 0.034956078976392746, 0.02444053255021572, 0.07084924727678299, 0.05983854830265045, 0.03970348462462425, 0.05932589992880821, 0.0367523692548275, 0.07694027572870255, 0.061716433614492416, 0.014049747958779335, 0.06745628267526627, 0.03784667328000069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2499532848596573, 0.0266294926404953, 0.039317600429058075, 0.04463211074471474, 0.07087580114603043, 0.034094203263521194, 0.02126503549516201, 0.05726419389247894, 0.05179664492607117, 0.035039860755205154, 0.043922752141952515, 0.03922875225543976, 0.06155908852815628, 0.0512242317199707, 0.015611985698342323, 0.08278114348649979, 0.03891747072339058, 0.035886336117982864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2226979285478592, 0.026518110185861588, 0.03978954255580902, 0.027604006230831146, 0.08196065574884415, 0.03582962974905968, 0.01537896879017353, 0.06874661892652512, 0.053430892527103424, 0.05469820648431778, 0.05721571296453476, 0.03161526471376419, 0.03717069700360298, 0.05228297784924507, 0.011851193383336067, 0.0720609650015831, 0.030878296121954918, 0.024411149322986603, 0.055859167128801346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07419286668300629, 0.050926972180604935, 0.028230959549546242, 0.034200046211481094, 0.03361426293849945, 0.05182218924164772, 0.00965837575495243, 0.049406129866838455, 0.09883324801921844, 0.053564853966236115, 0.02458186261355877, 0.05879181623458862, 0.013026787899434566, 0.10889018326997757, 0.01567711867392063, 0.04766633361577988, 0.06507279723882675, 0.038861677050590515, 0.039842620491981506, 0.10313885658979416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22614890336990356, 0.025769880041480064, 0.03336168825626373, 0.040344879031181335, 0.06116844713687897, 0.03166023641824722, 0.023036984726786613, 0.04815823957324028, 0.049641285091638565, 0.025703316554427147, 0.051324233412742615, 0.029225796461105347, 0.04627235606312752, 0.04679546877741814, 0.010471499525010586, 0.04346739500761032, 0.02797776274383068, 0.033561062067747116, 0.03209923952817917, 0.05562402680516243, 0.05818728730082512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19442985951900482, 0.020005788654088974, 0.02566997893154621, 0.027610069140791893, 0.07351992279291153, 0.025118572637438774, 0.016428444534540176, 0.05443365126848221, 0.04450267553329468, 0.022272871807217598, 0.029331285506486893, 0.030642621219158173, 0.0628659725189209, 0.0433916375041008, 0.010216018185019493, 0.09763482958078384, 0.029601458460092545, 0.02808043733239174, 0.037571631371974945, 0.05037403851747513, 0.06089744344353676, 0.015400816686451435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19766588509082794, 0.041157495230436325, 0.033601175993680954, 0.02387017011642456, 0.06633447855710983, 0.028016982600092888, 0.019582953304052353, 0.051785655319690704, 0.03159147500991821, 0.02653616853058338, 0.04164315015077591, 0.025757962837815285, 0.04328406602144241, 0.029939444735646248, 0.012750319205224514, 0.05606916919350624, 0.024677345529198647, 0.021505890414118767, 0.045592907816171646, 0.07480884343385696, 0.044334061443805695, 0.02199735678732395, 0.037497006356716156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1804356575012207, 0.023349829018115997, 0.02392975240945816, 0.04454169422388077, 0.040705952793359756, 0.03654098883271217, 0.0166165828704834, 0.05323563516139984, 0.05028390884399414, 0.025846900418400764, 0.04605168104171753, 0.024920545518398285, 0.03976091369986534, 0.0489342026412487, 0.012390018440783024, 0.06191660091280937, 0.02396535500884056, 0.028754480183124542, 0.025095410645008087, 0.0686592161655426, 0.07357670366764069, 0.018046921119093895, 0.022800730541348457, 0.009640374220907688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2424635887145996, 0.02469644322991371, 0.03868825361132622, 0.033572301268577576, 0.0547662079334259, 0.03380465880036354, 0.02072160132229328, 0.03275461494922638, 0.04349980503320694, 0.01856265962123871, 0.03646848723292351, 0.031002476811408997, 0.03862545266747475, 0.03993181139230728, 0.010727171786129475, 0.03831769526004791, 0.029280314221978188, 0.02467810921370983, 0.02788931503891945, 0.045021578669548035, 0.04307600110769272, 0.016905829310417175, 0.023586291819810867, 0.007298622280359268, 0.04366062581539154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11111285537481308, 0.029458746314048767, 0.024559475481510162, 0.03090251050889492, 0.0338648296892643, 0.033630650490522385, 0.012831445783376694, 0.030669262632727623, 0.06390535831451416, 0.023481549695134163, 0.025704601779580116, 0.037669796496629715, 0.016167856752872467, 0.06660735607147217, 0.01723177172243595, 0.030658653005957603, 0.03924048691987991, 0.024295227602124214, 0.02638878859579563, 0.058640256524086, 0.10666140168905258, 0.0176238976418972, 0.024245435371994972, 0.018279608339071274, 0.036750197410583496, 0.05941799655556679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17855212092399597, 0.043738700449466705, 0.027772290632128716, 0.032882511615753174, 0.08083149790763855, 0.03614853695034981, 0.01943814381957054, 0.036047156900167465, 0.031540632247924805, 0.022283848375082016, 0.026742076501250267, 0.020272962749004364, 0.049294859170913696, 0.028057031333446503, 0.010035554878413677, 0.028219010680913925, 0.018526628613471985, 0.025377929210662842, 0.03921227157115936, 0.05273987725377083, 0.024735102429986, 0.028674790635704994, 0.029405977576971054, 0.013893142342567444, 0.017606468871235847, 0.05648737773299217, 0.02148348279297352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15546518564224243, 0.04916935786604881, 0.02451593428850174, 0.02570052072405815, 0.05483131855726242, 0.0420927032828331, 0.011645050719380379, 0.06493964791297913, 0.031219754368066788, 0.026562806218862534, 0.028326863422989845, 0.018289046362042427, 0.03490860015153885, 0.029660992324352264, 0.008558596484363079, 0.046650487929582596, 0.01766982488334179, 0.023724358528852463, 0.01903872936964035, 0.0771576315164566, 0.02730739116668701, 0.019747762009501457, 0.024364111945033073, 0.014429938979446888, 0.01892835833132267, 0.05850560963153839, 0.024364043027162552, 0.022225424647331238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19332915544509888, 0.026298323646187782, 0.030392466112971306, 0.029611896723508835, 0.0452016144990921, 0.033079031854867935, 0.021840767934918404, 0.026594163849949837, 0.045382823795080185, 0.038439467549324036, 0.026915406808257103, 0.02453509159386158, 0.01579570583999157, 0.04036761075258255, 0.01063427235931158, 0.026345476508140564, 0.022409794852137566, 0.019946027547121048, 0.04401850700378418, 0.06709349155426025, 0.03345474228262901, 0.016084952279925346, 0.025615857914090157, 0.011409515514969826, 0.013623220846056938, 0.040554922074079514, 0.016662970185279846, 0.03589886799454689, 0.01846369542181492, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1596519500017166, 0.018642352893948555, 0.02667771652340889, 0.028047844767570496, 0.049077726900577545, 0.026045648381114006, 0.017807701602578163, 0.03679424151778221, 0.04596944898366928, 0.019110897555947304, 0.037963200360536575, 0.02762063406407833, 0.028341032564640045, 0.042964354157447815, 0.00884759146720171, 0.03253018856048584, 0.025659989565610886, 0.024485092610120773, 0.029966214671730995, 0.040540698915719986, 0.04233505204319954, 0.017694566398859024, 0.021217679604887962, 0.006380945909768343, 0.031033938750624657, 0.03557971864938736, 0.025066187605261803, 0.026607466861605644, 0.02430843561887741, 0.04303154721856117, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1966170370578766, 0.01979137398302555, 0.0330333411693573, 0.02568708173930645, 0.042206067591905594, 0.03137579187750816, 0.01624363474547863, 0.030698437243700027, 0.03629428520798683, 0.026839356869459152, 0.026712195947766304, 0.023046307265758514, 0.021950097754597664, 0.03344836086034775, 0.0103057362139225, 0.022743647918105125, 0.021480808034539223, 0.02109472267329693, 0.031490374356508255, 0.03692729026079178, 0.03723211959004402, 0.015422007068991661, 0.019252333790063858, 0.007320488803088665, 0.024766933172941208, 0.044391706585884094, 0.0198507197201252, 0.026133641600608826, 0.01812445931136608, 0.04270908609032631, 0.03681057319045067, 0.0, 0.0, 0.0, 0.0], [0.16313210129737854, 0.022932473570108414, 0.02958368882536888, 0.028615493327379227, 0.03778465464711189, 0.0265186857432127, 0.02622901275753975, 0.03103414550423622, 0.03483205288648605, 0.01897561550140381, 0.033202942460775375, 0.026435956358909607, 0.032307349145412445, 0.031329210847616196, 0.011727887205779552, 0.027169140055775642, 0.024106504395604134, 0.022620299831032753, 0.025755560025572777, 0.038014139980077744, 0.03207404166460037, 0.017354249954223633, 0.017987210303544998, 0.009293688461184502, 0.04247136414051056, 0.031215859577059746, 0.019006576389074326, 0.021523458883166313, 0.01754118874669075, 0.04014519974589348, 0.018160773441195488, 0.04091951623558998, 0.0, 0.0, 0.0], [0.15803849697113037, 0.016828814521431923, 0.02064238302409649, 0.045382529497146606, 0.04664173722267151, 0.027835214510560036, 0.020206501707434654, 0.04211510345339775, 0.03703448176383972, 0.017744597047567368, 0.030353324487805367, 0.024713635444641113, 0.03428484499454498, 0.03341371938586235, 0.009082170203328133, 0.03521377965807915, 0.022280128672719002, 0.020849991589784622, 0.02739020064473152, 0.032314613461494446, 0.0376492440700531, 0.014920786954462528, 0.01655365712940693, 0.006309076678007841, 0.023218737915158272, 0.027949875220656395, 0.02383222058415413, 0.015323548577725887, 0.01767047494649887, 0.02681528963148594, 0.022188855335116386, 0.028694557026028633, 0.03650737553834915, 0.0, 0.0], [0.1701940894126892, 0.03212590143084526, 0.029870105907320976, 0.019615596160292625, 0.054893799126148224, 0.03622058779001236, 0.0171522106975317, 0.032326590269804, 0.0315006859600544, 0.02962454967200756, 0.029094265773892403, 0.016624262556433678, 0.02998977340757847, 0.02761414460837841, 0.010949769988656044, 0.034366536885499954, 0.014830109663307667, 0.015176051296293736, 0.03050290420651436, 0.04159744456410408, 0.02064122073352337, 0.017041224986314774, 0.02617543749511242, 0.010538814589381218, 0.007441797759383917, 0.0343913659453392, 0.026077089831233025, 0.029615644365549088, 0.013693022541701794, 0.01807429827749729, 0.03374848887324333, 0.029002588242292404, 0.013117276132106781, 0.016172276809811592, 0.0], [0.11417390406131744, 0.01670067384839058, 0.023576240986585617, 0.032352838665246964, 0.05511363968253136, 0.029616646468639374, 0.01717638038098812, 0.059527862817049026, 0.0384342297911644, 0.02042110450565815, 0.03235999867320061, 0.019217444583773613, 0.019135108217597008, 0.035197511315345764, 0.010996301658451557, 0.04401443898677826, 0.016835959628224373, 0.01621248573064804, 0.023840539157390594, 0.04231659695506096, 0.037168413400650024, 0.009007315151393414, 0.01597953587770462, 0.005828782916069031, 0.020826421678066254, 0.028255829587578773, 0.03160591050982475, 0.024252014234662056, 0.0232852753251791, 0.01991748809814453, 0.024559779092669487, 0.02195737697184086, 0.013193146325647831, 0.02020881325006485, 0.03673398122191429]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import circuitsvis as cv\n",
        "from IPython.display import display\n",
        "\n",
        "html = cv.attention.attention_patterns(\n",
        "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
        "    attention=cache[\"pattern\", 0][0], # NOTE: this is the entire Attention matrix\n",
        ")\n",
        "display(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHMgJQ4NPhF"
      },
      "source": [
        "You can also use the `attention_heads` function, which has similar syntax but presents the information in a different (sometimes more helpful) way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7xEpkmMNPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>\n",
        "\n",
        "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
        "\n",
        "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
        "\n",
        "```python\n",
        "attn_heads = cv.attention.attention_heads(\n",
        "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
        "    attention=cache[\"pattern\", 0][0]\n",
        ")\n",
        "\n",
        "path = \"attn_heads.html\"\n",
        "\n",
        "with open(path, \"w\") as f:\n",
        "    f.write(str(attn_heads))\n",
        "\n",
        "webbrowser.open(path)\n",
        "```\n",
        "\n",
        "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-9KuZwhNPhF"
      },
      "source": [
        "---\n",
        "\n",
        "Note - don't worry if you don't get 100% accuracy here; the tests are pretty stringent. Even things like having your `einsum` input arguments in a different order might result in the output being very slightly different. You should be getting at least 99% accuracy though, so if the value is lower then this it probably means you've made a mistake somewhere.\n",
        "\n",
        "Also, this implementation will probably be the most challenging exercise on this page, so don't worry if it takes you some time! You should look at parts of the solution or hints if you're stuck.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf94rQuFNPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Hint: high level steps</summary>\n",
        "\n",
        "```python\n",
        "# Calculate query, key and value vectors\n",
        "# Calculate attention scores\n",
        "# Then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
        "# Take weighted sum of value vectors, according to attention probabilities\n",
        "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint: detailed steps with only a few blanks to fill.</summary>\n",
        "\n",
        "```python\n",
        "# Calculate query, key and value vectors\n",
        "q = (\n",
        "    einops.einsum(\n",
        "        normalized_resid_pre,\n",
        "        self.W_Q,\n",
        "        \"batch posn d_model, nheads d_model d_head -> ???\",\n",
        "    )\n",
        "    + self.b_Q\n",
        ")\n",
        "\n",
        "k = ...\n",
        "\n",
        "v = ...\n",
        "\n",
        "# Calculate attention scores\n",
        "attn_scores = einops.einsum(\n",
        "    q,\n",
        "    k,\n",
        "    \"???,??? -> batch nheads posn_Q posn_K\",\n",
        ")\n",
        "\n",
        "# then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
        "attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
        "attn_pattern = attn_scores_masked.softmax(dim=...)\n",
        "\n",
        "# Take weighted sum of value vectors, according to attention probabilities\n",
        "z = einops.einsum(\n",
        "    v,\n",
        "    attn_pattern,\n",
        "    \"???,??? -> batch posn_Q nheads d_head\",\n",
        ")\n",
        "\n",
        "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "attn_out = (\n",
        "    einops.einsum(\n",
        "        z,\n",
        "        self.W_O,\n",
        "        \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
        "    )\n",
        "    + self.b_O\n",
        ")\n",
        "return attn_out\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI7SyMgGNPhF"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    IGNORE: Float[Tensor, \"\"]\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=device))\n",
        "\n",
        "    # fmt: off\n",
        "    def forward(\n",
        "        self, normalized_resid_pre: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # Use einops!\n",
        "        # And to help us understand your code quickly, try to use only the following names with einops:\n",
        "        # batch head token token_k token_q d_model d_head\n",
        "\n",
        "        # Calculate query, key and value vectors\n",
        "        q = einops.eisum(\n",
        "            normalized_resid_pre, self.W_Q,\n",
        "            \"batch token d_model, head d_head d_model -> batch token head d_head\"\n",
        "        ) + self.b_Q\n",
        "        k = einops.einsum(\n",
        "            normalized_resid_pre, self.W_K,\n",
        "            \"batch token d_model, head d_head d_model -> batch token head d_head\"\n",
        "        ) + self.b_K\n",
        "        v = einops.einsum(\n",
        "            normalized_resid_pre, self.W_V,\n",
        "            \"batch token d_model, head d_head d_model -> batch token head d_head\"\n",
        "        ) + self.b_V\n",
        "\n",
        "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
        "        qk =einops.einsum(\n",
        "            q, k,\n",
        "            \"batch token head d_head, batch token head d_head -> batch head token\"\n",
        "        )\n",
        "\n",
        "        # Take weighted sum of value vectors, according to attention probabilities\n",
        "        ...  # TODO: ~17 words\n",
        "\n",
        "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "        ...  # TODO: ~20 words\n",
        "\n",
        "    # fmt: on\n",
        "    def apply_causal_mask(\n",
        "        self, attn_scores: Float[Tensor, \"batch head token_q token_k\"]\n",
        "    ) -> Float[Tensor, \"batch head token_q token_k\"]:\n",
        "        \"\"\"\n",
        "        Applies a causal mask to attention scores, and returns masked scores.\n",
        "        \"\"\"\n",
        "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
        "        all_ones = torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
        "        mask = torch.triu(all_ones, diagonal=1).bool()\n",
        "        # Apply the mask to attention scores, then return the masked scores\n",
        "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "        return attn_scores\n",
        "\n",
        "\n",
        "rand_float_test(Attention, [2, 4, 768])\n",
        "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjmmALSjNPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class Attention(nn.Module):\n",
        "    IGNORE: Float[Tensor, \"\"]\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=device))\n",
        "\n",
        "    # fmt: off\n",
        "    def forward(\n",
        "        self, normalized_resid_pre: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # Use einops!\n",
        "        # And to help us understand your code quickly, try to use only the following names with einops:\n",
        "        # batch head token token_k token_q d_model d_head\n",
        "\n",
        "        # Calculate query, key and value vectors\n",
        "        q = einops.einsum(normalized_resid_pre, self.W_Q,\n",
        "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
        "            ) + self.b_Q\n",
        "        k = einops.einsum(normalized_resid_pre, self.W_K,\n",
        "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
        "            ) + self.b_K\n",
        "        v = einops.einsum(normalized_resid_pre, self.W_V,\n",
        "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
        "            ) + self.b_V\n",
        "\n",
        "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
        "        attn_scores = einops.einsum(q, k,\n",
        "            \"batch token_q head d_head, batch token_k head d_head -> batch head token_q token_k\")\n",
        "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
        "        attn_pattern = attn_scores_masked.softmax(-1)\n",
        "\n",
        "        # Take weighted sum of value vectors, according to attention probabilities\n",
        "        z = einops.einsum(v, attn_pattern,\n",
        "            \"batch token_k head d_head, batch head token_q token_k -> batch token_q head d_head\")\n",
        "\n",
        "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "        attn_out = einops.einsum(z, self.W_O,\n",
        "                \"batch token_q head d_head, head d_head d_model -> batch token_q d_model\",\n",
        "            ) + self.b_O\n",
        "\n",
        "        return attn_out\n",
        "\n",
        "    # fmt: on\n",
        "    def apply_causal_mask(\n",
        "        self, attn_scores: Float[Tensor, \"batch head token_q token_k\"]\n",
        "    ) -> Float[Tensor, \"batch head token_q token_k\"]:\n",
        "        \"\"\"\n",
        "        Applies a causal mask to attention scores, and returns masked scores.\n",
        "        \"\"\"\n",
        "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
        "        all_ones = torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
        "        mask = torch.triu(all_ones, diagonal=1).bool()\n",
        "        # Apply the mask to attention scores, then return the masked scores\n",
        "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "        return attn_scores\n",
        "\n",
        "\n",
        "rand_float_test(Attention, [2, 4, 768])\n",
        "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rm1oCwyNPhF"
      },
      "source": [
        "Note: The `\"IGNORE\"` buffer is a very large negative number. This is the value you should mask your attention scores with (i.e. set them to this number wherever you want the probabilities to be zero).\n",
        "\n",
        "<details>\n",
        "<summary>Question - why do you think we mask the attention scores by setting them to a large negative number, rather than the attention probabilities by setting them to zero?</summary>\n",
        "\n",
        "If we masked the attention probabilities, then the probabilities would no longer sum to 1.\n",
        "\n",
        "We want to mask the scores and *then* take softmax, so that the probabilities are still valid probabilities (i.e. they sum to 1), and the values in the masked positions have no influence on the model's output.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyafKgLUNPhF"
      },
      "source": [
        "## MLP\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠🟠⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Next, you should implement the MLP layer, which consists of:\n",
        "\n",
        "* A linear layer, with weight `W_in`, bias `b_in`\n",
        "* A nonlinear functino (we usually use GELU; the function `gelu_new` has been imported for this purpose)\n",
        "* A linear layer, with weight `W_out`, bias `b_out`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNasz90SNPhF"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
        "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
        "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_mid: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]: ...  # TODO: ~37 words\n",
        "\n",
        "\n",
        "rand_float_test(MLP, [2, 4, 768])\n",
        "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y64Qj0sNPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
        "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
        "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_mid: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        pre = (\n",
        "            einops.einsum(\n",
        "                normalized_resid_mid,\n",
        "                self.W_in,\n",
        "                \"batch token d_model, d_model d_mlp -> batch token d_mlp\",\n",
        "            )\n",
        "            + self.b_in\n",
        "        )\n",
        "        post = gelu_new(pre)\n",
        "        mlp_out = (\n",
        "            einops.einsum(\n",
        "                post,\n",
        "                self.W_out,\n",
        "                \"batch token d_mlp, d_mlp d_model -> batch token d_model\",\n",
        "            )\n",
        "            + self.b_out\n",
        "        )\n",
        "        return mlp_out\n",
        "\n",
        "\n",
        "rand_float_test(MLP, [2, 4, 768])\n",
        "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5aOeSJrNPhF"
      },
      "source": [
        "## (Bonus) LayerNorm\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠🟠⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should fill in the code below, and then run the tests to verify that your layer is working correctly.\n",
        "\n",
        "Your LayerNorm should do the following:\n",
        "\n",
        "* Make mean 0\n",
        "* Normalize to have variance 1\n",
        "* Scale with learned weights\n",
        "* Translate with learned bias\n",
        "\n",
        "You can use the PyTorch [LayerNorm documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) as a reference. A few more notes:\n",
        "\n",
        "* Your layernorm implementation always has `affine=True`, i.e. you do learn parameters `w` and `b` (which are represented as $\\gamma$ and $\\beta$ respectively in the PyTorch documentation).\n",
        "* Remember that, after the centering and normalization, each vector of length `d_model` in your input should have mean 0 and variance 1.\n",
        "* As the PyTorch documentation page says, your variance should be computed using `unbiased=False`.\n",
        "* The `layer_norm_eps` argument in your config object corresponds to the $\\epsilon$ term in the PyTorch documentation (it is included to avoid division-by-zero errors).\n",
        "* We've given you a `debug` argument in your config. If `debug=True`, then you can print output like the shape of objects in your `forward` function to help you debug (this is a very useful trick to improve your coding speed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_jpM-diNPhF"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
        "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "\n",
        "    def forward(\n",
        "        self, residual: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
        "        residual_std = (\n",
        "            residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps\n",
        "        ).sqrt()\n",
        "\n",
        "        residual = (residual - residual_mean) / residual_std\n",
        "        return residual * self.w + self.b\n",
        "\n",
        "\n",
        "rand_float_test(LayerNorm, [2, 4, 768])\n",
        "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzXUcHiNPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
        "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "\n",
        "    def forward(\n",
        "        self, residual: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
        "        residual_std = (\n",
        "            residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps\n",
        "        ).sqrt()\n",
        "\n",
        "        residual = (residual - residual_mean) / residual_std\n",
        "        return residual * self.w + self.b\n",
        "\n",
        "\n",
        "rand_float_test(LayerNorm, [2, 4, 768])\n",
        "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtEfeAy2NPhF"
      },
      "source": [
        "## Transformer Block\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Now, we can put together the attention, MLP and layernorms into a single transformer block. Remember to implement the residual connections correctly!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AW6mGTzNPhF"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(\n",
        "        self, resid_pre: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # First, we add in the attention, but the residual stream needs to be normalized beforehand\n",
        "        ...  # TODO: ~7 words\n",
        "\n",
        "        # Then, we add in the MLP, again, the input of the MLP needs to be normalized beforehand\n",
        "        ...  # TODO: ~7 words\n",
        "\n",
        "        return resid_post\n",
        "\n",
        "\n",
        "rand_float_test(TransformerBlock, [2, 4, 768])\n",
        "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2hMWCAANPhF"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(\n",
        "        self, resid_pre: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
        "        # First, we add in the attention, but the residual stream needs to be normalized beforehand\n",
        "        resid_mid = resid_pre + self.attn(self.ln1(resid_pre))\n",
        "\n",
        "        # Then, we add in the MLP, again, the input of the MLP needs to be normalized beforehand\n",
        "        resid_post = resid_mid + self.mlp(self.ln2(resid_mid))\n",
        "\n",
        "        return resid_post\n",
        "\n",
        "\n",
        "rand_float_test(TransformerBlock, [2, 4, 768])\n",
        "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFL56CoBNPhG"
      },
      "source": [
        "## Unembedding\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "The unembedding is jus a linear layer (with weight `W_U` and bias `b_U`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCBUKg3BNPhG"
      },
      "outputs": [],
      "source": [
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_final: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_vocab\"]: ...  # TODO: ~16 words\n",
        "\n",
        "\n",
        "rand_float_test(Unembed, [2, 4, 768])\n",
        "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12lC1INPNPhG"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_final: Float[Tensor, \"batch token d_model\"]\n",
        "    ) -> Float[Tensor, \"batch token d_vocab\"]:\n",
        "        return (\n",
        "            einops.einsum(\n",
        "                normalized_resid_final,\n",
        "                self.W_U,\n",
        "                \"batch token d_model, d_model d_vocab -> batch token d_vocab\",\n",
        "            )\n",
        "            + self.b_U\n",
        "        )\n",
        "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
        "\n",
        "\n",
        "rand_float_test(Unembed, [2, 4, 768])\n",
        "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nij61lNDNPhG"
      },
      "source": [
        "## Full Transformer\n",
        "\n",
        "```c\n",
        "Difficulty: 🟠🟠⚪⚪⚪\n",
        "Importance: 🟠🟠🟠⚪⚪\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvxOdGmtNPhG"
      },
      "outputs": [],
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_vocab\"]:\n",
        "        # Hint: modules are defined in the order they should be used\n",
        "        ...  # TODO: ~23 words\n",
        "\n",
        "\n",
        "rand_int_test(DemoTransformer, [2, 4])\n",
        "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mWEbjKHNPhG"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_vocab\"]:\n",
        "        # Hint: modules are defined in the order they should be used\n",
        "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "        logits = self.unembed(self.ln_final(residual))\n",
        "        return logits\n",
        "\n",
        "\n",
        "rand_int_test(DemoTransformer, [2, 4])\n",
        "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC6yTNvwNPhG"
      },
      "source": [
        "**Try it out!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YngPTg23NPhG"
      },
      "outputs": [],
      "source": [
        "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
        "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
        "\n",
        "demo_logits = demo_gpt2(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEGacuw_NPhG"
      },
      "source": [
        "Let's take a test string, and calculate the loss!\n",
        "\n",
        "We're using the formula for **cross-entropy loss**. The cross entropy loss between a modelled distribution $Q$ and target distribution $P$ is:\n",
        "\n",
        "$$\n",
        "-\\sum_x P(x) \\log Q(x)\n",
        "$$\n",
        "\n",
        "In the case where $P$ is just the empirical distribution from target classes (i.e. $P(x^*) = 1$ for the correct class $x^*$) then this becomes:\n",
        "\n",
        "$$\n",
        "-\\log Q(x^*)\n",
        "$$\n",
        "\n",
        "in other words, the negative log prob of the true classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC56ryNHNPhG"
      },
      "outputs": [],
      "source": [
        "def get_log_probs(\n",
        "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
        ") -> Float[Tensor, \"batch posn-1\"]:\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
        "    log_probs_for_tokens = (\n",
        "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "    )\n",
        "\n",
        "    return log_probs_for_tokens\n",
        "\n",
        "\n",
        "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
        "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
        "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
        "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ADYHIBNPhG"
      },
      "source": [
        "<details>\n",
        "<summary>Show solution</summary>\n",
        "\n",
        "```python\n",
        "def get_log_probs(\n",
        "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
        ") -> Float[Tensor, \"batch posn-1\"]:\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
        "    log_probs_for_tokens = (\n",
        "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "    )\n",
        "\n",
        "    return log_probs_for_tokens\n",
        "\n",
        "\n",
        "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
        "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
        "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
        "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_awaY9nNPhG"
      },
      "source": [
        "We can also greedily generate text, by taking the most likely next token and continually appending it to our prompt before feeding it back into the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "557A53vANPhG"
      },
      "outputs": [],
      "source": [
        "test_string = (\n",
        "    \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
        ")\n",
        "for i in tqdm(range(100)):\n",
        "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
        "    demo_logits = demo_gpt2(test_tokens)\n",
        "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
        "\n",
        "print(test_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiE1jUUeNPhG"
      },
      "source": [
        "If you've finished this, congrats!\n",
        "You should ask the TA what to do next. One option is\n",
        "to look at training and sampling from tranformers in the rest of the arena notebook, which you can find it at https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer/transformer-arena.ipynb."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85778da3b43c4528964bf7c6c2587710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9599d3edf4f49329cdffc3a7ba27ac5",
              "IPY_MODEL_fbd696955fba4966863cf05554f55cc7",
              "IPY_MODEL_fb874333fddd4815a240c1810f6ec15f"
            ],
            "layout": "IPY_MODEL_d79baf00974a42ef87f36ccab966e6b9"
          }
        },
        "a9599d3edf4f49329cdffc3a7ba27ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3821d22e36444d7497086fe7f9bda32f",
            "placeholder": "​",
            "style": "IPY_MODEL_2a3fc8e2443347f29bfe362ef48567c0",
            "value": "config.json: 100%"
          }
        },
        "fbd696955fba4966863cf05554f55cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3a6b9485b9f4cdc8970603f781936ff",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c021cfe011f64990b1d1dbf0f158b193",
            "value": 665
          }
        },
        "fb874333fddd4815a240c1810f6ec15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825a3c2b03f64b18b630a7c9fa81e4f1",
            "placeholder": "​",
            "style": "IPY_MODEL_13ba44ae8d5a40a881e7b2aedf7f7fb9",
            "value": " 665/665 [00:00&lt;00:00, 55.4kB/s]"
          }
        },
        "d79baf00974a42ef87f36ccab966e6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3821d22e36444d7497086fe7f9bda32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a3fc8e2443347f29bfe362ef48567c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3a6b9485b9f4cdc8970603f781936ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c021cfe011f64990b1d1dbf0f158b193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825a3c2b03f64b18b630a7c9fa81e4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ba44ae8d5a40a881e7b2aedf7f7fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b465315320114cd8956e9928209a60f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2beb90d00cda44419f83fc52a193056e",
              "IPY_MODEL_ce1c53c7eaf84e03bc03516ca936fec5",
              "IPY_MODEL_13f8a70f3ca64e97aad119c9d6c1466e"
            ],
            "layout": "IPY_MODEL_1a2b8bf273e540418267d05db7334538"
          }
        },
        "2beb90d00cda44419f83fc52a193056e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3bafb4d1b284fb9b221d0ead6b69e98",
            "placeholder": "​",
            "style": "IPY_MODEL_cad80ac460c343f5bbd292229ed9f645",
            "value": "model.safetensors: 100%"
          }
        },
        "ce1c53c7eaf84e03bc03516ca936fec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74812e4a33d4455ba27404899a37fc4c",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f1162f1a4bb43e388a6993749505964",
            "value": 548105171
          }
        },
        "13f8a70f3ca64e97aad119c9d6c1466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f560b0721d4cc080418805b3078a20",
            "placeholder": "​",
            "style": "IPY_MODEL_249c09e2a8ae454db8f462184bf8b7cd",
            "value": " 548M/548M [00:07&lt;00:00, 78.9MB/s]"
          }
        },
        "1a2b8bf273e540418267d05db7334538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3bafb4d1b284fb9b221d0ead6b69e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad80ac460c343f5bbd292229ed9f645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74812e4a33d4455ba27404899a37fc4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1162f1a4bb43e388a6993749505964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01f560b0721d4cc080418805b3078a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249c09e2a8ae454db8f462184bf8b7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b321aecfdb4a0480d69fca636cfdf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ebc805b05a2491ea3deaa10d6db4ffc",
              "IPY_MODEL_5251ea5a262a4c4a8a7fe4e27b219a6b",
              "IPY_MODEL_6d1b8e0e35a14cfd8c27948650047db2"
            ],
            "layout": "IPY_MODEL_58a3aa732fab4f1797aa6fd1bb3f30ae"
          }
        },
        "5ebc805b05a2491ea3deaa10d6db4ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea4521464064b75aa502d3fafbd6a57",
            "placeholder": "​",
            "style": "IPY_MODEL_38564ed5c71449818df0e518eb7f2260",
            "value": "generation_config.json: 100%"
          }
        },
        "5251ea5a262a4c4a8a7fe4e27b219a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5989162fff34453ab70fa548f1242ec3",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43718209a89a4c1091350bf5fcfc8f9b",
            "value": 124
          }
        },
        "6d1b8e0e35a14cfd8c27948650047db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18575b2e04ce40b89c078c56942aa957",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8cc7de93f54ca9abc30278ffbe8d3d",
            "value": " 124/124 [00:00&lt;00:00, 12.3kB/s]"
          }
        },
        "58a3aa732fab4f1797aa6fd1bb3f30ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea4521464064b75aa502d3fafbd6a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38564ed5c71449818df0e518eb7f2260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5989162fff34453ab70fa548f1242ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43718209a89a4c1091350bf5fcfc8f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18575b2e04ce40b89c078c56942aa957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8cc7de93f54ca9abc30278ffbe8d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388de2bd00534a588a1af53453fee431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_feb36a4c9c7c40f9bf776a460dc45d7b",
              "IPY_MODEL_ae947960c5e24a95b9b723db0822b9db",
              "IPY_MODEL_36c68d9705224cb5a07f267641e410fc"
            ],
            "layout": "IPY_MODEL_0bfe73fe21a247759fa55c1ff9930014"
          }
        },
        "feb36a4c9c7c40f9bf776a460dc45d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93c1c30fc7048a9b3201db3742abba0",
            "placeholder": "​",
            "style": "IPY_MODEL_e449dc5542d44adca9197e19115ba81a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ae947960c5e24a95b9b723db0822b9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df3d75d8504e4a079270564cb7f48585",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e56a220dfa14187bbfbe422b717c641",
            "value": 26
          }
        },
        "36c68d9705224cb5a07f267641e410fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5eeb307dd14a6ebae570319edba9f3",
            "placeholder": "​",
            "style": "IPY_MODEL_8e630aee7afc4ea2970d92954f899fd4",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.71kB/s]"
          }
        },
        "0bfe73fe21a247759fa55c1ff9930014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93c1c30fc7048a9b3201db3742abba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e449dc5542d44adca9197e19115ba81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df3d75d8504e4a079270564cb7f48585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e56a220dfa14187bbfbe422b717c641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b5eeb307dd14a6ebae570319edba9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e630aee7afc4ea2970d92954f899fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c631347a11414b81a5ef83c2bc94c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62e506925fd34082a8e642407fbe39df",
              "IPY_MODEL_dbe130f685b64921809bec44b93d703d",
              "IPY_MODEL_1af82d3b69f6449697730ebd3e0b1182"
            ],
            "layout": "IPY_MODEL_5652e4f05e614f3696fdf9f83cab8a01"
          }
        },
        "62e506925fd34082a8e642407fbe39df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49903c000082497fb6b0938321906318",
            "placeholder": "​",
            "style": "IPY_MODEL_93c6a07f87b1426e8fcc30f55d4ec1a1",
            "value": "vocab.json: 100%"
          }
        },
        "dbe130f685b64921809bec44b93d703d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85cd7c877019468c9da15ac17ddf6139",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6274000cb7c4b04a8fd779ec11ae297",
            "value": 1042301
          }
        },
        "1af82d3b69f6449697730ebd3e0b1182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96c1d2fb225e450199b01f0de36e1edd",
            "placeholder": "​",
            "style": "IPY_MODEL_2190a0f4d15f4e018cc5eeb0f4de31c5",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.82MB/s]"
          }
        },
        "5652e4f05e614f3696fdf9f83cab8a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49903c000082497fb6b0938321906318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c6a07f87b1426e8fcc30f55d4ec1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85cd7c877019468c9da15ac17ddf6139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6274000cb7c4b04a8fd779ec11ae297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96c1d2fb225e450199b01f0de36e1edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2190a0f4d15f4e018cc5eeb0f4de31c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "301b45a61f71462688c5e41c1eb24c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62973b7cbff74197afb856f89f065c45",
              "IPY_MODEL_de6df693d2fd4b6da9b5f54294f80a83",
              "IPY_MODEL_420f0deb117b43f0b98805cb042e705a"
            ],
            "layout": "IPY_MODEL_4800a45f697043e897f630e6d6001b1b"
          }
        },
        "62973b7cbff74197afb856f89f065c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b8461de12d4a70b0e98d7bec6d37c3",
            "placeholder": "​",
            "style": "IPY_MODEL_548104867bcb436f8c93899d28562fbc",
            "value": "merges.txt: 100%"
          }
        },
        "de6df693d2fd4b6da9b5f54294f80a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3d84ea51ae440ba8865dd29e9a7c1fa",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_608ca3c37d714ec295ed8a87dd21a89e",
            "value": 456318
          }
        },
        "420f0deb117b43f0b98805cb042e705a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5f3c4fd12242e9931d07686b9ad6b5",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf0d9e17dfa4deaa80fc7c7b666f6e0",
            "value": " 456k/456k [00:00&lt;00:00, 11.0MB/s]"
          }
        },
        "4800a45f697043e897f630e6d6001b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b8461de12d4a70b0e98d7bec6d37c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "548104867bcb436f8c93899d28562fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3d84ea51ae440ba8865dd29e9a7c1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608ca3c37d714ec295ed8a87dd21a89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd5f3c4fd12242e9931d07686b9ad6b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf0d9e17dfa4deaa80fc7c7b666f6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29c0a10c4a264098a91a9a6005292b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4680476fbefd4127bf4143872363d868",
              "IPY_MODEL_51bdded0afb444528d51f3e057030b90",
              "IPY_MODEL_836a4d46c4e94db9b6f11109f94b6a37"
            ],
            "layout": "IPY_MODEL_614d6b28b5af465ca2ab06ca0640869d"
          }
        },
        "4680476fbefd4127bf4143872363d868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f6f49c4a62242ceacd1db0c8bf981a1",
            "placeholder": "​",
            "style": "IPY_MODEL_dc77a4d01e3746a0bc7f9595122b9416",
            "value": "tokenizer.json: 100%"
          }
        },
        "51bdded0afb444528d51f3e057030b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f471a38cb3584b348057edd30aa80b5b",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff438f0e5ea14eb3ad7df630356a015f",
            "value": 1355256
          }
        },
        "836a4d46c4e94db9b6f11109f94b6a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e751bae3cf41468ab4cbe9156008cf",
            "placeholder": "​",
            "style": "IPY_MODEL_b9856aa51ae84b81b155a24c4ad7cca2",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.74MB/s]"
          }
        },
        "614d6b28b5af465ca2ab06ca0640869d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f6f49c4a62242ceacd1db0c8bf981a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc77a4d01e3746a0bc7f9595122b9416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f471a38cb3584b348057edd30aa80b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff438f0e5ea14eb3ad7df630356a015f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5e751bae3cf41468ab4cbe9156008cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9856aa51ae84b81b155a24c4ad7cca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}